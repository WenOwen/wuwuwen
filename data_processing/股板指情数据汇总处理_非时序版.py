#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®å¤„ç† - éæ—¶åºç‰ˆ
ä¸“æ³¨å½“æ—¥ç‰¹å¾ï¼šæå–æ¯ä¸ªäº¤æ˜“æ—¥çš„ç‰¹å¾ï¼Œåˆ—ååŒ…å«æ—¥æœŸï¼Œä¸ä½¿ç”¨æ—¶åºçª—å£
"""

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import glob
import warnings
from datetime import datetime, timedelta
import json
import yaml

warnings.filterwarnings('ignore')


class NonTimeSeriesStockDataProcessor:
    """éæ—¶åºè‚¡ç¥¨æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        # åŠ è½½é…ç½®
        self.config = self.load_config(config_path) if config_path else {}
        
        # è®¾ç½®è·¯å¾„
        base_path = self.config.get('paths', {}).get('data_base_path', './data')
        paths = self.config.get('paths', {})
        self.stock_path = os.path.join(base_path, paths.get('stock_path', 'datas_em'))
        self.sector_path = os.path.join(base_path, paths.get('industry_path', 'datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²'))
        self.index_path = os.path.join(base_path, paths.get('index_path', 'datas_index/datas_index'))
        self.concept_path = os.path.join(base_path, paths.get('concept_path', 'datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²'))
        
        # ç‰¹å¾é…ç½®
        self.features_config = self.config.get('features', {})
        self.processing_config = self.config.get('processing', {})
        self.output_config = self.config.get('output', {})
        
        # éæ—¶åºç‰¹å®šé…ç½®
        self.include_date_in_columns = self.processing_config.get('include_date_in_columns', False)
        self.date_format = self.processing_config.get('date_format', '%Y%m%d')
        self.target_prediction_days = self.processing_config.get('target_prediction_days', 1)
        
        # è¾“å‡ºç»„ç»‡é…ç½®
        self.group_by_date = self.processing_config.get('group_by_date', True)
        self.month_folder_format = self.processing_config.get('month_folder_format', '%y%m')
        self.daily_file_format = self.processing_config.get('daily_file_format', '%Y%m%d')
        self.only_save_full_data = self.output_config.get('only_save_full_data', True)
    
    @staticmethod
    def load_config(config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def get_feature_columns(self, feature_type: str) -> List[str]:
        """è·å–æŒ‡å®šç±»å‹çš„ç‰¹å¾åˆ—"""
        config = self.features_config.get(feature_type, {})
        
        if feature_type == 'stock':
            # åŸºç¡€ç‰¹å¾
            basic = config.get('basic_features', [])
            
            # æŠ€æœ¯æŒ‡æ ‡
            tech = []
            if config.get('technical_indicators', {}).get('enabled', True):
                indicators = config.get('technical_indicators', {}).get('indicators', [])
                for ind in indicators:
                    name = ind.get('name')
                    if name == 'RSI':
                        tech.append('RSI')
                    elif name == 'MACD':
                        tech.extend(['MACD', 'MACD_signal'])
                    elif name == 'å¸ƒæ—å¸¦':
                        tech.extend(['BB_upper', 'BB_middle', 'BB_lower'])
                    elif name == 'ATR':
                        tech.append('ATR')
                    elif name == 'ROC':
                        tech.append('ROC')
            
            return basic + tech
            
        elif feature_type == 'sector':
            return config.get('features', ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡', 'æ¶¨è·Œå¹…'])
        
        return []
    
    def load_stock_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½è‚¡ç¥¨æ•°æ®"""
        if not self.features_config.get('stock', {}).get('enabled', True):
            return {}
        
        limit = self.processing_config.get('limit_stocks')
        files = glob.glob(os.path.join(self.stock_path, "*.csv"))
        if limit:
            files = files[:limit]
        
        feature_cols = self.get_feature_columns('stock')
        target_dims = self.features_config.get('stock', {}).get('target_dimensions', 16)
        
        stock_data = {}
        for file_path in files:
            try:
                code = os.path.basename(file_path).replace('.csv', '')
                df = pd.read_csv(file_path, encoding='utf-8')
                df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
                df = df.set_index('äº¤æ˜“æ—¥æœŸ').sort_index()
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                df = self.calculate_indicators(df)
                
                # é€‰æ‹©ç‰¹å¾
                available_cols = [col for col in feature_cols if col in df.columns]
                df_selected = df[available_cols].copy() if available_cols else df.iloc[:, :target_dims].copy()
                
                # è°ƒæ•´ç»´åº¦
                while len(df_selected.columns) < target_dims:
                    df_selected[f'feature_{len(df_selected.columns)}'] = 0
                df_selected = df_selected.iloc[:, :target_dims]
                
                stock_data[code] = df_selected
                
            except Exception:
                continue
        
        return stock_data
    
    def load_sector_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½è¡Œä¸šæ•°æ®"""
        if not self.features_config.get('sector', {}).get('enabled', True):
            return {}
        
        files = glob.glob(os.path.join(self.sector_path, "*.csv"))
        feature_cols = self.get_feature_columns('sector')
        target_dims = self.features_config.get('sector', {}).get('target_dimensions', 6)
        
        sector_data = {}
        for file_path in files:
            try:
                name = os.path.basename(file_path).split('(')[0]
                df = pd.read_csv(file_path, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                df = df.set_index('æ—¥æœŸ').sort_index()
                
                # é€‰æ‹©ç‰¹å¾
                available_cols = [col for col in feature_cols if col in df.columns]
                if len(available_cols) >= 4:
                    df_selected = df[available_cols].copy()
                    
                    # è°ƒæ•´ç»´åº¦
                    while len(df_selected.columns) < target_dims:
                        df_selected[f'feature_{len(df_selected.columns)}'] = 0
                    df_selected = df_selected.iloc[:, :target_dims]
                    
                    sector_data[name] = df_selected
                    
            except Exception:
                continue
        
        return sector_data
    
    def load_index_data(self) -> pd.DataFrame:
        """åŠ è½½æŒ‡æ•°æ•°æ®"""
        if not self.features_config.get('index', {}).get('enabled', True):
            return pd.DataFrame()
        
        index_mapping = self.features_config.get('index', {}).get('index_mapping', {
            'zs000001.csv': 'ä¸Šè¯æŒ‡æ•°',
            'zs000300.csv': 'æ²ªæ·±300',
            'zs399001.csv': 'æ·±è¯æˆæŒ‡',
            'zs399006.csv': 'åˆ›ä¸šæ¿æŒ‡',
            'zs000905.csv': 'ä¸­è¯500'
        })
        
        target_dims = self.features_config.get('index', {}).get('target_dimensions', 5)
        all_data = []
        
        for filename, name in index_mapping.items():
            file_path = os.path.join(self.index_path, filename)
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                    df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
                    df = df.set_index('äº¤æ˜“æ—¥æœŸ').sort_index()
                    df_selected = df[['æ”¶ç›˜ä»·']].copy()
                    df_selected.columns = [name]
                    all_data.append(df_selected)
                except Exception:
                    continue
        
        if all_data:
            combined = all_data[0]
            for df in all_data[1:]:
                combined = combined.join(df, how='outer')
            
            # è°ƒæ•´ç»´åº¦
            while len(combined.columns) < target_dims:
                combined[f'index_{len(combined.columns)}'] = 0
            combined = combined.iloc[:, :target_dims]
            
            return combined.fillna(0)
        
        return pd.DataFrame()
    
    def load_sentiment_data(self) -> pd.DataFrame:
        """åŠ è½½æƒ…ç»ªæ•°æ®"""
        if not self.features_config.get('sentiment', {}).get('enabled', True):
            return pd.DataFrame()
        
        files = glob.glob(os.path.join(self.concept_path, "*.csv"))
        target_dims = self.features_config.get('sentiment', {}).get('target_dimensions', 2)
        
        # ç®€åŒ–ï¼šåªå–å‰ä¸¤ä¸ªæ¦‚å¿µæ–‡ä»¶ä½œä¸ºæƒ…ç»ªç‰¹å¾
        sentiment_data = []
        for file_path in files[:target_dims]:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                df = df.set_index('æ—¥æœŸ').sort_index()
                if 'æ¶¨è·Œå¹…' in df.columns:
                    sentiment_data.append(df[['æ¶¨è·Œå¹…']])
            except Exception:
                continue
        
        if sentiment_data:
            combined = pd.concat(sentiment_data, axis=1)
            combined.columns = [f'sentiment_{i}' for i in range(len(combined.columns))]
            
            # è°ƒæ•´ç»´åº¦
            while len(combined.columns) < target_dims:
                combined[f'sentiment_{len(combined.columns)}'] = 0
            combined = combined.iloc[:, :target_dims]
            
            return combined.fillna(0)
        
        return pd.DataFrame()
    
    def calculate_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
        try:
            # RSI
            delta = df['æ”¶ç›˜ä»·'].diff()
            gain = delta.where(delta > 0, 0).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            df['RSI'] = 100 - (100 / (1 + rs))
            
            # MACD
            exp1 = df['æ”¶ç›˜ä»·'].ewm(span=12).mean()
            exp2 = df['æ”¶ç›˜ä»·'].ewm(span=26).mean()
            df['MACD'] = exp1 - exp2
            df['MACD_signal'] = df['MACD'].ewm(span=9).mean()
            
            # å¸ƒæ—å¸¦
            rolling_mean = df['æ”¶ç›˜ä»·'].rolling(20).mean()
            rolling_std = df['æ”¶ç›˜ä»·'].rolling(20).std()
            df['BB_upper'] = rolling_mean + (rolling_std * 2)
            df['BB_middle'] = rolling_mean
            df['BB_lower'] = rolling_mean - (rolling_std * 2)
            
            # ATR
            high_low = df['æœ€é«˜ä»·'] - df['æœ€ä½ä»·']
            high_close = np.abs(df['æœ€é«˜ä»·'] - df['æ”¶ç›˜ä»·'].shift())
            low_close = np.abs(df['æœ€ä½ä»·'] - df['æ”¶ç›˜ä»·'].shift())
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = ranges.max(axis=1)
            df['ATR'] = true_range.rolling(14).mean()
            
            # ROC
            df['ROC'] = df['æ”¶ç›˜ä»·'].pct_change(12) * 100
            
        except Exception:
            pass
        
        return df
    
    def create_daily_samples_with_feature_names(self, stock_data: Dict, sector_data: Dict, index_data: pd.DataFrame, sentiment_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str], List[str], List[str]]:
        """åˆ›å»ºæ¯æ—¥æ ·æœ¬å¹¶ç”Ÿæˆç‰¹å¾åç§°ï¼ˆä¸å«æ—¥æœŸï¼‰"""
        start_date = self.processing_config.get('date_range', {}).get('start_date')
        end_date = self.processing_config.get('date_range', {}).get('end_date')
        
        all_samples = []
        all_targets = []
        all_codes = []
        all_dates = []      # å­˜å‚¨æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ—¥æœŸ
        feature_names = []  # å­˜å‚¨ç‰¹å¾åç§°
        feature_names_generated = False
        
        # å°†è¡Œä¸šæ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¾¿äºæŒ‰è‚¡ç¥¨åˆ†é…
        sector_list = list(sector_data.items()) if sector_data else []
        
        for stock_idx, (code, stock_df) in enumerate(stock_data.items()):
            try:
                # æ•°æ®å¯¹é½
                dates = stock_df.index
                
                # ä¸ºå½“å‰è‚¡ç¥¨åˆ†é…è¡Œä¸šæ•°æ®ï¼ˆåŸºäºè‚¡ç¥¨ç´¢å¼•å¾ªç¯åˆ†é…ï¼‰
                current_sector_df = None
                current_sector_name = None
                if sector_list:
                    sector_idx = stock_idx % len(sector_list)  # å¾ªç¯åˆ†é…è¡Œä¸šæ•°æ®
                    current_sector_name, current_sector_df = sector_list[sector_idx]
                
                # å¯¹é½æ—¥æœŸç´¢å¼•
                if current_sector_df is not None and not index_data.empty:
                    dates = dates.intersection(current_sector_df.index)
                    dates = dates.intersection(index_data.index)
                elif current_sector_df is not None:
                    dates = dates.intersection(current_sector_df.index)
                elif not index_data.empty:
                    dates = dates.intersection(index_data.index)
                
                if not sentiment_data.empty:
                    dates = dates.intersection(sentiment_data.index)
                
                # æ—¥æœŸèŒƒå›´è¿‡æ»¤
                if start_date:
                    dates = dates[dates >= pd.to_datetime(start_date)]
                if end_date:
                    dates = dates[dates <= pd.to_datetime(end_date)]
                
                if len(dates) < 1:  # ä¿®æ”¹ï¼šåªéœ€è¦è‡³å°‘1å¤©æ•°æ®
                    continue
                
                dates = dates.sort_values()
                
                # å¯¹é½æ•°æ®
                stock_aligned = stock_df.reindex(dates).fillna(0)
                
                # ä½¿ç”¨ä¸ºå½“å‰è‚¡ç¥¨åˆ†é…çš„è¡Œä¸šæ•°æ®
                if current_sector_df is not None:
                    sector_aligned = current_sector_df.reindex(dates).fillna(0)
                    if stock_idx < 3:  # åªä¸ºå‰3ä¸ªè‚¡ç¥¨æ˜¾ç¤ºè¡Œä¸šåˆ†é…ä¿¡æ¯
                        print(f"ğŸ“ˆ è‚¡ç¥¨ {code} åˆ†é…è¡Œä¸šæ•°æ®: {current_sector_name}")
                else:
                    sector_aligned = None
                
                if not index_data.empty:
                    index_aligned = index_data.reindex(dates).fillna(0)
                else:
                    index_aligned = None
                
                if not sentiment_data.empty:
                    sentiment_aligned = sentiment_data.reindex(dates).fillna(0)
                else:
                    sentiment_aligned = None
                
                # åˆ›å»ºæ¯æ—¥æ ·æœ¬ - å¤„ç†æ‰€æœ‰æ—¥æœŸ
                for i in range(len(dates)):
                    current_date = dates[i]
                    
                    # å½“å¤©çš„ç‰¹å¾
                    daily_features = []
                    current_feature_names = []  # å½“å‰æ ·æœ¬çš„ç‰¹å¾åç§°
                    
                    # è®°å½•å½“å‰æ—¥æœŸï¼ˆç”¨äºæ–‡ä»¶åˆ†ç»„ï¼‰
                    date_str = current_date.strftime(self.date_format)
                    
                    # ä¸ªè‚¡ç‰¹å¾ - å½“å¤©
                    if self.features_config.get('stock', {}).get('enabled', True):
                        stock_values = stock_aligned.iloc[i].values
                        daily_features.extend(stock_values)
                        
                        # ç”Ÿæˆä¸ªè‚¡ç‰¹å¾åç§° (åªåœ¨ç¬¬ä¸€æ¬¡ç”Ÿæˆ)
                        if not feature_names_generated:
                            for col_name in stock_aligned.columns:
                                feature_name = f"ä¸ªè‚¡_{col_name}"
                                current_feature_names.append(feature_name)
                    
                    # è¡Œä¸šç‰¹å¾ - å½“å¤©
                    if self.features_config.get('sector', {}).get('enabled', True) and sector_aligned is not None:
                        sector_values = sector_aligned.iloc[i].values
                        daily_features.extend(sector_values)
                        
                        # ç”Ÿæˆè¡Œä¸šç‰¹å¾åç§°
                        if not feature_names_generated:
                            for col_name in sector_aligned.columns:
                                feature_name = f"è¡Œä¸š_{col_name}"
                                current_feature_names.append(feature_name)
                    
                    # æŒ‡æ•°ç‰¹å¾ - å½“å¤©
                    if self.features_config.get('index', {}).get('enabled', True) and index_aligned is not None:
                        index_values = index_aligned.iloc[i].values
                        daily_features.extend(index_values)
                        
                        # ç”ŸæˆæŒ‡æ•°ç‰¹å¾åç§°
                        if not feature_names_generated:
                            for col_name in index_aligned.columns:
                                feature_name = f"æŒ‡æ•°_{col_name}"
                                current_feature_names.append(feature_name)
                    
                    # æƒ…ç»ªç‰¹å¾ - å½“å¤©
                    if self.features_config.get('sentiment', {}).get('enabled', True) and sentiment_aligned is not None:
                        sentiment_values = sentiment_aligned.iloc[i].values
                        daily_features.extend(sentiment_values)
                        
                        # ç”Ÿæˆæƒ…ç»ªç‰¹å¾åç§°
                        if not feature_names_generated:
                            for col_name in sentiment_aligned.columns:
                                feature_name = f"æƒ…ç»ª_{col_name}"
                                current_feature_names.append(feature_name)
                    
                    if daily_features:
                        combined = np.array(daily_features)
                        
                        # ä¿å­˜ç‰¹å¾åç§° (åªåœ¨ç¬¬ä¸€æ¬¡)
                        if not feature_names_generated:
                            feature_names = current_feature_names
                            feature_names_generated = True
                        
                        # ä¸å†è®¡ç®—targetï¼Œåªä¿å­˜ç‰¹å¾æ•°æ®
                        all_samples.append(combined)
                        all_targets.append(0)  # å ä½ç¬¦ï¼Œåç»­ä¼šè¢«ç§»é™¤
                        all_codes.append(code)
                        all_dates.append(current_date)
                
            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_samples:
            return np.array(all_samples), np.array(all_targets), all_codes, feature_names, all_dates
        else:
            raise ValueError("æ²¡æœ‰ç”Ÿæˆæ ·æœ¬")
    
    def save_data_with_meaningful_names(self, X: np.ndarray, y: np.ndarray, codes: List[str], feature_names: List[str] = None, dates: List = None) -> str:
        """æŒ‰æ—¥æœŸåˆ†ç»„ä¿å­˜æ•°æ®åˆ°å¹´æœˆæ–‡ä»¶å¤¹"""
        if dates is None:
            raise ValueError("dateså‚æ•°ä¸èƒ½ä¸ºç©º")
            
        save_base_dir = self.output_config.get('save_base_dir', './data/datas_final')
        
        # å¤„ç†Xæ•°æ® (éæ—¶åºæ•°æ®åº”è¯¥æ˜¯2Dçš„)
        if len(X.shape) == 2:
            X_processed = X
            if feature_names is None or len(feature_names) != X.shape[1]:
                feature_names = [f'feature_{i:02d}' for i in range(X.shape[1])]
        else:
            raise ValueError(f"æœŸæœ›2Dæ•°æ®ï¼Œä½†å¾—åˆ°{len(X.shape)}Dæ•°æ®: {X.shape}")
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æ•°æ®
        data_by_date = {}
        for i, (sample, target, code, date) in enumerate(zip(X_processed, y, codes, dates)):
            date_key = date.strftime(self.daily_file_format)
            if date_key not in data_by_date:
                data_by_date[date_key] = {
                    'samples': [],
                    'targets': [],
                    'codes': [],
                    'date_obj': date
                }
            data_by_date[date_key]['samples'].append(sample)
            data_by_date[date_key]['targets'].append(target)
            data_by_date[date_key]['codes'].append(code)
        
        saved_files = []
        
        # ä¸ºæ¯ä¸ªæ—¥æœŸåˆ›å»ºæ–‡ä»¶
        for date_key, date_data in data_by_date.items():
            date_obj = date_data['date_obj']
            
            # åˆ›å»ºå¹´æœˆæ–‡ä»¶å¤¹ (ä¾‹å¦‚: 2508 è¡¨ç¤º2025å¹´8æœˆ)
            month_folder = date_obj.strftime(self.month_folder_format)
            month_dir = os.path.join(save_base_dir, month_folder)
            os.makedirs(month_dir, exist_ok=True)
            
            # åˆ›å»ºå½“æ—¥çš„DataFrame
            daily_X = np.array(date_data['samples'])
            daily_y = np.array(date_data['targets'])
            daily_codes = date_data['codes']
            
            # åˆ›å»ºå®Œæ•´æ•°æ®DataFrameï¼ˆstock_codeä½œä¸ºç¬¬ä¸€åˆ—ï¼Œä¸åŒ…å«targetï¼‰
            full_df = pd.DataFrame()
            full_df['stock_code'] = daily_codes  # ç¬¬ä¸€åˆ—ï¼šè‚¡ç¥¨ä»£ç 
            
            # æ·»åŠ ç‰¹å¾åˆ—
            feature_df = pd.DataFrame(daily_X, columns=feature_names)
            full_df = pd.concat([full_df, feature_df], axis=1)
            
            # ä¿å­˜å½“æ—¥çš„full_data.csv
            daily_file_path = os.path.join(month_dir, f"{date_key}.csv")
            full_df.to_csv(daily_file_path, index=False, encoding='utf-8')
            saved_files.append(daily_file_path)
            
            print(f"ğŸ“… {date_key}: ä¿å­˜äº† {len(daily_codes)} ä¸ªæ ·æœ¬")
        
        # åœ¨åŸºç¡€ç›®å½•ä¿å­˜ç‰¹å¾åç§°ä¿¡æ¯ï¼ˆä¾›å‚è€ƒï¼‰
        os.makedirs(save_base_dir, exist_ok=True)
        
        # ä¿å­˜ç‰¹å¾åç§°ä¿¡æ¯
        with open(os.path.join(save_base_dir, "feature_names.json"), 'w', encoding='utf-8') as f:
            json.dump(feature_names, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å¤„ç†ä¿¡æ¯
        info = {
            'processing_time': datetime.now().isoformat(),
            'total_samples': len(X_processed),
            'total_stocks': len(set(codes)),
            'total_dates': len(data_by_date),
            'feature_count': len(feature_names),
            'month_folder_format': self.month_folder_format,
            'daily_file_format': self.daily_file_format,
            'output_structure': 'yearly_monthly_folders',
            'file_type': 'daily_features_csv',
            'columns_order': ['stock_code'] + feature_names,
            'has_target': False,
            'date_range': {
                'start': min(dates).strftime('%Y-%m-%d'),
                'end': max(dates).strftime('%Y-%m-%d')
            },
            'files_created': [os.path.relpath(f, save_base_dir) for f in saved_files]
        }
        
        with open(os.path.join(save_base_dir, "processing_info.json"), 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print("=" * 60)
        print("ğŸ“Š æ•°æ®ä¿å­˜å®Œæˆ:")
        print(f"   ğŸ“ åŸºç¡€ç›®å½•: {save_base_dir}")
        print(f"   ğŸ“… æ—¥æœŸèŒƒå›´: {info['date_range']['start']} ~ {info['date_range']['end']}")
        print(f"   ğŸ—‚ï¸  æœˆä»½æ–‡ä»¶å¤¹: {len(set(date.strftime(self.month_folder_format) for date in dates))} ä¸ª")
        print(f"   ğŸ“„ CSVæ–‡ä»¶: {len(saved_files)} ä¸ª")
        print(f"   ğŸ·ï¸  ç‰¹å¾æ•°é‡: {len(feature_names)} ä¸ª")
        print(f"   ğŸ“‹ åˆ—ç»“æ„: stock_code (ç¬¬1åˆ—) + {len(feature_names)}ä¸ªç‰¹å¾åˆ—")
        print(f"   ğŸ¯ ä¸åŒ…å«targetåˆ— (ç”±è®­ç»ƒè„šæœ¬å®šä¹‰)")
        print()
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤¹ç»“æ„ç¤ºä¾‹
        month_folders = set(date.strftime(self.month_folder_format) for date in dates)
        print("ğŸ“ æ–‡ä»¶å¤¹ç»“æ„ç¤ºä¾‹:")
        for month_folder in sorted(month_folders)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæœˆä»½
            month_files = [f for f in saved_files if f"/{month_folder}/" in f]
            print(f"   {save_base_dir}/{month_folder}/")
            for file_path in month_files[:3]:  # æ¯ä¸ªæœˆä»½åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                filename = os.path.basename(file_path)
                print(f"     â”œâ”€â”€ {filename}")
            if len(month_files) > 3:
                print(f"     â””â”€â”€ ... è¿˜æœ‰ {len(month_files) - 3} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºç‰¹å¾åç§°ç¤ºä¾‹
        print("\nğŸ·ï¸  CSVæ–‡ä»¶ç»“æ„ç¤ºä¾‹:")
        print("   ç¬¬1åˆ—: stock_code")
        feature_types = {}
        for name in feature_names:
            if name.startswith('ä¸ªè‚¡_'):
                feature_type = 'ä¸ªè‚¡'
            elif name.startswith('è¡Œä¸š_'):
                feature_type = 'è¡Œä¸š'
            elif name.startswith('æŒ‡æ•°_'):
                feature_type = 'æŒ‡æ•°'
            elif name.startswith('æƒ…ç»ª_'):
                feature_type = 'æƒ…ç»ª'
            else:
                feature_type = 'å…¶ä»–'
            
            if feature_type not in feature_types:
                feature_types[feature_type] = []
            feature_types[feature_type].append(name)
        
        col_index = 2  # ä»ç¬¬2åˆ—å¼€å§‹
        for feature_type, features_in_type in feature_types.items():
            examples = features_in_type[:3]
            if len(features_in_type) > 3:
                examples_str = f"{', '.join(examples)}... (+{len(features_in_type)-3}ä¸ª)"
            else:
                examples_str = ', '.join(examples)
            print(f"   ç¬¬{col_index}-{col_index+len(features_in_type)-1}åˆ—: {feature_type}ç‰¹å¾ ({examples_str})")
            col_index += len(features_in_type)
        
        return save_base_dir


def main(config_path: str = None):
    """ä¸»å¤„ç†æµç¨‹ - éæ—¶åºç‰ˆ"""
    if config_path is None:
        config_path = "config/datas_process/data_process_non_timeseries.yaml"
    
    print(f"ä½¿ç”¨é…ç½®: {config_path}")
    print("=" * 60)
    print("ğŸš€ éæ—¶åºè‚¡ç¥¨æ•°æ®å¤„ç†å¼€å§‹")
    print("=" * 60)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = NonTimeSeriesStockDataProcessor(config_path)
    
    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“ˆ åŠ è½½è‚¡ç¥¨æ•°æ®...")
        stock_data = processor.load_stock_data()
        print(f"   åŠ è½½äº† {len(stock_data)} åªè‚¡ç¥¨")
        
        print("ğŸ­ åŠ è½½è¡Œä¸šæ•°æ®...")
        sector_data = processor.load_sector_data()
        print(f"   åŠ è½½äº† {len(sector_data)} ä¸ªè¡Œä¸š")
        
        print("ğŸ“Š åŠ è½½æŒ‡æ•°æ•°æ®...")
        index_data = processor.load_index_data()
        print(f"   åŠ è½½äº† {len(index_data.columns) if not index_data.empty else 0} ä¸ªæŒ‡æ•°")
        
        print("ğŸ’­ åŠ è½½æƒ…ç»ªæ•°æ®...")
        sentiment_data = processor.load_sentiment_data()
        print(f"   åŠ è½½äº† {len(sentiment_data.columns) if not sentiment_data.empty else 0} ä¸ªæƒ…ç»ªæŒ‡æ ‡")
        
        # åˆ›å»ºæ ·æœ¬
        print("ğŸ”§ åˆ›å»ºéæ—¶åºæ ·æœ¬...")
        X, y, codes, feature_names, dates = processor.create_daily_samples_with_feature_names(
            stock_data, sector_data, index_data, sentiment_data
        )
        
        print(f"   ç”Ÿæˆäº† {len(X)} ä¸ªæ ·æœ¬")
        print(f"   æ¯ä¸ªæ ·æœ¬åŒ…å« {len(feature_names)} ä¸ªç‰¹å¾")
        print(f"   æ—¥æœŸèŒƒå›´: {min(dates).strftime('%Y-%m-%d')} ~ {max(dates).strftime('%Y-%m-%d')}")
        
        # ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜æ•°æ®...")
        save_dir = processor.save_data_with_meaningful_names(X, y, codes, feature_names, dates)
        
        print("=" * 60)
        print("ğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: X{X.shape}")
        print(f"ğŸ”¢ ç‰¹å¾æ•°é‡: {len(feature_names)}")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {save_dir}")
        print(f"ğŸ¯ è¾“å‡ºæ ¼å¼: æ¯æ—¥CSVæ–‡ä»¶ï¼Œç¬¬1åˆ—ä¸ºstock_codeï¼Œå…¶ä½™ä¸ºç‰¹å¾åˆ—")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import sys
    
    config_path = None
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help':
            print("ç”¨æ³•: python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_éæ—¶åºç‰ˆ.py [é…ç½®æ–‡ä»¶è·¯å¾„]")
            print("ç¤ºä¾‹: python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_éæ—¶åºç‰ˆ.py config/datas_process/data_process_non_timeseries.yaml")
            print("")
            print("ç‰¹ç‚¹:")
            print("  - éæ—¶åºæ•°æ®ï¼šæ¯ä¸ªæ ·æœ¬åªåŒ…å«å½“å¤©çš„ç‰¹å¾")
            print("  - åˆ—ååŒ…å«æ—¥æœŸï¼šä¾¿äºè¯†åˆ«æ•°æ®æ—¶é—´ç‚¹")
            print("  - é¢„æµ‹æœªæ¥ï¼šé¢„æµ‹Nå¤©åçš„æ¶¨è·Œå¹…")
            sys.exit(0)
        else:
            config_path = sys.argv[1]
    
    main(config_path)