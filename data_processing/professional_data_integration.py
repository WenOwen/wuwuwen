#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šé‡‘èæ•°æ®æ•´åˆå¤„ç†è„šæœ¬
ä¸¥æ ¼æŒ‰ç…§é‡‘èå·¥ç¨‹è¦æ±‚ï¼š
1. è‚¡ç¥¨ä»£ç æ”¾ç¬¬ä¸€åˆ—
2. æ¯ä¸ªè¡Œä¸š/æ¦‚å¿µå›ºå®š3åˆ—ï¼šå½’å±æ ‡è¯† + æ¶¨è·Œå¹… + äº¤äº’ç‰¹å¾
3. åŠ è½½å…¨éƒ¨æ¦‚å¿µå’Œè¡Œä¸š
4. ä¸¥æ ¼çš„æ•°æ®ç±»å‹æ§åˆ¶
"""

import pandas as pd
import numpy as np
import os
import gc
from pathlib import Path
import warnings
from datetime import datetime
import re
from tqdm import tqdm
import json

warnings.filterwarnings('ignore')

class ProfessionalDataProcessor:
    def __init__(self, data_root='data', start_date=None, end_date=None, recent_days=None, skip_existing=True):
        """
        åˆå§‹åŒ–ä¸“ä¸šæ•°æ®å¤„ç†å™¨
        
        Args:
            data_root: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            start_date: å¼€å§‹æ—¥æœŸ (å­—ç¬¦ä¸²æ ¼å¼: 'YYYY-MM-DD' æˆ– datetime.date å¯¹è±¡)
            end_date: ç»“æŸæ—¥æœŸ (å­—ç¬¦ä¸²æ ¼å¼: 'YYYY-MM-DD' æˆ– datetime.date å¯¹è±¡)
            recent_days: åªå¤„ç†æœ€è¿‘å‡ ä¸ªäº¤æ˜“æ—¥ï¼ˆå¦‚æœä¸æŒ‡å®šstart_dateå’Œend_dateï¼‰
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œé¿å…é‡å¤å¤„ç†
        """
        self.data_root = Path(data_root)
        self.output_dir = self.data_root / 'professional_parquet'
        self.output_dir.mkdir(exist_ok=True)
        
        # æ—¥æœŸèŒƒå›´è®¾ç½®
        self.start_date = self._parse_date(start_date) if start_date else None
        self.end_date = self._parse_date(end_date) if end_date else None
        self.recent_days = recent_days
        self.skip_existing = skip_existing
        
        # æ•°æ®è·¯å¾„é…ç½®
        self.paths = {
            'stock_data': self.data_root / 'datas_em',
            'sector_mapping': self.data_root / 'datas_sector_historical' / 'è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv',
            'industry_data': self.data_root / 'datas_sector_historical' / 'è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²',
            'concept_data': self.data_root / 'datas_sector_historical' / 'æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²',
        }
        
        # å­˜å‚¨æ˜ å°„å…³ç³»å’Œå…¨é›†
        self.stock_mapping = {}
        self.industry_universe = []  # æ‰€æœ‰è¡Œä¸šçš„å›ºå®šå…¨é›†
        self.concept_universe = []   # æ‰€æœ‰æ¦‚å¿µçš„å›ºå®šå…¨é›†
        self.industry_pct_data = {}  # è¡Œä¸šæ¶¨è·Œå¹…æ•°æ®ï¼š{industry: {date: pct}}
        self.concept_pct_data = {}   # æ¦‚å¿µæ¶¨è·Œå¹…æ•°æ®ï¼š{concept: {date: pct}}
        
    def _parse_date(self, date_input):
        """è§£ææ—¥æœŸè¾“å…¥ï¼Œæ”¯æŒå­—ç¬¦ä¸²å’Œdateå¯¹è±¡"""
        if isinstance(date_input, str):
            try:
                return datetime.strptime(date_input, '%Y-%m-%d').date()
            except ValueError:
                print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_input}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
                return None
        elif hasattr(date_input, 'date'):
            return date_input.date()
        elif hasattr(date_input, 'year'):
            return date_input
        else:
            print(f"âŒ æ— æ³•è§£ææ—¥æœŸ: {date_input}")
            return None
        
    def load_stock_mapping(self):
        """åŠ è½½è‚¡ç¥¨æ˜ å°„è¡¨å¹¶å»ºç«‹å®Œæ•´çš„è¡Œä¸šæ¦‚å¿µå…¨é›†"""
        print("ğŸ“Š åŠ è½½è‚¡ç¥¨æ˜ å°„è¡¨å¹¶å»ºç«‹å®Œæ•´å…¨é›†...")
        try:
            mapping_df = pd.read_csv(self.paths['sector_mapping'], encoding='utf-8')
            
            # å»ºç«‹è¡Œä¸šå…¨é›†
            all_industries = set()
            for industry in mapping_df['æ‰€å±è¡Œä¸š'].fillna(''):
                if industry.strip():
                    all_industries.add(industry.strip())
            
            # å»ºç«‹æ¦‚å¿µå…¨é›†
            all_concepts = set()
            for concepts in mapping_df['æ¦‚å¿µæ¿å—'].fillna(''):
                if concepts.strip():
                    concept_list = [c.strip() for c in concepts.split(',') if c.strip()]
                    all_concepts.update(concept_list)
            
            # æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œç¡®ä¿ä¸€è‡´æ€§
            self.industry_universe = sorted(list(all_industries))
            self.concept_universe = sorted(list(all_concepts))
            
            print(f"   âœ… è¡Œä¸šå…¨é›†: {len(self.industry_universe)} ä¸ª")
            print(f"   âœ… æ¦‚å¿µå…¨é›†: {len(self.concept_universe)} ä¸ª")
            
            # æ„å»ºè‚¡ç¥¨æ˜ å°„å­—å…¸
            for _, row in mapping_df.iterrows():
                stock_code = row['è‚¡ç¥¨ä»£ç ']
                concepts = row['æ¦‚å¿µæ¿å—'] if pd.notna(row['æ¦‚å¿µæ¿å—']) else ''
                concept_list = [c.strip() for c in concepts.split(',') if c.strip()]
                
                self.stock_mapping[stock_code] = {
                    'name': row['è‚¡ç¥¨åç§°'],
                    'industry': row['æ‰€å±è¡Œä¸š'].strip() if pd.notna(row['æ‰€å±è¡Œä¸š']) else '',
                    'concepts': concept_list,
                }
                
            print(f"   âœ… è‚¡ç¥¨æ˜ å°„: {len(self.stock_mapping)} åªè‚¡ç¥¨")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½è‚¡ç¥¨æ˜ å°„è¡¨å¤±è´¥: {e}")
            return False
    
    def load_all_industry_data(self):
        """åŠ è½½æ‰€æœ‰è¡Œä¸šçš„æ¶¨è·Œå¹…æ•°æ®"""
        print("ğŸ­ åŠ è½½æ‰€æœ‰è¡Œä¸šæ¶¨è·Œå¹…æ•°æ®...")
        
        industry_files = list(self.paths['industry_data'].glob('*_daily_å†å²æ•°æ®.csv'))
        print(f"   å‘ç° {len(industry_files)} ä¸ªè¡Œä¸šæ•°æ®æ–‡ä»¶")
        
        loaded_industries = set()
        
        for industry_file in tqdm(industry_files, desc="åŠ è½½è¡Œä¸šæ•°æ®"):
            try:
                df = pd.read_csv(industry_file, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.date
                
                # è·å–è¡Œä¸šåç§°
                industry_name = df['æ¿å—åç§°'].iloc[0] if len(df) > 0 else None
                
                if industry_name and 'æ¶¨è·Œå¹…' in df.columns:
                    # æ„å»ºæ—¥æœŸ->æ¶¨è·Œå¹…çš„æ˜ å°„
                    date_pct_map = {}
                    for _, row in df.iterrows():
                        date = row['æ—¥æœŸ']
                        pct = row['æ¶¨è·Œå¹…'] / 100.0  # è½¬æ¢ä¸ºå°æ•°æ ¼å¼
                        date_pct_map[date] = pct
                    
                    self.industry_pct_data[industry_name] = date_pct_map
                    loaded_industries.add(industry_name)
                
            except Exception as e:
                print(f"   âš ï¸ å¤„ç† {industry_file} å¤±è´¥: {e}")
                continue
        
        print(f"   âœ… æˆåŠŸåŠ è½½ {len(loaded_industries)} ä¸ªè¡Œä¸šçš„æ¶¨è·Œå¹…æ•°æ®")
        
        # ä¸ºæ²¡æœ‰æ•°æ®çš„è¡Œä¸šå¡«å……0
        for industry in self.industry_universe:
            if industry not in self.industry_pct_data:
                self.industry_pct_data[industry] = {}
                print(f"   âš ï¸ è¡Œä¸š '{industry}' æ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œå°†ä½¿ç”¨0å¡«å……")
        
    def load_all_concept_data(self):
        """åŠ è½½æ‰€æœ‰æ¦‚å¿µçš„æ¶¨è·Œå¹…æ•°æ®"""
        print("ğŸ’¡ åŠ è½½æ‰€æœ‰æ¦‚å¿µæ¶¨è·Œå¹…æ•°æ®...")
        
        concept_files = list(self.paths['concept_data'].glob('*_daily_å†å²æ•°æ®.csv'))
        print(f"   å‘ç° {len(concept_files)} ä¸ªæ¦‚å¿µæ•°æ®æ–‡ä»¶")
        
        loaded_concepts = set()
        
        for concept_file in tqdm(concept_files, desc="åŠ è½½æ¦‚å¿µæ•°æ®"):
            try:
                df = pd.read_csv(concept_file, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.date
                
                # è·å–æ¦‚å¿µåç§°
                concept_name = df['æ¿å—åç§°'].iloc[0] if len(df) > 0 else None
                
                if concept_name and 'æ¶¨è·Œå¹…' in df.columns:
                    # æ„å»ºæ—¥æœŸ->æ¶¨è·Œå¹…çš„æ˜ å°„
                    date_pct_map = {}
                    for _, row in df.iterrows():
                        date = row['æ—¥æœŸ']
                        pct = row['æ¶¨è·Œå¹…'] / 100.0  # è½¬æ¢ä¸ºå°æ•°æ ¼å¼
                        date_pct_map[date] = pct
                    
                    self.concept_pct_data[concept_name] = date_pct_map
                    loaded_concepts.add(concept_name)
                
            except Exception as e:
                print(f"   âš ï¸ å¤„ç† {concept_file} å¤±è´¥: {e}")
                continue
        
        print(f"   âœ… æˆåŠŸåŠ è½½ {len(loaded_concepts)} ä¸ªæ¦‚å¿µçš„æ¶¨è·Œå¹…æ•°æ®")
        
        # ä¸ºæ²¡æœ‰æ•°æ®çš„æ¦‚å¿µå¡«å……0
        for concept in self.concept_universe:
            if concept not in self.concept_pct_data:
                self.concept_pct_data[concept] = {}
                print(f"   âš ï¸ æ¦‚å¿µ '{concept}' æ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œå°†ä½¿ç”¨0å¡«å……")
    
    def get_target_dates(self):
        """è·å–ç›®æ ‡å¤„ç†æ—¥æœŸèŒƒå›´"""
        # ä»å‡ ä¸ªæ ·æœ¬è‚¡ç¥¨ä¸­è·å–æ‰€æœ‰å¯ç”¨çš„äº¤æ˜“æ—¥æœŸ
        print("ğŸ” æ‰«æå¯ç”¨çš„äº¤æ˜“æ—¥æœŸ...")
        stock_files = list(self.paths['stock_data'].glob('*.csv'))[:20]
        
        all_dates = set()
        for stock_file in stock_files[:5]:
            try:
                df = pd.read_csv(stock_file, encoding='utf-8', usecols=['äº¤æ˜“æ—¥æœŸ'])
                dates = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ']).dt.date
                all_dates.update(dates)
            except:
                continue
        
        if not all_dates:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥æœŸ")
            return []
        
        # æ’åºæ‰€æœ‰æ—¥æœŸ
        sorted_dates = sorted(all_dates)
        
        # æ ¹æ®è®¾ç½®é€‰æ‹©æ—¥æœŸèŒƒå›´
        if self.start_date and self.end_date:
            # æŒ‡å®šäº†èµ·å§‹å’Œç»“æŸæ—¥æœŸ
            target_dates = [d for d in sorted_dates if self.start_date <= d <= self.end_date]
            print(f"   ğŸ“… æŒ‡å®šæ—¥æœŸèŒƒå›´: {self.start_date} åˆ° {self.end_date}")
            print(f"   ğŸ“Š å¯ç”¨äº¤æ˜“æ—¥: {len(target_dates)} ä¸ª")
        elif self.start_date:
            # åªæŒ‡å®šäº†å¼€å§‹æ—¥æœŸï¼Œå¤„ç†ä»å¼€å§‹æ—¥æœŸåˆ°æœ€æ–°çš„æ‰€æœ‰æ•°æ®
            target_dates = [d for d in sorted_dates if d >= self.start_date]
            print(f"   ğŸ“… ä»æŒ‡å®šæ—¥æœŸå¼€å§‹: {self.start_date} åˆ° {sorted_dates[-1]}")
            print(f"   ğŸ“Š å¯ç”¨äº¤æ˜“æ—¥: {len(target_dates)} ä¸ª")
        elif self.end_date:
            # åªæŒ‡å®šäº†ç»“æŸæ—¥æœŸï¼Œå¤„ç†åˆ°æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ•°æ®
            target_dates = [d for d in sorted_dates if d <= self.end_date]
            print(f"   ğŸ“… åˆ°æŒ‡å®šæ—¥æœŸç»“æŸ: {sorted_dates[0]} åˆ° {self.end_date}")
            print(f"   ğŸ“Š å¯ç”¨äº¤æ˜“æ—¥: {len(target_dates)} ä¸ª")
        elif self.recent_days:
            # ä½¿ç”¨æœ€è¿‘Nå¤©æ¨¡å¼
            target_dates = sorted_dates[-self.recent_days:]
            print(f"   ğŸ“… æœ€è¿‘ {self.recent_days} ä¸ªäº¤æ˜“æ—¥: {target_dates[0]} åˆ° {target_dates[-1]}")
        else:
            # å¦‚æœéƒ½æ²¡æŒ‡å®šï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰æ—¥æœŸ
            target_dates = sorted_dates
            print(f"   ğŸ“… å¤„ç†æ‰€æœ‰å¯ç”¨æ—¥æœŸ: {sorted_dates[0]} åˆ° {sorted_dates[-1]} (å…± {len(target_dates)} ä¸ª)")
        
        # å¦‚æœéœ€è¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if self.skip_existing:
            existing_files = set()
            for f in self.output_dir.glob('*.parquet'):
                try:
                    date_obj = datetime.strptime(f.stem, '%Y-%m-%d').date()
                    existing_files.add(date_obj)
                except:
                    continue
            
            original_count = len(target_dates)
            target_dates = [d for d in target_dates if d not in existing_files]
            skipped_count = original_count - len(target_dates)
            
            if skipped_count > 0:
                print(f"   â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {skipped_count} ä¸ª")
                print(f"   ğŸ¯ å®é™…éœ€è¦å¤„ç†: {len(target_dates)} ä¸ª")
        
        if not target_dates:
            if self.skip_existing:
                print("   âœ… æ‰€æœ‰ç›®æ ‡æ—¥æœŸçš„æ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†")
            else:
                print("   âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“æ—¥æœŸ")
            return []
        
        print(f"   ğŸ“ˆ æœ€ç»ˆå¤„ç†èŒƒå›´: {target_dates[0]} åˆ° {target_dates[-1]}")
        return target_dates
    
    def process_single_date(self, target_date):
        """å¤„ç†å•ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® - ä¸¥æ ¼æŒ‰ç…§ä¸“ä¸šæ ¼å¼"""
        print(f"   ğŸ“… å¤„ç†æ—¥æœŸ: {target_date}")
        
        stock_files = list(self.paths['stock_data'].glob('*.csv'))
        processed_stocks = []
        processed_count = 0
        
        # åªå¤„ç†å‰100åªè‚¡ç¥¨ä»¥åŠ é€Ÿæµ‹è¯•
        for stock_file in stock_files:  # ç§»é™¤[:100]é™åˆ¶
            try:
                stock_code = stock_file.stem
                if stock_code not in self.stock_mapping:
                    continue
                
                # è¯»å–è‚¡ç¥¨æ•°æ®
                df = pd.read_csv(stock_file, encoding='utf-8')
                df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ']).dt.date
                
                # ç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®
                date_df = df[df['äº¤æ˜“æ—¥æœŸ'] == target_date]
                
                if len(date_df) == 0:
                    continue
                
                # è·å–è‚¡ç¥¨ä¿¡æ¯
                stock_info = self.stock_mapping[stock_code]
                stock_industry = stock_info['industry']
                stock_concepts = stock_info['concepts']
                
                # æ„å»ºå•åªè‚¡ç¥¨çš„æ•°æ®è¡Œ
                stock_row = {}
                
                # 1. åŸºç¡€ä¿¡æ¯ (è‚¡ç¥¨ä»£ç æ”¾ç¬¬ä¸€åˆ—)
                stock_row['symbol'] = stock_code
                stock_row['name'] = stock_info['name']
                
                # 2. è‚¡ç¥¨äº¤æ˜“æ•°æ®
                for col in ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'æµé€šå¸‚å€¼']:
                    if col in date_df.columns:
                        stock_row[col] = date_df.iloc[0][col]
                
                # 3. ä¸ºæ¯ä¸ªè¡Œä¸šç”Ÿæˆå›ºå®šçš„3åˆ—
                for industry in self.industry_universe:
                    # 3.1 å½’å±æ ‡è¯† (uint8: 0æˆ–1)
                    stock_row[f'industry_{industry}'] = np.uint8(1 if industry == stock_industry else 0)
                    
                    # 3.2 è¡Œä¸šå½“æ—¥æ¶¨è·Œå¹… (float32: å°æ•°æ ¼å¼)
                    industry_pct = self.industry_pct_data.get(industry, {}).get(target_date, 0.0)
                    stock_row[f'industry_{industry}_pct'] = np.float32(industry_pct)
                    
                    # 3.3 äº¤äº’ç‰¹å¾ (float32: å½’å± Ã— æ¶¨è·Œå¹…)
                    interaction = stock_row[f'industry_{industry}'] * industry_pct
                    stock_row[f'industry_{industry}_x_ret'] = np.float32(interaction)
                
                # 4. ä¸ºæ¯ä¸ªæ¦‚å¿µç”Ÿæˆå›ºå®šçš„3åˆ—
                for concept in self.concept_universe:
                    # 4.1 å½’å±æ ‡è¯† (uint8: 0æˆ–1)
                    stock_row[f'concept_{concept}'] = np.uint8(1 if concept in stock_concepts else 0)
                    
                    # 4.2 æ¦‚å¿µå½“æ—¥æ¶¨è·Œå¹… (float32: å°æ•°æ ¼å¼)
                    concept_pct = self.concept_pct_data.get(concept, {}).get(target_date, 0.0)
                    stock_row[f'concept_{concept}_pct'] = np.float32(concept_pct)
                    
                    # 4.3 äº¤äº’ç‰¹å¾ (float32: å½’å± Ã— æ¶¨è·Œå¹…)
                    interaction = stock_row[f'concept_{concept}'] * concept_pct
                    stock_row[f'concept_{concept}_x_ret'] = np.float32(interaction)
                
                processed_stocks.append(stock_row)
                processed_count += 1
                
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†è‚¡ç¥¨ {stock_file.stem} å¤±è´¥: {e}")
                continue
        
        if not processed_stocks:
            print(f"      âš ï¸ æ—¥æœŸ {target_date} æ²¡æœ‰æ•°æ®")
            return False
        
        # è½¬æ¢ä¸ºDataFrame
        combined_df = pd.DataFrame(processed_stocks)
        
        # è®¾ç½®ç´¢å¼•
        combined_df = combined_df.set_index('symbol')
        
        # éªŒè¯åˆ—çš„é¡ºåºå’Œæ•°æ®ç±»å‹
        expected_col_count = 2 + 11 + len(self.industry_universe) * 3 + len(self.concept_universe) * 3  # name + 11ä¸ªè‚¡ç¥¨ç‰¹å¾ + è¡Œä¸š*3 + æ¦‚å¿µ*3
        print(f"      ğŸ“Š é¢„æœŸç‰¹å¾æ•°: {expected_col_count}, å®é™…ç‰¹å¾æ•°: {len(combined_df.columns)}")
        
        # ä¿å­˜ä¸ºParquetæ–‡ä»¶
        date_str = target_date.strftime('%Y-%m-%d')
        output_file = self.output_dir / f'{date_str}.parquet'
        combined_df.to_parquet(output_file, compression='snappy')
        
        print(f"      âœ… ä¿å­˜äº† {processed_count} åªè‚¡ç¥¨ï¼Œ{len(combined_df.columns)} ä¸ªç‰¹å¾")
        
        return True
    
    def generate_metadata(self):
        """ç”Ÿæˆè¯¦ç»†å…ƒæ•°æ®"""
        print("ğŸ“‹ ç”Ÿæˆä¸“ä¸šå…ƒæ•°æ®...")
        
        parquet_files = list(self.output_dir.glob('*.parquet'))
        
        if not parquet_files:
            print("   âŒ æ²¡æœ‰æ‰¾åˆ°ç”Ÿæˆçš„Parquetæ–‡ä»¶")
            return
        
        # åˆ†ææ ·æœ¬æ–‡ä»¶
        sample_file = parquet_files[0]
        sample_df = pd.read_parquet(sample_file)
        
        # éªŒè¯ç‰¹å¾ç»“æ„
        stock_features = ['name'] + [col for col in sample_df.columns if col in ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'æµé€šå¸‚å€¼']]
        
        industry_features = {
            'indicators': [col for col in sample_df.columns if col.startswith('industry_') and not col.endswith('_pct') and not col.endswith('_x_ret')],
            'pct': [col for col in sample_df.columns if col.startswith('industry_') and col.endswith('_pct')],
            'interaction': [col for col in sample_df.columns if col.startswith('industry_') and col.endswith('_x_ret')]
        }
        
        concept_features = {
            'indicators': [col for col in sample_df.columns if col.startswith('concept_') and not col.endswith('_pct') and not col.endswith('_x_ret')],
            'pct': [col for col in sample_df.columns if col.startswith('concept_') and col.endswith('_pct')],
            'interaction': [col for col in sample_df.columns if col.startswith('concept_') and col.endswith('_x_ret')]
        }
        
        metadata = {
            'version': 'professional_v1.0',
            'processing_standard': 'financial_engineering',
            'total_files': len(parquet_files),
            'date_range': f"{min(f.stem for f in parquet_files)} åˆ° {max(f.stem for f in parquet_files)}",
            'sample_stocks_count': len(sample_df),
            'total_columns': len(sample_df.columns),
            
            'feature_structure': {
                'stock_basic': len(stock_features),
                'industry_total': len(industry_features['indicators']) + len(industry_features['pct']) + len(industry_features['interaction']),
                'industry_breakdown': {
                    'indicators': len(industry_features['indicators']),
                    'pct': len(industry_features['pct']),
                    'interaction': len(industry_features['interaction'])
                },
                'concept_total': len(concept_features['indicators']) + len(concept_features['pct']) + len(concept_features['interaction']),
                'concept_breakdown': {
                    'indicators': len(concept_features['indicators']),
                    'pct': len(concept_features['pct']),
                    'interaction': len(concept_features['interaction'])
                }
            },
            
            'data_universe': {
                'industry_count': len(self.industry_universe),
                'concept_count': len(self.concept_universe),
                'industry_list': self.industry_universe,
                'concept_list': self.concept_universe
            },
            
            'data_types': {
                'symbol': 'string (index)',
                'name': 'string',
                'stock_features': 'float64',
                'industry_indicators': 'uint8 (0/1)',
                'industry_pct': 'float32 (decimal)',
                'industry_interaction': 'float32 (decimal)',
                'concept_indicators': 'uint8 (0/1)',
                'concept_pct': 'float32 (decimal)',
                'concept_interaction': 'float32 (decimal)'
            },
            
            'quality_check': {
                'expected_industry_triplets': len(self.industry_universe),
                'actual_industry_triplets': len(industry_features['indicators']),
                'expected_concept_triplets': len(self.concept_universe),
                'actual_concept_triplets': len(concept_features['indicators']),
                'industry_complete': len(industry_features['indicators']) == len(self.industry_universe),
                'concept_complete': len(concept_features['indicators']) == len(self.concept_universe)
            },
            
            'created_at': datetime.now().isoformat(),
            'notes': [
                'è‚¡ç¥¨ä»£ç (symbol)ä½œä¸ºç¬¬ä¸€åˆ—å’Œç´¢å¼•',
                'åˆ é™¤äº†åœ°åŒºæ•°æ®',
                'æ¯ä¸ªè¡Œä¸š/æ¦‚å¿µä¸¥æ ¼3åˆ—ï¼šå½’å±æ ‡è¯†+æ¶¨è·Œå¹…+äº¤äº’ç‰¹å¾',
                'æ¶¨è·Œå¹…å·²è½¬æ¢ä¸ºå°æ•°æ ¼å¼ (2.3% â†’ 0.023)',
                'æ•°æ®ç±»å‹ç»è¿‡ä¼˜åŒ–ï¼šuint8ç”¨äº0/1æ ‡è¯†ï¼Œfloat32ç”¨äºæ•°å€¼ç‰¹å¾'
            ]
        }
        
        with open(self.output_dir / 'professional_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # è¾“å‡ºè´¨é‡æŠ¥å‘Š
        print(f"   âœ… è´¨é‡æ£€æŸ¥æŠ¥å‘Šï¼š")
        print(f"      è¡Œä¸šä¸‰å…ƒç»„: {metadata['quality_check']['actual_industry_triplets']}/{metadata['quality_check']['expected_industry_triplets']} ({'âœ…' if metadata['quality_check']['industry_complete'] else 'âŒ'})")
        print(f"      æ¦‚å¿µä¸‰å…ƒç»„: {metadata['quality_check']['actual_concept_triplets']}/{metadata['quality_check']['expected_concept_triplets']} ({'âœ…' if metadata['quality_check']['concept_complete'] else 'âŒ'})")
        print(f"      æ€»ç‰¹å¾æ•°: {metadata['total_columns']}")
        
    def run_professional_integration(self):
        """è¿è¡Œä¸“ä¸šæ•°æ®æ•´åˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹ä¸“ä¸šé‡‘èæ•°æ®æ•´åˆå¤„ç†...")
        print("=" * 60)
        
        # 1. åŠ è½½è‚¡ç¥¨æ˜ å°„è¡¨å¹¶å»ºç«‹å®Œæ•´å…¨é›†
        if not self.load_stock_mapping():
            return False
        
        # 2. åŠ è½½æ‰€æœ‰è¡Œä¸šå’Œæ¦‚å¿µçš„å¸‚åœºæ•°æ®
        self.load_all_industry_data()
        self.load_all_concept_data()
        
        # 3. è·å–ç›®æ ‡å¤„ç†æ—¥æœŸ
        target_dates = self.get_target_dates()
        if not target_dates:
            return False
        
        # 4. å¤„ç†æ¯ä¸ªæ—¥æœŸ
        success_count = 0
        for date in target_dates:
            if self.process_single_date(date):
                success_count += 1
        
        # 5. ç”Ÿæˆä¸“ä¸šå…ƒæ•°æ®
        self.generate_metadata()
        
        print("=" * 60)
        print(f"âœ… ä¸“ä¸šæ•°æ®æ•´åˆå®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(target_dates)} ä¸ªäº¤æ˜“æ—¥")
        
        return success_count > 0

def create_processor_examples():
    """æä¾›ä¸åŒä½¿ç”¨åœºæ™¯çš„å¤„ç†å™¨åˆ›å»ºç¤ºä¾‹"""
    examples = {
        "ç»§ç»­å¤„ç†å†å²æ•°æ®": {
            "description": "å¦‚æœå·²å¤„ç†äº†æœ€è¿‘100å¤©ï¼Œç°åœ¨æƒ³å¤„ç†æ›´æ—©çš„æ•°æ®",
            "code": """
processor = ProfessionalDataProcessor(
    start_date='2015-01-01',    # ä»2015å¹´å¼€å§‹
    end_date='2023-01-01',      # åˆ°2023å¹´ç»“æŸ
    skip_existing=True          # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
)
            """.strip()
        },
        
        "è¡¥å……ç‰¹å®šå¹´ä»½æ•°æ®": {
            "description": "åªå¤„ç†æŸä¸ªç‰¹å®šå¹´ä»½çš„æ•°æ®",
            "code": """
processor = ProfessionalDataProcessor(
    start_date='2022-01-01',    # 2022å¹´å¼€å§‹
    end_date='2022-12-31',      # 2022å¹´ç»“æŸ
    skip_existing=True
)
            """.strip()
        },
        
        "ä»æŸæ—¥æœŸå¼€å§‹åˆ°æœ€æ–°": {
            "description": "ä»æŒ‡å®šæ—¥æœŸå¼€å§‹å¤„ç†åˆ°æœ€æ–°æ•°æ®",
            "code": """
processor = ProfessionalDataProcessor(
    start_date='2020-01-01',    # ä»2020å¹´å¼€å§‹
    skip_existing=True          # ä¼šå¤„ç†åˆ°æœ€æ–°æ—¥æœŸ
)
            """.strip()
        },
        
        "å¤„ç†æœ€è¿‘Nå¤©": {
            "description": "åªå¤„ç†æœ€è¿‘çš„Nä¸ªäº¤æ˜“æ—¥",
            "code": """
processor = ProfessionalDataProcessor(
    recent_days=50,             # æœ€è¿‘50ä¸ªäº¤æ˜“æ—¥
    skip_existing=True
)
            """.strip()
        }
    }
    
    print("ğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 60)
    for title, example in examples.items():
        print(f"\nğŸ”¹ {title}")
        print(f"   è¯´æ˜: {example['description']}")
        print(f"   ä»£ç :")
        for line in example['code'].split('\n'):
            print(f"   {line}")
    print("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ’¼ ä¸“ä¸šé‡‘èæ•°æ®æ•´åˆå¤„ç†å™¨")
    print("ğŸ¯ ä¸¥æ ¼æŒ‰ç…§é‡‘èå·¥ç¨‹æ ‡å‡†:")
    print("   âœ… è‚¡ç¥¨ä»£ç ç¬¬ä¸€åˆ—")
    print("   âœ… æ¯ä¸ªè¡Œä¸š/æ¦‚å¿µå›ºå®š3åˆ—")
    print("   âœ… å½’å±æ ‡è¯†(uint8) + æ¶¨è·Œå¹…(float32) + äº¤äº’ç‰¹å¾(float32)")
    print("   âœ… æ¶¨è·Œå¹…è½¬æ¢ä¸ºå°æ•°æ ¼å¼")
    print("   âœ… åŠ è½½å…¨éƒ¨è¡Œä¸šå’Œæ¦‚å¿µ")
    print()
    
    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    create_processor_examples()
    
    # å½“å‰è®¾ç½®ï¼šç»§ç»­å¤„ç†å†å²æ•°æ®ï¼ˆä½ å·²ç»æœ‰100å¤©çš„æ•°æ®äº†ï¼‰
    print("\nğŸš€ å½“å‰è¿è¡Œæ¨¡å¼ï¼šç»§ç»­å¤„ç†å†å²æ•°æ®")
    print("   ğŸ’¡ å¦‚éœ€ä¿®æ”¹æ—¥æœŸèŒƒå›´ï¼Œè¯·ç¼–è¾‘ä¸‹é¢çš„ä»£ç ")
    print()
    
    # æ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´è¿™é‡Œçš„æ—¥æœŸ
    processor = ProfessionalDataProcessor(
        start_date='2024-09-01',  # ä»2015å¹´å¼€å§‹ï¼ˆä½ å¯ä»¥æ”¹è¿™ä¸ªæ—¥æœŸï¼‰
        end_date='2025-01-01',    # åˆ°2024å¹´ç»“æŸï¼ˆä½ å¯ä»¥æ”¹è¿™ä¸ªæ—¥æœŸï¼‰
        skip_existing=True        # ä¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
    )
    
    # å…¶ä»–å¯é€‰æ¨¡å¼ï¼ˆå–æ¶ˆæ³¨é‡Šå³å¯ä½¿ç”¨ï¼‰ï¼š
    
    # 1. å¤„ç†ç‰¹å®šå¹´ä»½
    # processor = ProfessionalDataProcessor(
    #     start_date='2022-01-01',
    #     end_date='2022-12-31',
    #     skip_existing=True
    # )
    
    # 2. ä»æŸæ—¥æœŸå¼€å§‹åˆ°æœ€æ–°
    # processor = ProfessionalDataProcessor(
    #     start_date='2020-01-01',
    #     skip_existing=True
    # )
    
    # 3. ç»§ç»­å¤„ç†æœ€è¿‘Nå¤©
    # processor = ProfessionalDataProcessor(
    #     recent_days=200,
    #     skip_existing=True
    # )
    
    success = processor.run_professional_integration()
    
    if success:
        print("\nğŸ‰ ä¸“ä¸šå¤„ç†æˆåŠŸï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {processor.output_dir}")
        print("ğŸ“‹ æ•°æ®æ ¼å¼:")
        print("   symbol | name | è‚¡ç¥¨ç‰¹å¾ | industry_XXX | industry_XXX_pct | industry_XXX_x_ret | concept_XXX | concept_XXX_pct | concept_XXX_x_ret | ...")
        print()
        
        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®ç»“æ„
        parquet_files = list(processor.output_dir.glob('*.parquet'))
        if parquet_files:
            sample_file = parquet_files[0]
            df = pd.read_parquet(sample_file)
            print(f"ğŸ“Š æ ·æœ¬æ–‡ä»¶: {sample_file.name}")
            print(f"   ğŸ“ˆ è‚¡ç¥¨æ•°é‡: {len(df)}")
            print(f"   ğŸ“‹ ç‰¹å¾æ•°é‡: {len(df.columns)}")
            print(f"   ğŸ¯ è¡Œä¸šç‰¹å¾: {len([c for c in df.columns if c.startswith('industry_')])} ä¸ª")
            print(f"   ğŸ¯ æ¦‚å¿µç‰¹å¾: {len([c for c in df.columns if c.startswith('concept_')])} ä¸ª")
            
        print("\nâœ… æ•°æ®ç¬¦åˆä¸“ä¸šé‡‘èå»ºæ¨¡æ ‡å‡†ï¼")
    else:
        print("\nâŒ ä¸“ä¸šå¤„ç†å¤±è´¥ï¼")

if __name__ == "__main__":
    main()