#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è·å–çœŸå®çš„æ¿å—æ•°æ®å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
åŸºäºåŸå§‹çš„2.10è·å–æ¿å—æ•°æ®.pyï¼Œå¢åŠ CSVä¿å­˜åŠŸèƒ½
"""

import os
import sys
import pandas as pd
from datetime import datetime
import logging
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'core'))

# å¯¼å…¥é‡åŒ–å‡½æ•°
try:
    from samequant_functions import Spider_func
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥samequant_functions: {str(e)}")
    print("è¯·ç¡®ä¿samequant_functions.pyåœ¨é¡¹ç›®æ ¹ç›®å½•")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_real_sector_data():
    """è·å–çœŸå®çš„æ¿å—æ•°æ®å¹¶ä¿å­˜"""
    logger.info("ğŸš€ å¼€å§‹è·å–çœŸå®æ¿å—æ•°æ®...")
    
    # åˆå§‹åŒ–Spider_funcå®ä¾‹
    s_f_1 = Spider_func()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = "data/datas_sector"
    os.makedirs(save_dir, exist_ok=True)
    
    # 1. è·å–è¡Œä¸šæ¿å—æ•°æ®
    logger.info("ğŸ“Š è·å–è¡Œä¸šæ¿å—æ•°æ®...")
    try:
        df_industry = s_f_1.get_industry_data_from_eastmoney(sort_field='f3')
        if not df_industry.empty:
            industry_file = os.path.join(save_dir, "è¡Œä¸šæ¿å—æ•°æ®.csv")
            df_industry.to_csv(industry_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… è¡Œä¸šæ¿å—æ•°æ®å·²ä¿å­˜: {industry_file} ({len(df_industry)}è¡Œ)")
            print("æ¶¨å¹…æ’è¡Œå‰10çš„è¡Œä¸š:")
            print(df_industry[['è¡Œä¸šåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'æˆäº¤é¢', 'æ€»å¸‚å€¼']].head(10))
        else:
            logger.warning("âš ï¸ è¡Œä¸šæ¿å—æ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
        df_industry = pd.DataFrame()

    # 2. è·å–æ¦‚å¿µæ¿å—æ•°æ®ï¼ˆå®Œæ•´çš„438ä¸ªæ¦‚å¿µæ¿å—ï¼‰
    logger.info("ğŸ“Š è·å–å®Œæ•´æ¦‚å¿µæ¿å—æ•°æ®...")
    try:
        df_concept = s_f_1.get_concept_data_from_eastmoney(sort_field='f3')
        if not df_concept.empty:
            concept_file = os.path.join(save_dir, "æ¦‚å¿µæ¿å—æ•°æ®.csv")
            df_concept.to_csv(concept_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… æ¦‚å¿µæ¿å—æ•°æ®å·²ä¿å­˜: {concept_file} ({len(df_concept)}è¡Œ)")
            logger.info(f"ğŸ“Š æˆåŠŸè·å–å®Œæ•´çš„ {len(df_concept)} ä¸ªæ¦‚å¿µæ¿å—ï¼ˆé¢„æœŸ438ä¸ªï¼‰")
            print("æ¶¨å¹…æ’è¡Œå‰10çš„æ¦‚å¿µ:")
            print(df_concept[['æ¦‚å¿µåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'æˆäº¤é¢', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼']].head(10))
            
            # æ˜¾ç¤ºè·å–ç»Ÿè®¡
            up_count = len(df_concept[df_concept['æ¶¨è·Œå¹…'] > 0])
            down_count = len(df_concept[df_concept['æ¶¨è·Œå¹…'] < 0])
            flat_count = len(df_concept[df_concept['æ¶¨è·Œå¹…'] == 0])
            logger.info(f"ğŸ“ˆ æ¦‚å¿µæ¿å—è¡¨ç°: ä¸Šæ¶¨{up_count}ä¸ª, ä¸‹è·Œ{down_count}ä¸ª, å¹³ç›˜{flat_count}ä¸ª")
        else:
            logger.warning("âš ï¸ æ¦‚å¿µæ¿å—æ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–æ¦‚å¿µæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
        df_concept = pd.DataFrame()

    # 3. è·å–çƒ­é—¨æ¦‚å¿µæ’è¡Œ
    logger.info("ğŸ“Š è·å–çƒ­é—¨æ¦‚å¿µæ’è¡Œ...")
    try:
        df_hot_concepts = s_f_1.get_hot_concepts_from_eastmoney(limit=50)  # å¢åŠ åˆ°50ä¸ª
        if not df_hot_concepts.empty:
            hot_concepts_file = os.path.join(save_dir, "çƒ­é—¨æ¦‚å¿µæ’è¡Œ.csv")
            df_hot_concepts.to_csv(hot_concepts_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… çƒ­é—¨æ¦‚å¿µæ’è¡Œå·²ä¿å­˜: {hot_concepts_file} ({len(df_hot_concepts)}è¡Œ)")
            print("çƒ­é—¨æ¦‚å¿µæ’è¡Œå‰20:")
            print(df_hot_concepts[['æ¦‚å¿µåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'ä¸Šæ¶¨å®¶æ•°', 'ä¸‹è·Œå®¶æ•°']].head(20))
        else:
            logger.warning("âš ï¸ çƒ­é—¨æ¦‚å¿µæ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–çƒ­é—¨æ¦‚å¿µæ•°æ®å¤±è´¥: {str(e)}")
        df_hot_concepts = pd.DataFrame()

    return df_industry, df_concept, df_hot_concepts

def load_all_stock_list():
    """ä»all_stock_list.csvåŠ è½½å®Œæ•´çš„è‚¡ç¥¨åˆ—è¡¨"""
    logger.info("ğŸ“Š åŠ è½½å®Œæ•´è‚¡ç¥¨åˆ—è¡¨...")
    
    stock_list_paths = [
        "data/stockcode_list/all_stock_list.csv",
        "../data/stockcode_list/all_stock_list.csv",
        "stockcode_list/all_stock_list.csv",
        "all_stock_list.csv"
    ]
    
    stock_list_file = None
    for path in stock_list_paths:
        if os.path.exists(path):
            stock_list_file = path
            break
    
    if not stock_list_file:
        logger.error("âŒ æœªæ‰¾åˆ° all_stock_list.csv æ–‡ä»¶")
        return []
    
    try:
        df_stocks = pd.read_csv(stock_list_file, encoding='utf-8-sig')
        logger.info(f"âœ… æˆåŠŸåŠ è½½è‚¡ç¥¨åˆ—è¡¨: {stock_list_file}")
        logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {len(df_stocks)}")
        
        # ç­›é€‰æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨
        if 'ä¸Šå¸‚çŠ¶æ€' in df_stocks.columns:
            active_stocks = df_stocks[df_stocks['ä¸Šå¸‚çŠ¶æ€'] == 'æ­£å¸¸äº¤æ˜“']
            logger.info(f"ğŸ“Š æ­£å¸¸äº¤æ˜“è‚¡ç¥¨æ•°: {len(active_stocks)}")
        else:
            active_stocks = df_stocks
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¸Šå¸‚çŠ¶æ€åˆ—ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨")
        
        # æå–è‚¡ç¥¨ä»£ç 
        stock_codes = active_stocks['è‚¡ç¥¨ä»£ç '].tolist()
        logger.info(f"ğŸ“Š å°†è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„æ¿å—ä¿¡æ¯")
        
        return stock_codes
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []

def get_latest_data_date(file_path):
    """
    è·å–æ•°æ®æ–‡ä»¶ä¸­æœ€æ–°çš„æ—¥æœŸ
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DDæ ¼å¼) æˆ– None
    """
    if not os.path.exists(file_path):
        return None
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        if len(df) == 0:
            return None
        
        # æŸ¥æ‰¾æ—¥æœŸåˆ—
        date_columns = ['æ—¥æœŸ', 'date', 'æ—¶é—´', 'datetime']
        date_col = None
        for col in date_columns:
            if col in df.columns:
                date_col = col
                break
        
        if date_col is None:
            logger.warning(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸­æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
            return None
        
        # è·å–æœ€æ–°æ—¥æœŸ
        latest_date = df[date_col].max()
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        if pd.notna(latest_date):
            if isinstance(latest_date, str):
                return latest_date
            else:
                return latest_date.strftime('%Y-%m-%d')
        
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶ {file_path} æœ€æ–°æ—¥æœŸå¤±è´¥: {str(e)}")
        return None

def check_stock_mapping_update_needed():
    """
    æ£€æŸ¥è‚¡ç¥¨æ¿å—æ˜ å°„æ˜¯å¦éœ€è¦æ›´æ–°
    
    Returns:
        tuple: (éœ€è¦æ›´æ–°, æœ€æ–°æ—¥æœŸ, è¯´æ˜ä¿¡æ¯)
    """
    mapping_file = "data/datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv"
    
    if not os.path.exists(mapping_file):
        return True, None, "æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å…¨é‡è·å–"
    
    try:
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_mtime = os.path.getmtime(mapping_file)
        file_date = datetime.fromtimestamp(file_mtime)
        current_date = datetime.now()
        days_diff = (current_date - file_date).days
        
        if days_diff >= 7:  # è¶…è¿‡7å¤©åˆ™å»ºè®®æ›´æ–°
            return True, file_date.strftime('%Y-%m-%d'), f"æ–‡ä»¶å·²{days_diff}å¤©æœªæ›´æ–°ï¼Œå»ºè®®æ›´æ–°"
        else:
            return False, file_date.strftime('%Y-%m-%d'), f"æ–‡ä»¶è¾ƒæ–°ï¼ˆ{days_diff}å¤©å‰æ›´æ–°ï¼‰ï¼Œæ— éœ€æ›´æ–°"
            
    except Exception as e:
        logger.warning(f"âš ï¸ æ£€æŸ¥æ˜ å°„æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(e)}")
        return True, None, "æ— æ³•æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼Œå»ºè®®æ›´æ–°"

def get_stock_sector_mapping(max_stocks=None, start_from=0, batch_size=50, incremental=False):
    """
    è·å–è‚¡ç¥¨çš„æ¿å—æ˜ å°„ä¿¡æ¯
    
    Args:
        max_stocks: æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
        start_from: ä»ç¬¬å‡ åªè‚¡ç¥¨å¼€å§‹å¤„ç†ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        incremental: æ˜¯å¦ä¸ºå¢é‡æ›´æ–°æ¨¡å¼
    """
    logger.info("ğŸ“Š è·å–è‚¡ç¥¨æ¿å—æ˜ å°„ä¿¡æ¯...")
    
    # å¢é‡æ›´æ–°æ¨¡å¼æ£€æŸ¥
    if incremental:
        needs_update, last_date, message = check_stock_mapping_update_needed()
        logger.info(f"ğŸ” å¢é‡æ›´æ–°æ£€æŸ¥: {message}")
        
        if not needs_update:
            logger.info("âœ… è‚¡ç¥¨æ˜ å°„æ•°æ®è¾ƒæ–°ï¼Œè·³è¿‡æ›´æ–°")
            mapping_file = "data/datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv"
            if os.path.exists(mapping_file):
                return pd.read_csv(mapping_file, encoding='utf-8-sig')
            
        logger.info("ğŸ”„ å¯åŠ¨å¢é‡æ›´æ–°æ¨¡å¼")
    
    # åˆå§‹åŒ–Spider_funcå®ä¾‹
    s_f_1 = Spider_func()
    
    # ä»all_stock_list.csvåŠ è½½è‚¡ç¥¨ä»£ç 
    all_stock_codes = load_all_stock_list()
    
    if not all_stock_codes:
        logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤ä»£ç ")
        # ä½¿ç”¨ä¸€äº›å¸¸è§çš„è‚¡ç¥¨ä»£ç ä½œä¸ºç¤ºä¾‹
        all_stock_codes = [
            'sh600519', 'sz000001', 'sz000002', 'sh600000', 'sz000858', 
            'sh600036', 'sz000858', 'sh600276', 'sz002594', 'sh601318',
            'sh600309', 'sz000063', 'sh603259', 'sz002415', 'sh600887'
        ]
    
    # ç¡®å®šå¤„ç†èŒƒå›´
    end_index = min(start_from + max_stocks, len(all_stock_codes)) if max_stocks else len(all_stock_codes)
    stock_codes = all_stock_codes[start_from:end_index]
    
    logger.info(f"ğŸ“Š å¤„ç†èŒƒå›´: {start_from+1} - {end_index} / {len(all_stock_codes)} åªè‚¡ç¥¨")
    logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡å¤„ç†: {len(stock_codes)} åªè‚¡ç¥¨")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ä¸­é—´ç»“æœ
    temp_file = "data/datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    existing_data = []
    if os.path.exists(temp_file):
        try:
            df_existing = pd.read_csv(temp_file, encoding='utf-8-sig')
            existing_data = df_existing.to_dict('records')
            existing_codes = set(df_existing['è‚¡ç¥¨ä»£ç '].tolist())
            # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„è‚¡ç¥¨
            stock_codes = [code for code in stock_codes if code not in existing_codes]
            logger.info(f"ğŸ“Š å‘ç°ä¸­é—´ç»“æœï¼Œå·²å¤„ç† {len(existing_data)} åªè‚¡ç¥¨")
            logger.info(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†: {len(stock_codes)} åªè‚¡ç¥¨")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ä¸­é—´ç»“æœå¤±è´¥: {str(e)}")
            existing_data = []
    
    # è·å–è‚¡ç¥¨æ¿å—ä¿¡æ¯
    stock_sector_mapping = existing_data.copy()  # åŒ…å«å·²æœ‰æ•°æ®
    successful_count = len(existing_data)
    batch_count = 0
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("data/datas_sector_historical", exist_ok=True)
    
    logger.info(f"ğŸš€ å¼€å§‹è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„æ¿å—ä¿¡æ¯...")
    logger.info(f"ğŸ’¾ æ¯ {batch_size} åªè‚¡ç¥¨ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ")
    
    for i, stock_code in enumerate(stock_codes, 1):
        try:
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0:
                progress = (i / len(stock_codes)) * 100
                logger.info(f"ğŸ“Š è¿›åº¦: {i}/{len(stock_codes)} ({progress:.1f}%) - æˆåŠŸ: {successful_count}")
            
            # è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
            stock_info = None
            for retry in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                try:
                    stock_info = s_f_1.get_stock_industry_info_from_eastmoney(stock_code=stock_code)
                    if stock_info:
                        break
                except Exception as retry_e:
                    if retry == 2:  # æœ€åä¸€æ¬¡é‡è¯•
                        logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} é‡è¯•3æ¬¡åä»å¤±è´¥: {str(retry_e)}")
                    else:
                        time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            
            if stock_info:
                mapping_info = {
                    'è‚¡ç¥¨ä»£ç ': stock_code,
                    'è‚¡ç¥¨åç§°': stock_info.get('è‚¡ç¥¨åç§°', ''),
                    'æ‰€å±è¡Œä¸š': stock_info.get('æ‰€å±è¡Œä¸š', ''),
                    'æ¦‚å¿µæ¿å—': stock_info.get('æ¦‚å¿µæ¿å—', ''),
                    'åœ°åŒº': stock_info.get('åœ°åŒº', '')
                }
                stock_sector_mapping.append(mapping_info)
                successful_count += 1
                
                # æ˜¾ç¤ºå‰10ä¸ªæˆåŠŸçš„ç»“æœ
                if successful_count <= 10:
                    print(f"{stock_info['è‚¡ç¥¨åç§°']}({stock_code}):")
                    print(f"  æ‰€å±è¡Œä¸š: {stock_info['æ‰€å±è¡Œä¸š']}")
                    print(f"  æ¦‚å¿µæ¿å—: {stock_info['æ¦‚å¿µæ¿å—']}")
                    if stock_info.get('åœ°åŒº'):
                        print(f"  åœ°åŒº: {stock_info['åœ°åŒº']}")
                    print()
            else:
                logger.warning(f"âš ï¸ æ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„ä¿¡æ¯")
            
            # æ‰¹æ¬¡ä¿å­˜ä¸­é—´ç»“æœ
            batch_count += 1
            if batch_count >= batch_size:
                try:
                    df_temp = pd.DataFrame(stock_sector_mapping)
                    df_temp.to_csv(temp_file, index=False, encoding='utf-8-sig')
                    logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ: {len(stock_sector_mapping)} æ¡è®°å½•")
                    batch_count = 0
                except Exception as save_e:
                    logger.error(f"âŒ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {str(save_e)}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚å¤ªé¢‘ç¹
            time.sleep(0.1)
            
            # æ¯å¤„ç†100ä¸ªè‚¡ç¥¨æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ç»Ÿè®¡
            if i % 100 == 0:
                success_rate = (successful_count / i) * 100
                logger.info(f"ğŸ“Š ä¸­æœŸç»Ÿè®¡: æˆåŠŸç‡ {success_rate:.1f}% ({successful_count}/{i})")
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨ {stock_code} ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # æœ€ç»ˆä¿å­˜ï¼ˆåŒ…å«æœ€åä¸€æ‰¹æ•°æ®ï¼‰
    if batch_count > 0 and stock_sector_mapping:
        try:
            df_temp = pd.DataFrame(stock_sector_mapping)
            df_temp.to_csv(temp_file, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆä¸­é—´ç»“æœ: {len(stock_sector_mapping)} æ¡è®°å½•")
        except Exception as save_e:
            logger.error(f"âŒ ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {str(save_e)}")
    
    # ä¿å­˜è‚¡ç¥¨æ¿å—æ˜ å°„
    if stock_sector_mapping:
        df_stock_mapping = pd.DataFrame(stock_sector_mapping)
        
        # ä¿å­˜å®Œæ•´æ˜ å°„
        save_dir = "data/datas_sector_historical"
        os.makedirs(save_dir, exist_ok=True)
        mapping_file = os.path.join(save_dir, "è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv")
        df_stock_mapping.to_csv(mapping_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"âœ… è‚¡ç¥¨æ¿å—æ˜ å°„å·²ä¿å­˜: {mapping_file} ({len(df_stock_mapping)}è¡Œ)")
        logger.info(f"ğŸ“Š æˆåŠŸè·å–ä¿¡æ¯çš„è‚¡ç¥¨: {successful_count}/{len(all_stock_codes[start_from:end_index])} ({successful_count/(len(all_stock_codes[start_from:end_index]))*100:.1f}%)")
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
                logger.info("ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶")
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return df_stock_mapping
    
    return pd.DataFrame()

def create_sector_summary():
    """åˆ›å»ºæ¿å—æ•°æ®æ‘˜è¦"""
    logger.info("ğŸ“Š åˆ›å»ºæ¿å—æ•°æ®æ‘˜è¦...")
    
    save_dir = "data/datas_sector"
    if not os.path.exists(save_dir):
        logger.error("âŒ data/datas_sectorç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è·å–")
        return
    
    summary = {
        'æ•°æ®è·å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'æ–‡ä»¶åˆ—è¡¨': []
    }
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    csv_files = [f for f in os.listdir(save_dir) if f.endswith('.csv')]
    
    for csv_file in csv_files:
        file_path = os.path.join(save_dir, csv_file)
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            file_info = {
                'æ–‡ä»¶å': csv_file,
                'è¡Œæ•°': len(df),
                'åˆ—æ•°': len(df.columns),
                'æ–‡ä»¶å¤§å°': f"{os.path.getsize(file_path) / 1024:.1f} KB"
            }
            summary['æ–‡ä»¶åˆ—è¡¨'].append(file_info)
            logger.info(f"âœ… {csv_file}: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
    
    # ä¿å­˜æ‘˜è¦
    summary_file = os.path.join(save_dir, "æ•°æ®æ‘˜è¦.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("æ¿å—æ•°æ®è·å–æ‘˜è¦\n")
        f.write("=" * 50 + "\n")
        f.write(f"è·å–æ—¶é—´: {summary['æ•°æ®è·å–æ—¶é—´']}\n\n")
        f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
        for file_info in summary['æ–‡ä»¶åˆ—è¡¨']:
            f.write(f"- {file_info['æ–‡ä»¶å']}: {file_info['è¡Œæ•°']}è¡Œ, {file_info['æ–‡ä»¶å¤§å°']}\n")
    
    logger.info(f"âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜: {summary_file}")

def check_sector_data_update_needed(sector_name, sector_code, sector_type):
    """
    æ£€æŸ¥æ¿å—æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
    
    Args:
        sector_name: æ¿å—åç§°
        sector_code: æ¿å—ä»£ç 
        sector_type: æ¿å—ç±»å‹
        
    Returns:
        tuple: (éœ€è¦æ›´æ–°, æœ€æ–°æ—¥æœŸ, æ•°æ®æ–‡ä»¶è·¯å¾„)
    """
    # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®æ–‡ä»¶
    possible_dirs = [
        "data/datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²",
        "data/datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²",
        "data/datas_sector_historical/è¡Œä¸šæ¿å—",
        "data/datas_sector_historical/æ¦‚å¿µæ¿å—",
        "data/datas_sector_historical"
    ]
    
    possible_filenames = [
        f"{sector_name}({sector_code})_daily_å†å²æ•°æ®.csv",
        f"{sector_name}_{sector_code}_daily_å†å²æ•°æ®.csv",
        f"æ¿å—{sector_code}_daily_å†å²æ•°æ®.csv",
        f"{sector_code}_daily_å†å²æ•°æ®.csv"
    ]
    
    data_file = None
    for dir_path in possible_dirs:
        if os.path.exists(dir_path):
            for filename in possible_filenames:
                file_path = os.path.join(dir_path, filename)
                if os.path.exists(file_path):
                    data_file = file_path
                    break
        if data_file:
            break
    
    if not data_file:
        return True, None, None
    
    # è·å–æœ€æ–°æ—¥æœŸ
    latest_date = get_latest_data_date(data_file)
    if not latest_date:
        return True, None, data_file
    
    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆè¶…è¿‡1å¤©ï¼‰
    try:
        latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
        current_dt = datetime.now()
        days_diff = (current_dt - latest_dt).days
        
        if days_diff >= 1:  # è¶…è¿‡1å¤©åˆ™éœ€è¦æ›´æ–°
            return True, latest_date, data_file
        else:
            return False, latest_date, data_file
            
    except Exception as e:
        logger.warning(f"âš ï¸ è§£ææ—¥æœŸå¤±è´¥: {str(e)}")
        return True, latest_date, data_file

def get_historical_data():
    """è·å–æ¿å—å†å²æ•°æ®"""
    logger.info("ğŸ“Š è·å–æ¿å—å†å²æ•°æ®æ¨¡å¼")
    
    print("è¯·é€‰æ‹©å†å²æ•°æ®è·å–é€‰é¡¹:")
    print("1. è·å–æ‰€æœ‰è¡Œä¸šæ¿å—å†å²æ•°æ®ï¼ˆæŒ‡å®šå¤©æ•°ï¼‰")
    print("2. è·å–æ‰€æœ‰æ¦‚å¿µæ¿å—å†å²æ•°æ®ï¼ˆæŒ‡å®šå¤©æ•°ï¼‰") 
    print("3. è·å–å•ä¸ªæ¿å—å†å²æ•°æ®ï¼ˆæŒ‡å®šå¤©æ•°ï¼‰")
    print("4. è·å–æ‰€æœ‰è¡Œä¸šæ¿å—å…¨éƒ¨å†å²æ•°æ®")
    print("5. è·å–æ‰€æœ‰æ¦‚å¿µæ¿å—å…¨éƒ¨å†å²æ•°æ®")
    print("6. è·å–å•ä¸ªæ¿å—å…¨éƒ¨å†å²æ•°æ®")
    print("7. ğŸ”„ å¢é‡æ›´æ–°æ‰€æœ‰è¡Œä¸šæ¿å—æ•°æ®ï¼ˆä»æœ€æ–°æ—¥æœŸå¼€å§‹ï¼‰")
    print("8. ğŸ”„ å¢é‡æ›´æ–°æ‰€æœ‰æ¦‚å¿µæ¿å—æ•°æ®ï¼ˆä»æœ€æ–°æ—¥æœŸå¼€å§‹ï¼‰")
    print("9. ğŸ”„ å¢é‡æ›´æ–°å•ä¸ªæ¿å—æ•°æ®ï¼ˆä»æœ€æ–°æ—¥æœŸå¼€å§‹ï¼‰")
    print("10. ğŸ§  æ™ºèƒ½å¢é‡æ›´æ–°ï¼ˆè‡ªåŠ¨æ£€æµ‹éœ€è¦æ›´æ–°çš„æ¿å—ï¼‰")
    print("11. è¿”å›ä¸»èœå•")
    
    choice = input("è¯·é€‰æ‹© (1-11): ").strip()
    
    if choice == "11":
        return
    
    # æ–°å¢æ™ºèƒ½å¢é‡æ›´æ–°åŠŸèƒ½
    if choice == "10":
        smart_incremental_update()
        return
    
    # åˆ¤æ–­æ˜¯å¦è·å–å…¨éƒ¨å†å²æ•°æ®
    get_all_history = choice in ["4", "5", "6"]
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºå¢é‡æ›´æ–°
    is_incremental = choice in ["7", "8", "9"]
    
    if get_all_history:
        print("\nğŸ“… å°†è·å–æ¿å—çš„å…¨éƒ¨å†å²æ•°æ®ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰")
        trading_days = 2000  # è®¾ç½®ä¸€ä¸ªå¤§æ•°å€¼è·å–å°½å¯èƒ½å¤šçš„æ•°æ®
    elif is_incremental:
        print("\nğŸ”„ å¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†ä»æœ€æ–°æ•°æ®æ—¥æœŸå¼€å§‹è·å–åˆ°ä»Šå¤©")
        trading_days = None  # å¢é‡æ¨¡å¼ä¸éœ€è¦è®¾ç½®å¤©æ•°
    else:
        # è®¾ç½®äº¤æ˜“æ—¥æ•°é‡
        print("\nè®¾ç½®è·å–æ•°æ®é‡:")
        try:
            trading_days = int(input("è¯·è¾“å…¥è¦è·å–æœ€è¿‘å¤šå°‘ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® (å›è½¦é»˜è®¤30ä¸ª): ").strip() or "30")
            if trading_days <= 0 or trading_days > 1000:
                trading_days = 30
                print("æ•°é‡æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤30ä¸ªäº¤æ˜“æ—¥")
        except ValueError:
            trading_days = 30
            print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤30ä¸ªäº¤æ˜“æ—¥")
        
        print(f"å°†è·å–æœ€è¿‘ {trading_days} ä¸ªäº¤æ˜“æ—¥çš„æ—¥çº¿æ•°æ®")
    
    s_f_1 = Spider_func()
    
    if choice in ["1", "4", "7"]:
        # è·å–æ‰€æœ‰è¡Œä¸šæ¿å—å†å²æ•°æ®
        if choice == "7":  # å¢é‡æ›´æ–°
            data_type = "å¢é‡æ›´æ–°"
            save_dir = "data/datas_sector_historical/è¡Œä¸šæ¿å—"
            logger.info(f"å¼€å§‹å¢é‡æ›´æ–°æ‰€æœ‰è¡Œä¸šæ¿å—æ•°æ®...")
        else:
            data_type = "å…¨éƒ¨" if get_all_history else f"æœ€è¿‘{trading_days}ä¸ªäº¤æ˜“æ—¥"
            save_dir = "data/datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²" if get_all_history else "data/datas_sector_historical/è¡Œä¸šæ¿å—"
            logger.info(f"å¼€å§‹è·å–æ‰€æœ‰è¡Œä¸šæ¿å—{data_type}å†å²æ•°æ®...")
        
        all_data = s_f_1.get_all_sectors_historical_data(
            sector_type='industry',
            trading_days=trading_days,
            period='daily',
            save_dir=save_dir,
            is_incremental=(choice == "7")
        )
        logger.info(f"âœ… æˆåŠŸè·å– {len(all_data)} ä¸ªè¡Œä¸šæ¿å—çš„{data_type}å†å²æ•°æ®")
        
    elif choice in ["2", "5", "8"]:
        # è·å–æ‰€æœ‰æ¦‚å¿µæ¿å—å†å²æ•°æ®
        if choice == "8":  # å¢é‡æ›´æ–°
            data_type = "å¢é‡æ›´æ–°"
            save_dir = "data/datas_sector_historical/æ¦‚å¿µæ¿å—"
            logger.info(f"å¼€å§‹å¢é‡æ›´æ–°æ‰€æœ‰æ¦‚å¿µæ¿å—æ•°æ®...")
        else:
            data_type = "å…¨éƒ¨" if get_all_history else f"æœ€è¿‘{trading_days}ä¸ªäº¤æ˜“æ—¥"
            save_dir = "data/datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²" if get_all_history else "data/datas_sector_historical/æ¦‚å¿µæ¿å—"
            logger.info(f"å¼€å§‹è·å–æ‰€æœ‰æ¦‚å¿µæ¿å—{data_type}å†å²æ•°æ®...")
        
        all_data = s_f_1.get_all_sectors_historical_data(
            sector_type='concept',
            trading_days=trading_days,
            period='daily',
            save_dir=save_dir,
            is_incremental=(choice == "8")
        )
        logger.info(f"âœ… æˆåŠŸè·å– {len(all_data)} ä¸ªæ¦‚å¿µæ¿å—çš„{data_type}å†å²æ•°æ®")
        
    elif choice in ["3", "6", "9"]:
        # è·å–å•ä¸ªæ¿å—å†å²æ•°æ®
        print("\nè¾“å…¥æ¿å—ä¿¡æ¯:")
        sector_code = input("æ¿å—ä»£ç : ").strip()
        sector_type = input("æ¿å—ç±»å‹ (industry/concept): ").strip()
        
        if not sector_code or sector_type not in ['industry', 'concept']:
            logger.error("âŒ æ¿å—ä¿¡æ¯è¾“å…¥é”™è¯¯")
            return
        
        if choice == "9":  # å¢é‡æ›´æ–°
            data_type = "å¢é‡æ›´æ–°"
            logger.info(f"å¼€å§‹å¢é‡æ›´æ–°æ¿å— {sector_code} æ•°æ®...")
            
            historical_df = s_f_1.get_historical_sector_data_from_eastmoney(
                sector_code=sector_code,
                sector_type=sector_type,
                trading_days=None,
                period='daily',
                is_incremental=True
            )
        else:
            data_type = "å…¨éƒ¨" if get_all_history else f"æœ€è¿‘{trading_days}ä¸ªäº¤æ˜“æ—¥"
            logger.info(f"å¼€å§‹è·å–æ¿å— {sector_code} çš„{data_type}å†å²æ•°æ®...")
            
            historical_df = s_f_1.get_historical_sector_data_from_eastmoney(
                sector_code=sector_code,
                sector_type=sector_type,
                trading_days=trading_days,
                period='daily'
            )
        
        if not historical_df.empty:
            # ä¿å­˜æ•°æ®
            if choice == "9":
                suffix = "å¢é‡æ›´æ–°"
                save_dir = "data/datas_sector_historical"
            else:
                suffix = "å…¨éƒ¨å†å²" if get_all_history else f"æœ€è¿‘{trading_days}å¤©"
                save_dir = "data/datas_sector"
            
            filename = f"æ¿å—{sector_code}_daily_{suffix}_æ•°æ®.csv"
            filepath = os.path.join(save_dir, filename)
            os.makedirs(save_dir, exist_ok=True)
            historical_df.to_csv(filepath, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… {data_type}å†å²æ•°æ®å·²ä¿å­˜: {filepath} ({len(historical_df)}æ¡)")
        else:
            logger.warning(f"âš ï¸ æ— æ³•è·å–æ¿å— {sector_code} çš„{data_type}å†å²æ•°æ®")

def smart_incremental_update():
    """æ™ºèƒ½å¢é‡æ›´æ–°åŠŸèƒ½"""
    logger.info("ğŸ§  å¯åŠ¨æ™ºèƒ½å¢é‡æ›´æ–°æ¨¡å¼")
    
    s_f_1 = Spider_func()
    
    # 1. æ£€æŸ¥å¹¶æ›´æ–°è¡Œä¸šæ¿å—æ•°æ®
    logger.info("ğŸ” æ£€æŸ¥è¡Œä¸šæ¿å—æ•°æ®...")
    try:
        industry_df = s_f_1.get_industry_data_from_eastmoney()
        if not industry_df.empty:
            update_count = 0
            skip_count = 0
            
            for index, row in industry_df.iterrows():
                sector_code = row['è¡Œä¸šä»£ç ']
                sector_name = row['è¡Œä¸šåç§°']
                
                needs_update, latest_date, data_file = check_sector_data_update_needed(
                    sector_name, sector_code, 'industry'
                )
                
                if needs_update:
                    logger.info(f"ğŸ”„ æ›´æ–°è¡Œä¸šæ¿å—: {sector_name}({sector_code})")
                    if latest_date:
                        logger.info(f"  ğŸ“… ä» {latest_date} å¼€å§‹å¢é‡æ›´æ–°")
                    
                    # æ‰§è¡Œå¢é‡æ›´æ–°
                    historical_df = s_f_1.get_historical_sector_data_from_eastmoney(
                        sector_code=sector_code,
                        sector_type='industry',
                        trading_days=None,
                        period='daily',
                        is_incremental=True
                    )
                    
                    if not historical_df.empty:
                        # ä¿å­˜æˆ–åˆå¹¶æ•°æ®
                        save_dir = "data/datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²"
                        os.makedirs(save_dir, exist_ok=True)
                        filename = f"{sector_name}({sector_code})_daily_å†å²æ•°æ®.csv"
                        filepath = os.path.join(save_dir, filename)
                        
                        if data_file and os.path.exists(data_file):
                            # åˆå¹¶ç°æœ‰æ•°æ®
                            existing_df = pd.read_csv(data_file, encoding='utf-8-sig')
                            combined_df = pd.concat([existing_df, historical_df], ignore_index=True)
                            # å»é‡å¹¶æŒ‰æ—¥æœŸæ’åº
                            combined_df = combined_df.drop_duplicates(subset=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
                            combined_df.to_csv(filepath, index=False, encoding='utf-8-sig')
                        else:
                            historical_df.to_csv(filepath, index=False, encoding='utf-8-sig')
                        
                        update_count += 1
                        logger.info(f"  âœ… å·²æ›´æ–°: {len(historical_df)} æ¡æ–°æ•°æ®")
                    else:
                        logger.warning(f"  âš ï¸ æ— æ–°æ•°æ®")
                else:
                    skip_count += 1
                    if index < 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªè·³è¿‡çš„
                        logger.info(f"  â­ï¸ è·³è¿‡ {sector_name}: æ•°æ®å·²æ˜¯æœ€æ–°")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(0.2)
            
            logger.info(f"ğŸ“Š è¡Œä¸šæ¿å—æ£€æŸ¥å®Œæˆ: æ›´æ–°{update_count}ä¸ª, è·³è¿‡{skip_count}ä¸ª")
            
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
    
    # 2. æ£€æŸ¥å¹¶æ›´æ–°æ¦‚å¿µæ¿å—æ•°æ®
    logger.info("ğŸ” æ£€æŸ¥æ¦‚å¿µæ¿å—æ•°æ®...")
    try:
        concept_df = s_f_1.get_concept_data_from_eastmoney()
        if not concept_df.empty:
            update_count = 0
            skip_count = 0
            
            # ç”±äºæ¦‚å¿µæ¿å—è¾ƒå¤šï¼Œåªæ£€æŸ¥å‰100ä¸ªï¼ˆå¯é…ç½®ï¼‰
            max_concepts = 100
            logger.info(f"ğŸ“ æ¦‚å¿µæ¿å—è¾ƒå¤š({len(concept_df)}ä¸ª)ï¼Œæ£€æŸ¥å‰{max_concepts}ä¸ª")
            
            for index, row in concept_df.head(max_concepts).iterrows():
                sector_code = row['æ¦‚å¿µä»£ç ']
                sector_name = row['æ¦‚å¿µåç§°']
                
                needs_update, latest_date, data_file = check_sector_data_update_needed(
                    sector_name, sector_code, 'concept'
                )
                
                if needs_update:
                    logger.info(f"ğŸ”„ æ›´æ–°æ¦‚å¿µæ¿å—: {sector_name}({sector_code})")
                    if latest_date:
                        logger.info(f"  ğŸ“… ä» {latest_date} å¼€å§‹å¢é‡æ›´æ–°")
                    
                    # æ‰§è¡Œå¢é‡æ›´æ–°
                    historical_df = s_f_1.get_historical_sector_data_from_eastmoney(
                        sector_code=sector_code,
                        sector_type='concept',
                        trading_days=None,
                        period='daily',
                        is_incremental=True
                    )
                    
                    if not historical_df.empty:
                        # ä¿å­˜æˆ–åˆå¹¶æ•°æ®
                        save_dir = "data/datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²"
                        os.makedirs(save_dir, exist_ok=True)
                        filename = f"{sector_name}({sector_code})_daily_å†å²æ•°æ®.csv"
                        filepath = os.path.join(save_dir, filename)
                        
                        if data_file and os.path.exists(data_file):
                            # åˆå¹¶ç°æœ‰æ•°æ®
                            existing_df = pd.read_csv(data_file, encoding='utf-8-sig')
                            combined_df = pd.concat([existing_df, historical_df], ignore_index=True)
                            # å»é‡å¹¶æŒ‰æ—¥æœŸæ’åº
                            combined_df = combined_df.drop_duplicates(subset=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
                            combined_df.to_csv(filepath, index=False, encoding='utf-8-sig')
                        else:
                            historical_df.to_csv(filepath, index=False, encoding='utf-8-sig')
                        
                        update_count += 1
                        logger.info(f"  âœ… å·²æ›´æ–°: {len(historical_df)} æ¡æ–°æ•°æ®")
                    else:
                        logger.warning(f"  âš ï¸ æ— æ–°æ•°æ®")
                else:
                    skip_count += 1
                    if index < 3:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªè·³è¿‡çš„
                        logger.info(f"  â­ï¸ è·³è¿‡ {sector_name}: æ•°æ®å·²æ˜¯æœ€æ–°")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(0.2)
            
            logger.info(f"ğŸ“Š æ¦‚å¿µæ¿å—æ£€æŸ¥å®Œæˆ: æ›´æ–°{update_count}ä¸ª, è·³è¿‡{skip_count}ä¸ª")
            
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥æ¦‚å¿µæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
    
    # 3. æ£€æŸ¥å¹¶æ›´æ–°è‚¡ç¥¨æ¿å—æ˜ å°„
    logger.info("ğŸ” æ£€æŸ¥è‚¡ç¥¨æ¿å—æ˜ å°„...")
    try:
        needs_update, last_date, message = check_stock_mapping_update_needed()
        if needs_update:
            logger.info(f"ğŸ”„ æ›´æ–°è‚¡ç¥¨æ¿å—æ˜ å°„: {message}")
            get_stock_sector_mapping(max_stocks=None, incremental=True)
        else:
            logger.info(f"â­ï¸ è·³è¿‡è‚¡ç¥¨æ˜ å°„æ›´æ–°: {message}")
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥è‚¡ç¥¨æ˜ å°„å¤±è´¥: {str(e)}")
    
    logger.info("ğŸ‰ æ™ºèƒ½å¢é‡æ›´æ–°å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - æ¿å—æ•°æ®è·å–å·¥å…·")
    logger.info("=" * 60)
    
    # ä¸»èœå•
    print("è¯·é€‰æ‹©åŠŸèƒ½:")
    print("1. è·å–å½“å‰æ¿å—æ•°æ®å’Œè‚¡ç¥¨æ¿å—æ˜ å°„")
    print("2. è·å–æ¿å—å†å²æ•°æ®")
    print("3. ğŸ§  æ™ºèƒ½å¢é‡æ›´æ–°ï¼ˆæ¨èï¼‰")
    print("4. ğŸ”„ è‚¡ç¥¨æ˜ å°„å¢é‡æ›´æ–°")
    print("5. é€€å‡º")
    
    try:
        main_choice = input("è¯·é€‰æ‹© (1-5): ").strip()
        
        if main_choice == "2":
            get_historical_data()
            return
        elif main_choice == "3":
            smart_incremental_update()
            return
        elif main_choice == "4":
            logger.info("ğŸ”„ å¯åŠ¨è‚¡ç¥¨æ˜ å°„å¢é‡æ›´æ–°...")
            get_stock_sector_mapping(max_stocks=None, incremental=True)
            return
        elif main_choice == "5":
            logger.info("ç¨‹åºé€€å‡º")
            return
        elif main_choice != "1":
            logger.info("ä½¿ç”¨é»˜è®¤æ¨¡å¼: è·å–å½“å‰æ¿å—æ•°æ®")
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    
    # åŸæœ‰çš„è‚¡ç¥¨æ¿å—æ˜ å°„åŠŸèƒ½
    print("\nè¯·é€‰æ‹©è‚¡ç¥¨æ•°æ®è·å–æ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (100åªè‚¡ç¥¨)")
    print("2. ä¸­ç­‰è§„æ¨¡æ¨¡å¼ (500åªè‚¡ç¥¨)")
    print("3. å¤§è§„æ¨¡æ¨¡å¼ (1000åªè‚¡ç¥¨)")
    print("4. å®Œæ•´æ¨¡å¼ (æ‰€æœ‰æ­£å¸¸äº¤æ˜“è‚¡ç¥¨)")
    print("5. è‡ªå®šä¹‰æ•°é‡")
    
    try:
        choice = input("è¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == "1":
            max_stocks = 100
            logger.info("ğŸ§ª å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
        elif choice == "2":
            max_stocks = 500
            logger.info("ğŸ“Š ä¸­ç­‰è§„æ¨¡æ¨¡å¼: å¤„ç†500åªè‚¡ç¥¨")
        elif choice == "3":
            max_stocks = 1000
            logger.info("ğŸ“ˆ å¤§è§„æ¨¡æ¨¡å¼: å¤„ç†1000åªè‚¡ç¥¨")
        elif choice == "4":
            max_stocks = None
            logger.info("ğŸ¯ å®Œæ•´æ¨¡å¼: å¤„ç†æ‰€æœ‰æ­£å¸¸äº¤æ˜“è‚¡ç¥¨")
        elif choice == "5":
            try:
                max_stocks = int(input("è¯·è¾“å…¥è¦å¤„ç†çš„è‚¡ç¥¨æ•°é‡: ").strip())
                logger.info(f"ğŸ”§ è‡ªå®šä¹‰æ¨¡å¼: å¤„ç†{max_stocks}åªè‚¡ç¥¨")
            except ValueError:
                logger.error("âŒ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
                max_stocks = 100
        else:
            logger.info("ğŸ§ª é»˜è®¤ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
            max_stocks = 100
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    except Exception:
        logger.info("ğŸ§ª ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
        max_stocks = 100
    
    try:
        start_time = datetime.now()
        
        # 1. è·å–æ¿å—æ•°æ®
        logger.info("ç¬¬ä¸€æ­¥: è·å–æ¿å—è¡Œä¸šæ•°æ®...")
        df_industry, df_concept, df_hot_concepts = get_real_sector_data()
        
        # 2. è·å–è‚¡ç¥¨æ¿å—æ˜ å°„
        logger.info("ç¬¬äºŒæ­¥: è·å–è‚¡ç¥¨æ¿å—æ˜ å°„...")
        df_stock_mapping = get_stock_sector_mapping(
            max_stocks=max_stocks,
            start_from=0,
            batch_size=50,
            incremental=False
        )
        
        # 3. åˆ›å»ºæ•°æ®æ‘˜è¦
        logger.info("ç¬¬ä¸‰æ­¥: åˆ›å»ºæ•°æ®æ‘˜è¦...")
        create_sector_summary()
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("ğŸ‰ æ‰€æœ‰æ¿å—æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        logger.info("ğŸ“ æ¿å—æ•°æ®ä¿å­˜åœ¨ data/datas_sector/ ç›®å½•ä¸­")
        logger.info("ğŸ“ è‚¡ç¥¨æ˜ å°„è¡¨ä¿å­˜åœ¨ data/datas_sector_historical/ ç›®å½•ä¸­")
        logger.info("ğŸ’¡ å¯ä»¥ç›´æ¥ä½¿ç”¨ è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        
        # æ˜¾ç¤ºè·å–ç»Ÿè®¡
        if not df_stock_mapping.empty:
            unique_industries = df_stock_mapping['æ‰€å±è¡Œä¸š'].nunique()
            unique_concepts = df_stock_mapping['æ¦‚å¿µæ¿å—'].nunique()
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df_stock_mapping)}åªè‚¡ç¥¨, {unique_industries}ä¸ªè¡Œä¸š, {unique_concepts}ä¸ªæ¦‚å¿µ")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        logger.info("ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œæ¥å®Œæˆå‰©ä½™å·¥ä½œ")
    except Exception as e:
        logger.error(f"âŒ æ¿å—æ•°æ®è·å–å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

def check_for_incomplete_tasks():
    """
    æ£€æŸ¥æœªå®Œæˆçš„ä»»åŠ¡
    
    Returns:
        dict: åŒ…å«å„ç§æœªå®Œæˆä»»åŠ¡çš„çŠ¶æ€ä¿¡æ¯
    """
    status = {
        'stock_mapping_incomplete': False,
        'temp_file': None,
        'processed_count': 0,
        'sector_data_incomplete': [],
        'recommendations': []
    }
    
    # 1. æ£€æŸ¥è‚¡ç¥¨æ˜ å°„ä¸´æ—¶æ–‡ä»¶
    temp_file = "data/datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    if os.path.exists(temp_file):
        try:
            df_temp = pd.read_csv(temp_file, encoding='utf-8-sig')
            status['stock_mapping_incomplete'] = True
            status['temp_file'] = temp_file
            status['processed_count'] = len(df_temp)
            status['recommendations'].append(f"å‘ç°æœªå®Œæˆçš„è‚¡ç¥¨æ˜ å°„ä»»åŠ¡ï¼Œå·²å¤„ç†{len(df_temp)}åªè‚¡ç¥¨")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    # 2. æ£€æŸ¥æ¿å—æ•°æ®å®Œæ•´æ€§
    data_dirs = [
        ("data/datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²", "è¡Œä¸šæ¿å—"),
        ("data/datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²", "æ¦‚å¿µæ¿å—")
    ]
    
    for data_dir, data_type in data_dirs:
        if os.path.exists(data_dir):
            files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
            outdated_files = []
            
            for file in files[:10]:  # åªæ£€æŸ¥å‰10ä¸ªæ–‡ä»¶ï¼Œé¿å…è¿‡æ…¢
                file_path = os.path.join(data_dir, file)
                latest_date = get_latest_data_date(file_path)
                if latest_date:
                    try:
                        latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
                        days_diff = (datetime.now() - latest_dt).days
                        if days_diff >= 2:  # è¶…è¿‡2å¤©æœªæ›´æ–°
                            outdated_files.append((file, latest_date, days_diff))
                    except:
                        pass
            
            if outdated_files:
                status['sector_data_incomplete'].append({
                    'type': data_type,
                    'outdated_count': len(outdated_files),
                    'examples': outdated_files[:3]
                })
                status['recommendations'].append(f"{data_type}æœ‰{len(outdated_files)}ä¸ªæ–‡ä»¶éœ€è¦æ›´æ–°")
    
    return status

def smart_resume_dialog():
    """æ™ºèƒ½æ¢å¤å¯¹è¯"""
    logger.info("ğŸ” æ£€æŸ¥æœªå®Œæˆçš„ä»»åŠ¡...")
    
    status = check_for_incomplete_tasks()
    
    if not status['stock_mapping_incomplete'] and not status['sector_data_incomplete']:
        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ¢å¤")
        return False
    
    print("\n" + "=" * 50)
    print("ğŸ”„ å‘ç°æœªå®Œæˆçš„ä»»åŠ¡")
    print("=" * 50)
    
    if status['stock_mapping_incomplete']:
        print(f"ğŸ“Š è‚¡ç¥¨æ¿å—æ˜ å°„: å·²å¤„ç†{status['processed_count']}åªè‚¡ç¥¨ï¼Œæœ‰å¾…ç»§ç»­")
    
    for incomplete in status['sector_data_incomplete']:
        print(f"ğŸ“ˆ {incomplete['type']}: {incomplete['outdated_count']}ä¸ªæ–‡ä»¶éœ€è¦æ›´æ–°")
        for file, date, days in incomplete['examples'][:2]:
            print(f"  - {file}: æœ€æ–°æ•°æ®{date} ({days}å¤©å‰)")
    
    print("\nå»ºè®®æ“ä½œ:")
    for i, recommendation in enumerate(status['recommendations'], 1):
        print(f"{i}. {recommendation}")
    
    print("\né€‰æ‹©æ“ä½œ:")
    print("1. ğŸ§  æ™ºèƒ½å¢é‡æ›´æ–°ï¼ˆæ¨èï¼‰")
    print("2. ğŸ”„ ç»§ç»­è‚¡ç¥¨æ˜ å°„ä»»åŠ¡")
    print("3. â­ï¸ è·³è¿‡ï¼Œæ‰§è¡Œæ–°ä»»åŠ¡")
    print("4. ğŸšª é€€å‡º")
    
    choice = input("è¯·é€‰æ‹© (1-4): ").strip()
    
    if choice == "1":
        smart_incremental_update()
        return True
    elif choice == "2" and status['stock_mapping_incomplete']:
        continue_from_interruption()
        return True
    elif choice == "3":
        return False
    elif choice == "4":
        logger.info("ç”¨æˆ·é€‰æ‹©é€€å‡º")
        return True
    else:
        logger.info("æ— æ•ˆé€‰æ‹©ï¼Œç»§ç»­æ‰§è¡Œæ–°ä»»åŠ¡")
        return False

def continue_from_interruption():
    """ä»ä¸­æ–­å¤„ç»§ç»­è·å–æ•°æ®"""
    logger.info("ğŸ”„ ç»§ç»­ä»ä¸­æ–­å¤„è·å–æ•°æ®...")
    
    # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶
    temp_file = "data/datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    if not os.path.exists(temp_file):
        logger.error("âŒ æœªæ‰¾åˆ°ä¸­æ–­çš„ä¸´æ—¶æ–‡ä»¶")
        return
    
    try:
        df_existing = pd.read_csv(temp_file, encoding='utf-8-sig')
        processed_count = len(df_existing)
        logger.info(f"ğŸ“Š å·²å¤„ç† {processed_count} åªè‚¡ç¥¨ï¼Œç»§ç»­å¤„ç†å‰©ä½™è‚¡ç¥¨...")
        
        # ç»§ç»­å¤„ç†
        df_stock_mapping = get_stock_sector_mapping(
            max_stocks=None,  # å¤„ç†æ‰€æœ‰å‰©ä½™
            start_from=0,     # å‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„
            batch_size=50,
            incremental=False
        )
        
        create_sector_summary()
        logger.info("ğŸ‰ ç»§ç»­å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç»§ç»­å¤„ç†å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    # å¯åŠ¨æ™ºèƒ½ä»»åŠ¡æ£€æŸ¥å’Œæ¢å¤
    if smart_resume_dialog():
        # å¦‚æœæ™ºèƒ½æ¢å¤å¤„ç†äº†ä»»åŠ¡ï¼Œå°±ä¸å†æ‰§è¡Œä¸»ç¨‹åº
        pass
    else:
        # æ‰§è¡Œæ­£å¸¸çš„ä¸»ç¨‹åº
        main()

