#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è·å–çœŸå®çš„æ¿å—æ•°æ®å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
åŸºäºåŸå§‹çš„2.10è·å–æ¿å—æ•°æ®.pyï¼Œå¢åŠ CSVä¿å­˜åŠŸèƒ½
"""

import os
import sys
import pandas as pd
from datetime import datetime
import logging
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'core'))

# å¯¼å…¥é‡åŒ–å‡½æ•°
try:
    from samequant_functions import Spider_func
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥samequant_functions: {str(e)}")
    print("è¯·ç¡®ä¿samequant_functions.pyåœ¨é¡¹ç›®æ ¹ç›®å½•")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_real_sector_data():
    """è·å–çœŸå®çš„æ¿å—æ•°æ®å¹¶ä¿å­˜"""
    logger.info("ğŸš€ å¼€å§‹è·å–çœŸå®æ¿å—æ•°æ®...")
    
    # åˆå§‹åŒ–Spider_funcå®ä¾‹
    s_f_1 = Spider_func()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = "data/datas_sector"
    os.makedirs(save_dir, exist_ok=True)
    
    # 1. è·å–è¡Œä¸šæ¿å—æ•°æ®
    logger.info("ğŸ“Š è·å–è¡Œä¸šæ¿å—æ•°æ®...")
    try:
        df_industry = s_f_1.get_industry_data_from_eastmoney(sort_field='f3')
        if not df_industry.empty:
            industry_file = os.path.join(save_dir, "è¡Œä¸šæ¿å—æ•°æ®.csv")
            df_industry.to_csv(industry_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… è¡Œä¸šæ¿å—æ•°æ®å·²ä¿å­˜: {industry_file} ({len(df_industry)}è¡Œ)")
            print("æ¶¨å¹…æ’è¡Œå‰10çš„è¡Œä¸š:")
            print(df_industry[['è¡Œä¸šåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'æˆäº¤é¢', 'æ€»å¸‚å€¼']].head(10))
        else:
            logger.warning("âš ï¸ è¡Œä¸šæ¿å—æ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
        df_industry = pd.DataFrame()

    # 2. è·å–æ¦‚å¿µæ¿å—æ•°æ®
    logger.info("ğŸ“Š è·å–æ¦‚å¿µæ¿å—æ•°æ®...")
    try:
        df_concept = s_f_1.get_concept_data_from_eastmoney(sort_field='f3')
        if not df_concept.empty:
            concept_file = os.path.join(save_dir, "æ¦‚å¿µæ¿å—æ•°æ®.csv")
            df_concept.to_csv(concept_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… æ¦‚å¿µæ¿å—æ•°æ®å·²ä¿å­˜: {concept_file} ({len(df_concept)}è¡Œ)")
            print("æ¶¨å¹…æ’è¡Œå‰10çš„æ¦‚å¿µ:")
            print(df_concept[['æ¦‚å¿µåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'æˆäº¤é¢', 'æ€»å¸‚å€¼', 'æµé€šå¸‚å€¼']].head(10))
        else:
            logger.warning("âš ï¸ æ¦‚å¿µæ¿å—æ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–æ¦‚å¿µæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
        df_concept = pd.DataFrame()

    # 3. è·å–çƒ­é—¨æ¦‚å¿µæ’è¡Œ
    logger.info("ğŸ“Š è·å–çƒ­é—¨æ¦‚å¿µæ’è¡Œ...")
    try:
        df_hot_concepts = s_f_1.get_hot_concepts_from_eastmoney(limit=50)  # å¢åŠ åˆ°50ä¸ª
        if not df_hot_concepts.empty:
            hot_concepts_file = os.path.join(save_dir, "çƒ­é—¨æ¦‚å¿µæ’è¡Œ.csv")
            df_hot_concepts.to_csv(hot_concepts_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… çƒ­é—¨æ¦‚å¿µæ’è¡Œå·²ä¿å­˜: {hot_concepts_file} ({len(df_hot_concepts)}è¡Œ)")
            print("çƒ­é—¨æ¦‚å¿µæ’è¡Œå‰20:")
            print(df_hot_concepts[['æ¦‚å¿µåç§°', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'ä¸Šæ¶¨å®¶æ•°', 'ä¸‹è·Œå®¶æ•°']].head(20))
        else:
            logger.warning("âš ï¸ çƒ­é—¨æ¦‚å¿µæ•°æ®ä¸ºç©º")
    except Exception as e:
        logger.error(f"âŒ è·å–çƒ­é—¨æ¦‚å¿µæ•°æ®å¤±è´¥: {str(e)}")
        df_hot_concepts = pd.DataFrame()

    return df_industry, df_concept, df_hot_concepts

def load_all_stock_list():
    """ä»all_stock_list.csvåŠ è½½å®Œæ•´çš„è‚¡ç¥¨åˆ—è¡¨"""
    logger.info("ğŸ“Š åŠ è½½å®Œæ•´è‚¡ç¥¨åˆ—è¡¨...")
    
    stock_list_paths = [
        "data/stockcode_list/all_stock_list.csv",
        "../data/stockcode_list/all_stock_list.csv",
        "stockcode_list/all_stock_list.csv",
        "all_stock_list.csv"
    ]
    
    stock_list_file = None
    for path in stock_list_paths:
        if os.path.exists(path):
            stock_list_file = path
            break
    
    if not stock_list_file:
        logger.error("âŒ æœªæ‰¾åˆ° all_stock_list.csv æ–‡ä»¶")
        return []
    
    try:
        df_stocks = pd.read_csv(stock_list_file, encoding='utf-8-sig')
        logger.info(f"âœ… æˆåŠŸåŠ è½½è‚¡ç¥¨åˆ—è¡¨: {stock_list_file}")
        logger.info(f"ğŸ“Š æ€»è‚¡ç¥¨æ•°: {len(df_stocks)}")
        
        # ç­›é€‰æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨
        if 'ä¸Šå¸‚çŠ¶æ€' in df_stocks.columns:
            active_stocks = df_stocks[df_stocks['ä¸Šå¸‚çŠ¶æ€'] == 'æ­£å¸¸äº¤æ˜“']
            logger.info(f"ğŸ“Š æ­£å¸¸äº¤æ˜“è‚¡ç¥¨æ•°: {len(active_stocks)}")
        else:
            active_stocks = df_stocks
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¸Šå¸‚çŠ¶æ€åˆ—ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨")
        
        # æå–è‚¡ç¥¨ä»£ç 
        stock_codes = active_stocks['è‚¡ç¥¨ä»£ç '].tolist()
        logger.info(f"ğŸ“Š å°†è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„æ¿å—ä¿¡æ¯")
        
        return stock_codes
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return []

def get_stock_sector_mapping(max_stocks=None, start_from=0, batch_size=50):
    """
    è·å–è‚¡ç¥¨çš„æ¿å—æ˜ å°„ä¿¡æ¯
    
    Args:
        max_stocks: æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
        start_from: ä»ç¬¬å‡ åªè‚¡ç¥¨å¼€å§‹å¤„ç†ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
    """
    logger.info("ğŸ“Š è·å–è‚¡ç¥¨æ¿å—æ˜ å°„ä¿¡æ¯...")
    
    # åˆå§‹åŒ–Spider_funcå®ä¾‹
    s_f_1 = Spider_func()
    
    # ä»all_stock_list.csvåŠ è½½è‚¡ç¥¨ä»£ç 
    all_stock_codes = load_all_stock_list()
    
    if not all_stock_codes:
        logger.error("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤ä»£ç ")
        # ä½¿ç”¨ä¸€äº›å¸¸è§çš„è‚¡ç¥¨ä»£ç ä½œä¸ºç¤ºä¾‹
        all_stock_codes = [
            'sh600519', 'sz000001', 'sz000002', 'sh600000', 'sz000858', 
            'sh600036', 'sz000858', 'sh600276', 'sz002594', 'sh601318',
            'sh600309', 'sz000063', 'sh603259', 'sz002415', 'sh600887'
        ]
    
    # ç¡®å®šå¤„ç†èŒƒå›´
    end_index = min(start_from + max_stocks, len(all_stock_codes)) if max_stocks else len(all_stock_codes)
    stock_codes = all_stock_codes[start_from:end_index]
    
    logger.info(f"ğŸ“Š å¤„ç†èŒƒå›´: {start_from+1} - {end_index} / {len(all_stock_codes)} åªè‚¡ç¥¨")
    logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡å¤„ç†: {len(stock_codes)} åªè‚¡ç¥¨")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ä¸­é—´ç»“æœ
    temp_file = "data/datas_sector/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    existing_data = []
    if os.path.exists(temp_file):
        try:
            df_existing = pd.read_csv(temp_file, encoding='utf-8-sig')
            existing_data = df_existing.to_dict('records')
            existing_codes = set(df_existing['è‚¡ç¥¨ä»£ç '].tolist())
            # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„è‚¡ç¥¨
            stock_codes = [code for code in stock_codes if code not in existing_codes]
            logger.info(f"ğŸ“Š å‘ç°ä¸­é—´ç»“æœï¼Œå·²å¤„ç† {len(existing_data)} åªè‚¡ç¥¨")
            logger.info(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†: {len(stock_codes)} åªè‚¡ç¥¨")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ä¸­é—´ç»“æœå¤±è´¥: {str(e)}")
            existing_data = []
    
    # è·å–è‚¡ç¥¨æ¿å—ä¿¡æ¯
    stock_sector_mapping = existing_data.copy()  # åŒ…å«å·²æœ‰æ•°æ®
    successful_count = len(existing_data)
    batch_count = 0
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("data/datas_sector", exist_ok=True)
    
    logger.info(f"ğŸš€ å¼€å§‹è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„æ¿å—ä¿¡æ¯...")
    logger.info(f"ğŸ’¾ æ¯ {batch_size} åªè‚¡ç¥¨ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ")
    
    for i, stock_code in enumerate(stock_codes, 1):
        try:
            # æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0:
                progress = (i / len(stock_codes)) * 100
                logger.info(f"ğŸ“Š è¿›åº¦: {i}/{len(stock_codes)} ({progress:.1f}%) - æˆåŠŸ: {successful_count}")
            
            # è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
            stock_info = None
            for retry in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                try:
                    stock_info = s_f_1.get_stock_industry_info_from_eastmoney(stock_code=stock_code)
                    if stock_info:
                        break
                except Exception as retry_e:
                    if retry == 2:  # æœ€åä¸€æ¬¡é‡è¯•
                        logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} é‡è¯•3æ¬¡åä»å¤±è´¥: {str(retry_e)}")
                    else:
                        time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            
            if stock_info:
                mapping_info = {
                    'è‚¡ç¥¨ä»£ç ': stock_code,
                    'è‚¡ç¥¨åç§°': stock_info.get('è‚¡ç¥¨åç§°', ''),
                    'æ‰€å±è¡Œä¸š': stock_info.get('æ‰€å±è¡Œä¸š', ''),
                    'æ¦‚å¿µæ¿å—': stock_info.get('æ¦‚å¿µæ¿å—', ''),
                    'åœ°åŒº': stock_info.get('åœ°åŒº', ''),
                    'æ€»è‚¡æœ¬': stock_info.get('æ€»è‚¡æœ¬', ''),
                    'æµé€šè‚¡': stock_info.get('æµé€šè‚¡', '')
                }
                stock_sector_mapping.append(mapping_info)
                successful_count += 1
                
                # æ˜¾ç¤ºå‰10ä¸ªæˆåŠŸçš„ç»“æœ
                if successful_count <= 10:
                    print(f"{stock_info['è‚¡ç¥¨åç§°']}({stock_code}):")
                    print(f"  æ‰€å±è¡Œä¸š: {stock_info['æ‰€å±è¡Œä¸š']}")
                    print(f"  æ¦‚å¿µæ¿å—: {stock_info['æ¦‚å¿µæ¿å—']}")
                    print()
            else:
                logger.warning(f"âš ï¸ æ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„ä¿¡æ¯")
            
            # æ‰¹æ¬¡ä¿å­˜ä¸­é—´ç»“æœ
            batch_count += 1
            if batch_count >= batch_size:
                try:
                    df_temp = pd.DataFrame(stock_sector_mapping)
                    df_temp.to_csv(temp_file, index=False, encoding='utf-8-sig')
                    logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ: {len(stock_sector_mapping)} æ¡è®°å½•")
                    batch_count = 0
                except Exception as save_e:
                    logger.error(f"âŒ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {str(save_e)}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚å¤ªé¢‘ç¹
            time.sleep(0.1)
                
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨ {stock_code} ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            continue
    
    # æœ€ç»ˆä¿å­˜ï¼ˆåŒ…å«æœ€åä¸€æ‰¹æ•°æ®ï¼‰
    if batch_count > 0 and stock_sector_mapping:
        try:
            df_temp = pd.DataFrame(stock_sector_mapping)
            df_temp.to_csv(temp_file, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆä¸­é—´ç»“æœ: {len(stock_sector_mapping)} æ¡è®°å½•")
        except Exception as save_e:
            logger.error(f"âŒ ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {str(save_e)}")
    
    # ä¿å­˜è‚¡ç¥¨æ¿å—æ˜ å°„
    if stock_sector_mapping:
        df_stock_mapping = pd.DataFrame(stock_sector_mapping)
        
        # ä¿å­˜å®Œæ•´æ˜ å°„
        save_dir = "data/datas_sector"
        os.makedirs(save_dir, exist_ok=True)
        mapping_file = os.path.join(save_dir, "è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv")
        df_stock_mapping.to_csv(mapping_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"âœ… è‚¡ç¥¨æ¿å—æ˜ å°„å·²ä¿å­˜: {mapping_file} ({len(df_stock_mapping)}è¡Œ)")
        logger.info(f"ğŸ“Š æˆåŠŸè·å–ä¿¡æ¯çš„è‚¡ç¥¨: {successful_count}/{len(all_stock_codes[start_from:end_index])} ({successful_count/(len(all_stock_codes[start_from:end_index]))*100:.1f}%)")
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
                logger.info("ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶")
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return df_stock_mapping
    
    return pd.DataFrame()

def create_sector_summary():
    """åˆ›å»ºæ¿å—æ•°æ®æ‘˜è¦"""
    logger.info("ğŸ“Š åˆ›å»ºæ¿å—æ•°æ®æ‘˜è¦...")
    
    save_dir = "data/datas_sector"
    if not os.path.exists(save_dir):
        logger.error("âŒ data/datas_sectorç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è·å–")
        return
    
    summary = {
        'æ•°æ®è·å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'æ–‡ä»¶åˆ—è¡¨': []
    }
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    csv_files = [f for f in os.listdir(save_dir) if f.endswith('.csv')]
    
    for csv_file in csv_files:
        file_path = os.path.join(save_dir, csv_file)
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            file_info = {
                'æ–‡ä»¶å': csv_file,
                'è¡Œæ•°': len(df),
                'åˆ—æ•°': len(df.columns),
                'æ–‡ä»¶å¤§å°': f"{os.path.getsize(file_path) / 1024:.1f} KB"
            }
            summary['æ–‡ä»¶åˆ—è¡¨'].append(file_info)
            logger.info(f"âœ… {csv_file}: {len(df)}è¡Œ x {len(df.columns)}åˆ—")
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
    
    # ä¿å­˜æ‘˜è¦
    summary_file = os.path.join(save_dir, "æ•°æ®æ‘˜è¦.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("æ¿å—æ•°æ®è·å–æ‘˜è¦\n")
        f.write("=" * 50 + "\n")
        f.write(f"è·å–æ—¶é—´: {summary['æ•°æ®è·å–æ—¶é—´']}\n\n")
        f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
        for file_info in summary['æ–‡ä»¶åˆ—è¡¨']:
            f.write(f"- {file_info['æ–‡ä»¶å']}: {file_info['è¡Œæ•°']}è¡Œ, {file_info['æ–‡ä»¶å¤§å°']}\n")
    
    logger.info(f"âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - çœŸå®æ¿å—æ•°æ®è·å–")
    logger.info("=" * 60)
    
    # é…ç½®é€‰é¡¹
    print("è¯·é€‰æ‹©æ•°æ®è·å–æ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (100åªè‚¡ç¥¨)")
    print("2. ä¸­ç­‰è§„æ¨¡æ¨¡å¼ (500åªè‚¡ç¥¨)")
    print("3. å¤§è§„æ¨¡æ¨¡å¼ (1000åªè‚¡ç¥¨)")
    print("4. å®Œæ•´æ¨¡å¼ (æ‰€æœ‰æ­£å¸¸äº¤æ˜“è‚¡ç¥¨)")
    print("5. è‡ªå®šä¹‰æ•°é‡")
    
    try:
        choice = input("è¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == "1":
            max_stocks = 100
            logger.info("ğŸ§ª å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
        elif choice == "2":
            max_stocks = 500
            logger.info("ğŸ“Š ä¸­ç­‰è§„æ¨¡æ¨¡å¼: å¤„ç†500åªè‚¡ç¥¨")
        elif choice == "3":
            max_stocks = 1000
            logger.info("ğŸ“ˆ å¤§è§„æ¨¡æ¨¡å¼: å¤„ç†1000åªè‚¡ç¥¨")
        elif choice == "4":
            max_stocks = None
            logger.info("ğŸ¯ å®Œæ•´æ¨¡å¼: å¤„ç†æ‰€æœ‰æ­£å¸¸äº¤æ˜“è‚¡ç¥¨")
        elif choice == "5":
            try:
                max_stocks = int(input("è¯·è¾“å…¥è¦å¤„ç†çš„è‚¡ç¥¨æ•°é‡: ").strip())
                logger.info(f"ğŸ”§ è‡ªå®šä¹‰æ¨¡å¼: å¤„ç†{max_stocks}åªè‚¡ç¥¨")
            except ValueError:
                logger.error("âŒ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
                max_stocks = 100
        else:
            logger.info("ğŸ§ª é»˜è®¤ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
            max_stocks = 100
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    except Exception:
        logger.info("ğŸ§ª ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼: å¤„ç†100åªè‚¡ç¥¨")
        max_stocks = 100
    
    try:
        start_time = datetime.now()
        
        # 1. è·å–æ¿å—æ•°æ®
        logger.info("ç¬¬ä¸€æ­¥: è·å–æ¿å—è¡Œä¸šæ•°æ®...")
        df_industry, df_concept, df_hot_concepts = get_real_sector_data()
        
        # 2. è·å–è‚¡ç¥¨æ¿å—æ˜ å°„
        logger.info("ç¬¬äºŒæ­¥: è·å–è‚¡ç¥¨æ¿å—æ˜ å°„...")
        df_stock_mapping = get_stock_sector_mapping(
            max_stocks=max_stocks,
            start_from=0,
            batch_size=50
        )
        
        # 3. åˆ›å»ºæ•°æ®æ‘˜è¦
        logger.info("ç¬¬ä¸‰æ­¥: åˆ›å»ºæ•°æ®æ‘˜è¦...")
        create_sector_summary()
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("ğŸ‰ æ‰€æœ‰æ¿å—æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration}")
        logger.info("ğŸ“ æ•°æ®ä¿å­˜åœ¨ data/datas_sector/ ç›®å½•ä¸­")
        logger.info("ğŸ’¡ å¯ä»¥ç›´æ¥ä½¿ç”¨ è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        
        # æ˜¾ç¤ºè·å–ç»Ÿè®¡
        if not df_stock_mapping.empty:
            unique_industries = df_stock_mapping['æ‰€å±è¡Œä¸š'].nunique()
            unique_concepts = df_stock_mapping['æ¦‚å¿µæ¿å—'].nunique()
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df_stock_mapping)}åªè‚¡ç¥¨, {unique_industries}ä¸ªè¡Œä¸š, {unique_concepts}ä¸ªæ¦‚å¿µ")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        logger.info("ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œæ¥å®Œæˆå‰©ä½™å·¥ä½œ")
    except Exception as e:
        logger.error(f"âŒ æ¿å—æ•°æ®è·å–å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

def continue_from_interruption():
    """ä»ä¸­æ–­å¤„ç»§ç»­è·å–æ•°æ®"""
    logger.info("ğŸ”„ ç»§ç»­ä»ä¸­æ–­å¤„è·å–æ•°æ®...")
    
    # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶
    temp_file = "data/datas_sector/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    if not os.path.exists(temp_file):
        logger.error("âŒ æœªæ‰¾åˆ°ä¸­æ–­çš„ä¸´æ—¶æ–‡ä»¶")
        return
    
    try:
        df_existing = pd.read_csv(temp_file, encoding='utf-8-sig')
        processed_count = len(df_existing)
        logger.info(f"ğŸ“Š å·²å¤„ç† {processed_count} åªè‚¡ç¥¨ï¼Œç»§ç»­å¤„ç†å‰©ä½™è‚¡ç¥¨...")
        
        # ç»§ç»­å¤„ç†
        df_stock_mapping = get_stock_sector_mapping(
            max_stocks=None,  # å¤„ç†æ‰€æœ‰å‰©ä½™
            start_from=0,     # å‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„
            batch_size=50
        )
        
        create_sector_summary()
        logger.info("ğŸ‰ ç»§ç»­å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç»§ç»­å¤„ç†å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­çš„ä¸´æ—¶æ–‡ä»¶
    temp_file = "data/datas_sector/è‚¡ç¥¨æ¿å—æ˜ å°„_ä¸´æ—¶.csv"
    if os.path.exists(temp_file):
        print("å‘ç°ä¸­æ–­çš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ˜¯å¦ç»§ç»­ä¹‹å‰çš„å·¥ä½œï¼Ÿ")
        continue_choice = input("è¾“å…¥ 'c' ç»§ç»­ï¼Œæˆ–ä»»æ„é”®é‡æ–°å¼€å§‹: ").strip().lower()
        if continue_choice == 'c':
            continue_from_interruption()
        else:
            main()
    else:
        main()

