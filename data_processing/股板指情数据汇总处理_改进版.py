#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å— - æ”¹è¿›ç‰ˆ
æ·»åŠ äº†æ—¥æœŸèŒƒå›´æ§åˆ¶åŠŸèƒ½
"""

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import glob
import warnings
from datetime import datetime
import pickle
import json

warnings.filterwarnings('ignore')


class ImprovedDataProcessorV2:
    """æ”¹è¿›çš„æ•°æ®å¤„ç†å™¨V2ï¼šæ¯åªè‚¡ç¥¨åªä½¿ç”¨æ‰€å±è¡Œä¸šç‰¹å¾ + æ—¥æœŸèŒƒå›´æ§åˆ¶"""
    
    def __init__(self, data_base_path: str = "./data"):
        self.data_base_path = data_base_path
        self.stock_path = os.path.join(data_base_path, "datas_em")
        self.industry_path = os.path.join(data_base_path, "datas_sector_historical/è¡Œä¸šæ¿å—_å…¨éƒ¨å†å²")
        self.index_path = os.path.join(data_base_path, "datas_index/datas_index")
        self.concept_path = os.path.join(data_base_path, "datas_sector_historical/æ¦‚å¿µæ¿å—_å…¨éƒ¨å†å²")
        
        # è‚¡ç¥¨è¡Œä¸šæ˜ å°„ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µæ„å»ºï¼‰
        self.stock_sector_mapping = self._create_stock_sector_mapping()
        
        # è¡Œä¸šä»£ç åˆ°æ–‡ä»¶åçš„æ˜ å°„
        self.sector_file_mapping = self._create_sector_file_mapping()
        
    def _create_stock_sector_mapping(self) -> Dict[str, str]:
        """
        ä»çœŸå®çš„è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨åˆ›å»ºè‚¡ç¥¨åˆ°è¡Œä¸šçš„æ˜ å°„å…³ç³»
        """
        mapping = {}
        
        # è¯»å–çœŸå®çš„è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨
        mapping_file = os.path.join(self.data_base_path, "datas_sector_historical/è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv")
        
        if not os.path.exists(mapping_file):
            print(f"è­¦å‘Šï¼šæ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ {mapping_file}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
            return self._create_default_mapping()
        
        try:
            df = pd.read_csv(mapping_file, encoding='utf-8')
            
            for _, row in df.iterrows():
                stock_code = str(row['è‚¡ç¥¨ä»£ç ']).lower()  # è½¬ä¸ºå°å†™
                sector = row['æ‰€å±è¡Œä¸š']
                
                # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼ï¼Œå»æ‰å¯èƒ½çš„å‰ç¼€
                if stock_code.startswith('sh') or stock_code.startswith('sz'):
                    clean_code = stock_code[2:]  # å»æ‰sh/szå‰ç¼€
                else:
                    clean_code = stock_code
                
                # åŒæ—¶ä¿å­˜åŸå§‹æ ¼å¼å’Œæ¸…ç†åçš„æ ¼å¼
                mapping[stock_code] = sector
                mapping[clean_code] = sector
                
                # ä¹Ÿä¿å­˜æˆ‘ä»¬æ•°æ®æ–‡ä»¶ä¸­çš„æ ¼å¼ (å¦‚sz301559)
                if len(clean_code) == 6:
                    if clean_code.startswith('0') or clean_code.startswith('3'):
                        file_format = f"sz{clean_code}"
                    else:
                        file_format = f"sh{clean_code}"
                    mapping[file_format] = sector
            
            print(f"ä»æ˜ å°„è¡¨åŠ è½½äº† {len(df)} ä¸ªè‚¡ç¥¨çš„è¡Œä¸šæ˜ å°„")
            print(f"æ˜ å°„å­—å…¸åŒ…å« {len(mapping)} ä¸ªæ¡ç›®")
            
            # æ˜¾ç¤ºä¸€äº›è¡Œä¸šç»Ÿè®¡
            sector_counts = {}
            for sector in mapping.values():
                sector_counts[sector] = sector_counts.get(sector, 0) + 1
            
            print("ä¸»è¦è¡Œä¸šåˆ†å¸ƒ:")
            for sector, count in sorted(sector_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {sector}: {count} åªè‚¡ç¥¨")
                
        except Exception as e:
            print(f"è¯»å–æ˜ å°„æ–‡ä»¶å‡ºé”™: {e}")
            return self._create_default_mapping()
            
        return mapping
    
    def _create_default_mapping(self) -> Dict[str, str]:
        """åˆ›å»ºé»˜è®¤çš„è‚¡ç¥¨åˆ°è¡Œä¸šæ˜ å°„ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        mapping = {}
        stock_files = glob.glob(os.path.join(self.stock_path, "*.csv"))
        
        # é»˜è®¤è¡Œä¸šåˆ†é…
        default_sectors = ['é“¶è¡Œ', 'åŒ»è¯å•†ä¸š', 'è½¯ä»¶å¼€å‘', 'ä¸“ç”¨è®¾å¤‡', 'é£Ÿå“é¥®æ–™', 
                          'æˆ¿åœ°äº§å¼€å‘', 'çŸ³æ²¹è¡Œä¸š', 'åŒ–å­¦åˆ¶å“', 'èˆªè¿æ¸¯å£']
        
        for i, stock_file in enumerate(stock_files):
            stock_code = os.path.basename(stock_file).replace('.csv', '')
            sector = default_sectors[i % len(default_sectors)]
            mapping[stock_code] = sector
            
        print(f"ä½¿ç”¨é»˜è®¤æ˜ å°„åˆ›å»ºäº† {len(mapping)} ä¸ªè‚¡ç¥¨çš„è¡Œä¸šæ˜ å°„")
        return mapping
    
    def _create_sector_file_mapping(self) -> Dict[str, str]:
        """åˆ›å»ºè¡Œä¸šåç§°åˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„"""
        mapping = {}
        industry_files = glob.glob(os.path.join(self.industry_path, "*.csv"))
        
        for file_path in industry_files:
            filename = os.path.basename(file_path)
            sector_name = filename.split('(')[0]  # æå–è¡Œä¸šåç§°
            mapping[sector_name] = file_path
            
        print(f"æ‰¾åˆ° {len(mapping)} ä¸ªè¡Œä¸šæ¿å—æ–‡ä»¶")
        return mapping
    
    def load_individual_stock_data(self, limit_stocks: Optional[int] = None) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½ä¸ªè‚¡æ•°æ®ï¼Œä¿æŒæ¯åªè‚¡ç¥¨ç‹¬ç«‹
        """
        print("æ­£åœ¨åŠ è½½ä¸ªè‚¡æ•°æ®...")
        
        stock_files = glob.glob(os.path.join(self.stock_path, "*.csv"))
        if limit_stocks:
            stock_files = stock_files[:limit_stocks]
            
        print(f"æ‰¾åˆ° {len(stock_files)} ä¸ªè‚¡ç¥¨æ–‡ä»¶")
        
        stock_data_dict = {}
        
        for i, file_path in enumerate(stock_files):
            if i % 100 == 0:
                progress = (i+1) / len(stock_files) * 100
                print(f"å¤„ç†è¿›åº¦: {i+1}/{len(stock_files)} ({progress:.1f}%)")
                
            try:
                stock_code = os.path.basename(file_path).replace('.csv', '')
                df = pd.read_csv(file_path, encoding='utf-8')
                
                # æ ‡å‡†åŒ–åˆ—åå’Œç´¢å¼•
                df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
                df = df.drop_duplicates(subset=['äº¤æ˜“æ—¥æœŸ']).sort_values('äº¤æ˜“æ—¥æœŸ')
                df = df.set_index('äº¤æ˜“æ—¥æœŸ')
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                df = self._calculate_technical_indicators(df)
                
                # é€‰æ‹©ä¸ªè‚¡ç‰¹å¾ï¼ˆ16ç»´ï¼‰
                feature_cols = [
                    'å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡', 
                    'æ¶¨è·Œå¹…', 'æ¢æ‰‹ç‡', 'æµé€šå¸‚å€¼', 'RSI', 'MACD', 'MACD_signal', 
                    'BB_upper', 'BB_middle', 'BB_lower', 'ATR', 'ROC'
                ]
                
                # åªä¿ç•™å­˜åœ¨çš„åˆ—
                available_cols = [col for col in feature_cols if col in df.columns]
                df_selected = df[available_cols].copy()
                
                # ç¡®ä¿16ç»´ï¼Œä¸è¶³çš„ç”¨0å¡«å……
                while len(df_selected.columns) < 16:
                    df_selected[f'feature_{len(df_selected.columns)}'] = 0
                
                # è¶…è¿‡16ç»´åˆ™æˆªå–å‰16ç»´
                if len(df_selected.columns) > 16:
                    df_selected = df_selected.iloc[:, :16]
                
                stock_data_dict[stock_code] = df_selected
                
            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸåŠ è½½ {len(stock_data_dict)} åªè‚¡ç¥¨çš„æ•°æ®")
        return stock_data_dict
    
    def _calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰"""
        try:
            # RSI
            df['RSI'] = self._calculate_rsi(df['æ”¶ç›˜ä»·'])
            
            # MACD
            macd_data = self._calculate_macd(df['æ”¶ç›˜ä»·'])
            df['MACD'] = macd_data['MACD']
            df['MACD_signal'] = macd_data['MACD_signal']
            
            # å¸ƒæ—å¸¦
            bb_data = self._calculate_bollinger_bands(df['æ”¶ç›˜ä»·'])
            df['BB_upper'] = bb_data['upper']
            df['BB_middle'] = bb_data['middle']
            df['BB_lower'] = bb_data['lower']
            
            # ATR
            df['ATR'] = self._calculate_atr(df['æœ€é«˜ä»·'], df['æœ€ä½ä»·'], df['æ”¶ç›˜ä»·'])
            
            # ROC
            df['ROC'] = df['æ”¶ç›˜ä»·'].pct_change(periods=12) * 100
            
        except Exception as e:
            print(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            
        return df
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(window=period).mean()
        avg_loss = loss.rolling(window=period).mean()
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Dict:
        """è®¡ç®—MACD"""
        exp1 = prices.ewm(span=fast).mean()
        exp2 = prices.ewm(span=slow).mean()
        macd = exp1 - exp2
        macd_signal = macd.ewm(span=signal).mean()
        
        return {
            'MACD': macd,
            'MACD_signal': macd_signal
        }
    
    def _calculate_bollinger_bands(self, prices: pd.Series, period: int = 20, std_dev: int = 2) -> Dict:
        """è®¡ç®—å¸ƒæ—å¸¦"""
        middle = prices.rolling(window=period).mean()
        std = prices.rolling(window=period).std()
        upper = middle + (std * std_dev)
        lower = middle - (std * std_dev)
        
        return {
            'upper': upper,
            'middle': middle,
            'lower': lower
        }
    
    def _calculate_atr(self, high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—ATR"""
        prev_close = close.shift(1)
        tr1 = high - low
        tr2 = abs(high - prev_close)
        tr3 = abs(low - prev_close)
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period).mean()
        return atr
    
    def load_sector_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½è¡Œä¸šæ¿å—æ•°æ®ï¼Œä¿æŒæ¯ä¸ªè¡Œä¸šç‹¬ç«‹"""
        print("æ­£åœ¨åŠ è½½è¡Œä¸šæ¿å—æ•°æ®...")
        
        sector_data_dict = {}
        
        for sector_name, file_path in self.sector_file_mapping.items():
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                
                # å»é‡å¹¶æ’åº
                df = df.drop_duplicates(subset=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
                df = df.set_index('æ—¥æœŸ')
                
                # é€‰æ‹©è¡Œä¸šç‰¹å¾ï¼ˆ6ç»´ï¼šOHLCV + æ¶¨è·Œå¹…ï¼‰
                feature_cols = ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡', 'æ¶¨è·Œå¹…']
                available_cols = [col for col in feature_cols if col in df.columns]
                
                if len(available_cols) >= 4:  # è‡³å°‘è¦æœ‰OHLC
                    df_selected = df[available_cols].copy()
                    
                    # ç¡®ä¿6ç»´
                    while len(df_selected.columns) < 6:
                        df_selected[f'sector_feature_{len(df_selected.columns)}'] = 0
                    
                    if len(df_selected.columns) > 6:
                        df_selected = df_selected.iloc[:, :6]
                    
                    sector_data_dict[sector_name] = df_selected
                    
            except Exception as e:
                print(f"å¤„ç†è¡Œä¸šæ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸåŠ è½½ {len(sector_data_dict)} ä¸ªè¡Œä¸šæ¿å—æ•°æ®")
        return sector_data_dict
    
    def load_index_data(self) -> pd.DataFrame:
        """åŠ è½½æŒ‡æ•°æ•°æ®ï¼ˆ5ç»´ï¼‰"""
        print("æ­£åœ¨åŠ è½½æŒ‡æ•°æ•°æ®...")
        
        # ä¸»è¦æŒ‡æ•°æ˜ å°„
        index_mapping = {
            'zs000001.csv': 'ä¸Šè¯æŒ‡æ•°',
            'zs000300.csv': 'æ²ªæ·±300',
            'zs399001.csv': 'æ·±è¯æˆæŒ‡',
            'zs399006.csv': 'åˆ›ä¸šæ¿æŒ‡',
            'zs000905.csv': 'ä¸­è¯500'
        }
        
        all_index_data = []
        
        for filename, index_name in index_mapping.items():
            file_path = os.path.join(self.index_path, filename)
            
            if not os.path.exists(file_path):
                continue
                
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
                
                # å»é‡å¹¶æ’åº
                df = df.drop_duplicates(subset=['äº¤æ˜“æ—¥æœŸ']).sort_values('äº¤æ˜“æ—¥æœŸ')
                df = df.set_index('äº¤æ˜“æ—¥æœŸ')
                
                # åªå–ä¸€ä¸ªä¸»è¦ç‰¹å¾ï¼ˆæ”¶ç›˜ä»·ï¼‰
                df_selected = df[['æ”¶ç›˜ä»·']].copy()
                df_selected.columns = [f"{index_name}"]
                
                all_index_data.append(df_selected)
                
            except Exception as e:
                print(f"å¤„ç†æŒ‡æ•°æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_index_data:
            # åˆå¹¶æ‰€æœ‰æŒ‡æ•°æ•°æ®
            combined_data = all_index_data[0]
            for df in all_index_data[1:]:
                combined_data = combined_data.join(df, how='outer')
            
            # ç¡®ä¿5ç»´
            while len(combined_data.columns) < 5:
                combined_data[f'index_feature_{len(combined_data.columns)}'] = 0
            
            if len(combined_data.columns) > 5:
                combined_data = combined_data.iloc[:, :5]
                
            combined_data = combined_data.sort_index()
        else:
            # åˆ›å»ºç©ºæ•°æ®
            combined_data = pd.DataFrame()
        
        print(f"æŒ‡æ•°æ•°æ®å½¢çŠ¶: {combined_data.shape}")
        return combined_data
    
    def load_sentiment_data(self) -> pd.DataFrame:
        """
        åŠ è½½æƒ…ç»ªæ•°æ®ï¼šæ˜¨æ—¥è¿æ¿å’Œæ¶¨åœæ¿å—ï¼ˆ2ç»´ï¼‰
        """
        print("æ­£åœ¨åŠ è½½æƒ…ç»ªæ•°æ®...")
        
        # å¯»æ‰¾è¿æ¿å’Œæ¶¨åœç›¸å…³æ¿å—
        concept_files = glob.glob(os.path.join(self.concept_path, "*.csv"))
        
        limit_up_files = []
        consecutive_files = []
        
        for file_path in concept_files:
            filename = os.path.basename(file_path)
            if 'æ¶¨åœ' in filename:
                limit_up_files.append(file_path)
            if 'è¿æ¿' in filename or 'è¿ç»­' in filename:
                consecutive_files.append(file_path)
        

        # å¤„ç†æ¶¨åœæ•°æ®
        limit_up_data = self._process_sentiment_concept_data(limit_up_files[:1], "æ¶¨åœå¼ºåº¦")
        
        # å¤„ç†è¿æ¿æ•°æ®
        consecutive_data = self._process_sentiment_concept_data(consecutive_files[:1], "è¿æ¿å¼ºåº¦")
        
        # åˆå¹¶ä¸º2ç»´æƒ…ç»ªæ•°æ®
        if not limit_up_data.empty and not consecutive_data.empty:
            sentiment_data = pd.concat([limit_up_data, consecutive_data], axis=1)
        elif not limit_up_data.empty:
            sentiment_data = limit_up_data
            sentiment_data['è¿æ¿å¼ºåº¦'] = 0  # æ·»åŠ ç¬¬äºŒç»´
        elif not consecutive_data.empty:
            sentiment_data = consecutive_data
            sentiment_data['æ¶¨åœå¼ºåº¦'] = 0  # æ·»åŠ ç¬¬ä¸€ç»´
        else:
            # åˆ›å»ºç©ºæ•°æ®
            dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')
            sentiment_data = pd.DataFrame({
                'æ¶¨åœå¼ºåº¦': np.zeros(len(dates)),
                'è¿æ¿å¼ºåº¦': np.zeros(len(dates))
            }, index=dates)
        
        # ç¡®ä¿2ç»´
        while len(sentiment_data.columns) < 2:
            sentiment_data[f'sentiment_feature_{len(sentiment_data.columns)}'] = 0
        
        if len(sentiment_data.columns) > 2:
            sentiment_data = sentiment_data.iloc[:, :2]
        
        print(f"æƒ…ç»ªæ•°æ®å½¢çŠ¶: {sentiment_data.shape}")
        return sentiment_data
    
    def _process_sentiment_concept_data(self, file_list: List[str], feature_name: str) -> pd.DataFrame:
        """å¤„ç†æƒ…ç»ªæ¦‚å¿µæ•°æ®"""
        if not file_list:
            return pd.DataFrame()
        
        all_data = []
        
        for file_path in file_list:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                
                # å»é‡å¹¶æ’åº
                df = df.drop_duplicates(subset=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
                df = df.set_index('æ—¥æœŸ')
                
                # ä½¿ç”¨æ¶¨è·Œå¹…ä½œä¸ºå¼ºåº¦æŒ‡æ ‡
                if 'æ¶¨è·Œå¹…' in df.columns:
                    df_selected = df[['æ¶¨è·Œå¹…']].copy()
                    df_selected.columns = [feature_name]
                    all_data.append(df_selected)
                
            except Exception as e:
                print(f"å¤„ç†æƒ…ç»ªæ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue
        
        if all_data:
            # å–å¹³å‡å€¼
            combined_data = pd.concat(all_data, axis=1)
            result = combined_data.mean(axis=1).to_frame(feature_name)
            return result
        
        return pd.DataFrame()
    
    def create_training_samples(self, stock_data_dict: Dict[str, pd.DataFrame],
                              sector_data_dict: Dict[str, pd.DataFrame],
                              index_data: pd.DataFrame,
                              sentiment_data: pd.DataFrame,
                              window_size: int = 30,
                              start_date: Optional[str] = None,
                              end_date: Optional[str] = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        åˆ›å»ºè®­ç»ƒæ ·æœ¬ï¼šæ¯åªè‚¡ç¥¨ä½¿ç”¨å…¶æ‰€å±è¡Œä¸šçš„ç‰¹å¾ï¼Œæ”¯æŒæ—¥æœŸèŒƒå›´æ§åˆ¶
        
        Args:
            stock_data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸
            sector_data_dict: è¡Œä¸šæ•°æ®å­—å…¸  
            index_data: æŒ‡æ•°æ•°æ®
            sentiment_data: æƒ…ç»ªæ•°æ®
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
        """
        print("æ­£åœ¨åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
        
        # è½¬æ¢æ—¥æœŸå‚æ•°
        start_dt = pd.to_datetime(start_date) if start_date else None
        end_dt = pd.to_datetime(end_date) if end_date else None
        
        if start_dt or end_dt:
            date_range_str = f"ä» {start_date if start_date else 'æœ€æ—©'} åˆ° {end_date if end_date else 'æœ€æ–°'}"
            print(f"ğŸ“… æ—¥æœŸèŒƒå›´é™åˆ¶: {date_range_str}")
        
        all_samples = []
        all_targets = []
        all_stock_codes = []
        date_stats = {'min_date': None, 'max_date': None, 'total_dates': 0}
        
        for stock_code, stock_df in stock_data_dict.items():
            try:
                # è·å–è¯¥è‚¡ç¥¨æ‰€å±çš„è¡Œä¸š
                sector_name = self.stock_sector_mapping.get(stock_code, 'é“¶è¡Œ')  # é»˜è®¤é“¶è¡Œ
                
                # å¯»æ‰¾åŒ¹é…çš„è¡Œä¸šæ•°æ®
                sector_df = None
                for sector_key, sector_data in sector_data_dict.items():
                    if sector_name in sector_key:
                        sector_df = sector_data
                        break
                
                if sector_df is None:
                    # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„è¡Œä¸šï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„è¡Œä¸šæ•°æ®
                    if sector_data_dict:
                        sector_df = list(sector_data_dict.values())[0]
                    else:
                        # åˆ›å»ºç©ºçš„è¡Œä¸šæ•°æ®
                        sector_df = pd.DataFrame(
                            np.zeros((len(stock_df), 6)),
                            index=stock_df.index,
                            columns=[f'sector_feature_{i}' for i in range(6)]
                        )
                
                # æ•°æ®å¯¹é½
                common_dates = stock_df.index.intersection(sector_df.index)
                common_dates = common_dates.intersection(index_data.index)
                common_dates = common_dates.intersection(sentiment_data.index)
                
                # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
                if start_dt or end_dt:
                    if start_dt:
                        common_dates = common_dates[common_dates >= start_dt]
                    if end_dt:
                        common_dates = common_dates[common_dates <= end_dt]
                
                # æ›´æ–°æ—¥æœŸç»Ÿè®¡
                if len(common_dates) > 0:
                    current_min = common_dates.min()
                    current_max = common_dates.max()
                    
                    if date_stats['min_date'] is None or current_min < date_stats['min_date']:
                        date_stats['min_date'] = current_min
                    if date_stats['max_date'] is None or current_max > date_stats['max_date']:
                        date_stats['max_date'] = current_max
                    date_stats['total_dates'] = max(date_stats['total_dates'], len(common_dates))
                
                if len(common_dates) < window_size + 1:
                    continue
                
                # é‡æ–°ç´¢å¼•å¹¶æ’åº
                common_dates = common_dates.sort_values()
                stock_aligned = stock_df.reindex(common_dates).fillna(method='ffill').fillna(0)
                sector_aligned = sector_df.reindex(common_dates).fillna(method='ffill').fillna(0)
                index_aligned = index_data.reindex(common_dates).fillna(method='ffill').fillna(0)
                sentiment_aligned = sentiment_data.reindex(common_dates).fillna(method='ffill').fillna(0)
                
                # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
                for i in range(len(common_dates) - window_size):
                    # ç‰¹å¾çª—å£
                    window_start = i
                    window_end = i + window_size
                    
                    # æ‹¼æ¥ç‰¹å¾ï¼šä¸ªè‚¡(16) + è¡Œä¸š(6) + æŒ‡æ•°(5) + æƒ…ç»ª(2) = 29ç»´
                    stock_window = stock_aligned.iloc[window_start:window_end].values  # (30, 16)
                    sector_window = sector_aligned.iloc[window_start:window_end].values  # (30, 6)
                    index_window = index_aligned.iloc[window_start:window_end].values  # (30, 5)
                    sentiment_window = sentiment_aligned.iloc[window_start:window_end].values  # (30, 2)
                    
                    # åˆå¹¶ç‰¹å¾
                    combined_window = np.hstack([
                        stock_window, sector_window, index_window, sentiment_window
                    ])  # (30, 29)
                    
                    # ç›®æ ‡ï¼šä¸‹ä¸€å¤©çš„è‚¡ç¥¨æ”¶ç›Šç‡
                    target_idx = window_end
                    if target_idx < len(stock_aligned) and 'æ¶¨è·Œå¹…' in stock_aligned.columns:
                        target = stock_aligned.iloc[target_idx]['æ¶¨è·Œå¹…']
                    else:
                        # è®¡ç®—æ”¶ç›Šç‡
                        if target_idx < len(stock_aligned) and 'æ”¶ç›˜ä»·' in stock_aligned.columns:
                            current_price = stock_aligned.iloc[window_end - 1]['æ”¶ç›˜ä»·']
                            next_price = stock_aligned.iloc[target_idx]['æ”¶ç›˜ä»·']
                            target = (next_price - current_price) / current_price * 100
                        else:
                            continue
                    
                    all_samples.append(combined_window)
                    all_targets.append(target)
                    all_stock_codes.append(stock_code)
                    
            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {e}")
                continue
        
        if not all_samples:
            raise ValueError("æ²¡æœ‰ç”Ÿæˆä»»ä½•è®­ç»ƒæ ·æœ¬")
        
        X = np.array(all_samples)
        y = np.array(all_targets)
        
        print(f"âœ… ç”Ÿæˆè®­ç»ƒæ ·æœ¬: X={X.shape}, y={y.shape}")
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: ä¸ªè‚¡(16) + è¡Œä¸š(6) + æŒ‡æ•°(5) + æƒ…ç»ª(2) = {X.shape[-1]}ç»´")
        print(f"ğŸ“ˆ æ ·æœ¬ç»Ÿè®¡: æ¶‰åŠ{len(set(all_stock_codes))}åªè‚¡ç¥¨")
        
        # æ‰“å°æ—¥æœŸèŒƒå›´ä¿¡æ¯
        if date_stats['min_date'] and date_stats['max_date']:
            print(f"ğŸ“… å®é™…æ•°æ®æ—¥æœŸèŒƒå›´: {date_stats['min_date'].strftime('%Y-%m-%d')} åˆ° {date_stats['max_date'].strftime('%Y-%m-%d')}")
            print(f"ğŸ“Š æœ€å¤§æœ‰æ•ˆäº¤æ˜“æ—¥æ•°é‡: {date_stats['total_dates']} å¤©")
        
        return X, y, all_stock_codes
    
    def save_processed_data(self, X: np.ndarray, y: np.ndarray, 
                       stock_codes: List[str], save_base_dir: str = "./data/processed", 
                       create_unique_folder: bool = True,
                       date_range_info: Optional[Dict] = None) -> str:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®  
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            save_base_dir: åŸºç¡€ä¿å­˜ç›®å½•
            create_unique_folder: æ˜¯å¦åˆ›å»ºå”¯ä¸€æ ‡è¯†æ–‡ä»¶å¤¹
            date_range_info: æ—¥æœŸèŒƒå›´ä¿¡æ¯
            
        Returns:
            str: å®é™…ä¿å­˜è·¯å¾„
        """
        
        if create_unique_folder:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼šæ—¶é—´æˆ³ + è‚¡ç¥¨æ•°é‡ + æ—¥æœŸèŒƒå›´
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_stocks = len(set(stock_codes))
            
            # æ·»åŠ æ—¥æœŸèŒƒå›´åˆ°æ–‡ä»¶å¤¹å
            if date_range_info and date_range_info.get('start_date') or date_range_info.get('end_date'):
                start_str = date_range_info.get('start_date', '').replace('-', '')
                end_str = date_range_info.get('end_date', '').replace('-', '')
                date_suffix = f"_{start_str}to{end_str}" if start_str or end_str else ""
            else:
                date_suffix = ""
            
            folder_name = f"processed_{timestamp}_stocks{unique_stocks}{date_suffix}"
            save_dir = os.path.join(save_base_dir, folder_name)
        else:
            save_dir = save_base_dir
        
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {save_dir}")
        
        # ä¿å­˜numpyæ•°ç»„
        np.save(os.path.join(save_dir, "X_features.npy"), X)
        np.save(os.path.join(save_dir, "y_targets.npy"), y)
        
        # ä¿å­˜è‚¡ç¥¨ä»£ç åˆ—è¡¨
        with open(os.path.join(save_dir, "stock_codes.json"), 'w', encoding='utf-8') as f:
            json.dump(stock_codes, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ•°æ®ä¿¡æ¯
        info = {
            'X_shape': X.shape,
            'y_shape': y.shape,
            'feature_dims': {
                'stock': 16,
                'sector': 6,
                'index': 5,
                'sentiment': 2,
                'total': X.shape[-1]
            },
            'num_samples': X.shape[0],
            'num_stocks': len(set(stock_codes)),
            'processing_time': datetime.now().isoformat(),
            'save_path': save_dir,
            'folder_name': os.path.basename(save_dir) if create_unique_folder else 'default',
            'date_range': date_range_info or {}
        }
        
        with open(os.path.join(save_dir, "data_info.json"), 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºä¸€ä¸ªREADMEæ–‡ä»¶è¯´æ˜æ•°æ®å†…å®¹
        date_info_str = ""
        if date_range_info and (date_range_info.get('start_date') or date_range_info.get('end_date')):
            start = date_range_info.get('start_date', 'æœ€æ—©')
            end = date_range_info.get('end_date', 'æœ€æ–°')
            date_info_str = f"- æ—¥æœŸèŒƒå›´: {start} åˆ° {end}\n"
        
        readme_content = f"""# è‚¡ç¥¨æ•°æ®å¤„ç†ç»“æœ (æ”¹è¿›ç‰ˆV2)

## å¤„ç†ä¿¡æ¯
- å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- è‚¡ç¥¨æ•°é‡: {len(set(stock_codes))} åª
- æ ·æœ¬æ•°é‡: {X.shape[0]:,} ä¸ª
- ç‰¹å¾ç»´åº¦: {X.shape[-1]} ç»´
{date_info_str}
## æ–‡ä»¶è¯´æ˜
- `X_features.npy`: ç‰¹å¾æ•°æ®ï¼Œå½¢çŠ¶ {X.shape}
- `y_targets.npy`: ç›®æ ‡æ•°æ®ï¼ˆæ”¶ç›Šç‡ï¼‰ï¼Œå½¢çŠ¶ {y.shape}
- `stock_codes.json`: å¯¹åº”çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
- `data_info.json`: è¯¦ç»†çš„æ•°æ®ä¿¡æ¯å’Œå…ƒæ•°æ®
- `README.md`: æœ¬è¯´æ˜æ–‡ä»¶

## ç‰¹å¾æ„æˆ (æ—¶åºçª—å£: 30å¤©)
- ä¸ªè‚¡ç‰¹å¾: 16 ç»´ (OHLCV + æŠ€æœ¯æŒ‡æ ‡ç­‰)
- è¡Œä¸šç‰¹å¾: 6 ç»´ (æ‰€å±è¡Œä¸šçš„OHLCVç­‰)
- æŒ‡æ•°ç‰¹å¾: 5 ç»´ (ä¸»è¦å¸‚åœºæŒ‡æ•°)
- æƒ…ç»ªç‰¹å¾: 2 ç»´ (æ¶¨åœå¼ºåº¦ã€è¿æ¿å¼ºåº¦)

## æ”¹è¿›ç‰ˆç‰¹ç‚¹
- âœ… æ”¯æŒæ—¥æœŸèŒƒå›´æ§åˆ¶
- âœ… å”¯ä¸€æ–‡ä»¶å¤¹å‘½å (å¸¦æ—¶é—´æˆ³å’Œæ—¥æœŸèŒƒå›´)
- âœ… è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯
- âœ… è‡ªåŠ¨ç”Ÿæˆè¯´æ˜æ–‡æ¡£

## ä½¿ç”¨æ–¹æ³•
```python
import numpy as np
import json

# åŠ è½½æ•°æ®
X = np.load('X_features.npy')
y = np.load('y_targets.npy')

# åŠ è½½è‚¡ç¥¨ä»£ç 
with open('stock_codes.json', 'r', encoding='utf-8') as f:
    stock_codes = json.load(f)

# åŠ è½½è¯¦ç»†ä¿¡æ¯
with open('data_info.json', 'r', encoding='utf-8') as f:
    data_info = json.load(f)

print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"ç›®æ ‡å½¢çŠ¶: {y.shape}")
print(f"è‚¡ç¥¨æ•°é‡: {data_info['num_stocks']}")
```
"""
        
        with open(os.path.join(save_dir, "README.md"), 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {save_dir}")
        print("ğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
        print("  - X_features.npy: ç‰¹å¾æ•°æ®")
        print("  - y_targets.npy: ç›®æ ‡æ•°æ®")
        print("  - stock_codes.json: è‚¡ç¥¨ä»£ç åˆ—è¡¨")
        print("  - data_info.json: æ•°æ®ä¿¡æ¯")
        print("  - README.md: è¯´æ˜æ–‡æ¡£")
        
        return save_dir


def main(limit_stocks=None, start_date=None, end_date=None):
    """
    ä¸»å¤„ç†æµç¨‹
    
    Args:
        limit_stocks: é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨è‚¡ç¥¨
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
    """
    print("ğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆæ•°æ®å¤„ç† (V2 - æ”¯æŒæ—¥æœŸèŒƒå›´æ§åˆ¶)...")
    
    if limit_stocks is None:
        print("âš ï¸  æ³¨æ„ï¼šå°†å¤„ç†å…¨éƒ¨è‚¡ç¥¨æ•°æ®ï¼ˆçº¦5400+åªï¼‰ï¼Œé¢„è®¡éœ€è¦è¾ƒé•¿æ—¶é—´å’Œå¤§é‡å†…å­˜")
        print("ğŸ’¡ å»ºè®®ï¼šå¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥è®¾ç½®limit_stockså‚æ•°é™åˆ¶è‚¡ç¥¨æ•°é‡")
        print("ğŸ“Š é¢„è®¡å†…å­˜éœ€æ±‚ï¼š16GB+ RAMï¼Œå¤„ç†æ—¶é—´ï¼š30-60åˆ†é’Ÿ")
    else:
        print(f"ğŸ“Š é™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ï¼š{limit_stocks} åª")
    
    if start_date or end_date:
        date_info = f"ä» {start_date if start_date else 'æœ€æ—©'} åˆ° {end_date if end_date else 'æœ€æ–°'}"
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´é™åˆ¶ï¼š{date_info}")
    
    # åˆ›å»ºæ”¹è¿›çš„æ•°æ®å¤„ç†å™¨
    processor = ImprovedDataProcessorV2(data_base_path="./data")
    
    try:
        # 1. åŠ è½½ä¸ªè‚¡æ•°æ®
        print(f"\nğŸ”„ æ­¥éª¤1/6: åŠ è½½ä¸ªè‚¡æ•°æ®...")
        stock_data_dict = processor.load_individual_stock_data(limit_stocks=limit_stocks)
        
        # 2. åŠ è½½è¡Œä¸šæ¿å—æ•°æ®
        print(f"\nğŸ”„ æ­¥éª¤2/6: åŠ è½½è¡Œä¸šæ¿å—æ•°æ®...")
        sector_data_dict = processor.load_sector_data()
        
        # 3. åŠ è½½æŒ‡æ•°æ•°æ®
        print(f"\nğŸ”„ æ­¥éª¤3/6: åŠ è½½æŒ‡æ•°æ•°æ®...")
        index_data = processor.load_index_data()
        
        # 4. åŠ è½½æƒ…ç»ªæ•°æ®
        print(f"\nğŸ”„ æ­¥éª¤4/6: åŠ è½½æƒ…ç»ªæ•°æ®...")
        sentiment_data = processor.load_sentiment_data()
        
        # 5. åˆ›å»ºè®­ç»ƒæ ·æœ¬ï¼ˆæ·»åŠ æ—¥æœŸèŒƒå›´æ§åˆ¶ï¼‰
        print(f"\nğŸ”„ æ­¥éª¤5/6: åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
        print("â³ è¿™ä¸€æ­¥å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        X, y, stock_codes = processor.create_training_samples(
            stock_data_dict, sector_data_dict, index_data, sentiment_data,
            start_date=start_date, end_date=end_date
        )
        
        # 6. ä¿å­˜å¤„ç†åçš„æ•°æ®
        print(f"\nğŸ”„ æ­¥éª¤6/6: ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        date_range_info = {'start_date': start_date, 'end_date': end_date}
        save_dir = processor.save_processed_data(X, y, stock_codes, 
                                               date_range_info=date_range_info)
        
        print("\nâœ… æ”¹è¿›ç‰ˆæ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶:")
        print(f"  ç‰¹å¾æ•°æ®: {X.shape}")
        print(f"  ç›®æ ‡æ•°æ®: {y.shape}")
        print(f"  ç‰¹å¾ç»´åº¦: {X.shape[-1]} (ä¸ªè‚¡16 + è¡Œä¸š6 + æŒ‡æ•°5 + æƒ…ç»ª2)")
        print(f"  æ¶‰åŠè‚¡ç¥¨: {len(set(stock_codes))} åª")
        print(f"ğŸ—‚ï¸  ä¿å­˜ä½ç½®: {save_dir}")
        
        # æ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  ç›®æ ‡å‡å€¼: {np.mean(y):.4f}")
        print(f"  ç›®æ ‡æ ‡å‡†å·®: {np.std(y):.4f}")
        print(f"  ç›®æ ‡èŒƒå›´: [{np.min(y):.4f}, {np.max(y):.4f}]")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def print_usage():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("\nğŸ’¡ ç”¨æ³•ï¼špython è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py [é€‰é¡¹]")
    print("\nğŸ“ é€‰é¡¹:")
    print("  æ•°å­—           é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼Œå¦‚ 100")
    print("  --start=æ—¥æœŸ   å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD")
    print("  --end=æ—¥æœŸ     ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD")
    print("\nğŸ“ ä½¿ç”¨ç¤ºä¾‹:")
    print("  python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py                           # å¤„ç†å…¨éƒ¨è‚¡ç¥¨ï¼Œå…¨éƒ¨æ—¥æœŸ")
    print("  python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py 100                       # å¤„ç†100åªè‚¡ç¥¨ï¼Œå…¨éƒ¨æ—¥æœŸ")
    print("  python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py --start=2023-01-01        # å…¨éƒ¨è‚¡ç¥¨ï¼Œä»2023å¹´å¼€å§‹")
    print("  python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py --end=2023-12-31          # å…¨éƒ¨è‚¡ç¥¨ï¼Œåˆ°2023å¹´ç»“æŸ")
    print("  python è‚¡æ¿æŒ‡æƒ…æ•°æ®æ±‡æ€»å¤„ç†_æ”¹è¿›ç‰ˆ.py 100 --start=2023-01-01 --end=2023-12-31  # 100åªè‚¡ç¥¨ï¼Œ2023å¹´æ•°æ®")
    print("\nğŸŒŸ æ”¹è¿›ç‰ˆç‰¹è‰²:")
    print("  âœ… æ”¯æŒç²¾ç¡®çš„æ—¥æœŸèŒƒå›´æ§åˆ¶")
    print("  âœ… è‡ªåŠ¨åˆ›å»ºå”¯ä¸€æ—¶é—´æˆ³æ–‡ä»¶å¤¹")
    print("  âœ… è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡å’Œè¯´æ˜æ–‡æ¡£")
    print("  âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º")


if __name__ == "__main__":
    import sys
    
    # å‘½ä»¤è¡Œå‚æ•°å¤„ç†
    if len(sys.argv) > 1:
        try:
            limit_stocks = None
            start_date = None
            end_date = None
            
            # è§£æå‘½ä»¤è¡Œå‚æ•°
            for i, arg in enumerate(sys.argv[1:], 1):
                if arg.startswith('--start='):
                    start_date = arg.split('=')[1]
                elif arg.startswith('--end='):
                    end_date = arg.split('=')[1]
                elif arg == '--help' or arg == '-h':
                    print_usage()
                    sys.exit(0)
                elif arg.isdigit():
                    limit_stocks = int(arg)
                else:
                    print(f"âŒ æœªçŸ¥å‚æ•°: {arg}")
                    print_usage()
                    sys.exit(1)
            
            # éªŒè¯æ—¥æœŸæ ¼å¼
            if start_date:
                try:
                    pd.to_datetime(start_date)
                except:
                    print(f"âŒ é”™è¯¯ï¼šå¼€å§‹æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®: {start_date}")
                    print("ğŸ’¡ æ—¥æœŸæ ¼å¼åº”ä¸º: YYYY-MM-DDï¼Œå¦‚ 2023-01-01")
                    sys.exit(1)
            
            if end_date:
                try:
                    pd.to_datetime(end_date)
                except:
                    print(f"âŒ é”™è¯¯ï¼šç»“æŸæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®: {end_date}")
                    print("ğŸ’¡ æ—¥æœŸæ ¼å¼åº”ä¸º: YYYY-MM-DDï¼Œå¦‚ 2023-12-31")
                    sys.exit(1)
            
            # æ‰“å°å‚æ•°ä¿¡æ¯
            print("ğŸ“‹ è¿è¡Œå‚æ•°:")
            print(f"  è‚¡ç¥¨æ•°é‡é™åˆ¶: {limit_stocks if limit_stocks else 'å…¨éƒ¨'}")
            print(f"  å¼€å§‹æ—¥æœŸ: {start_date if start_date else 'æœ€æ—©'}")
            print(f"  ç»“æŸæ—¥æœŸ: {end_date if end_date else 'æœ€æ–°'}")
            
            main(limit_stocks=limit_stocks, start_date=start_date, end_date=end_date)
            
        except ValueError as e:
            print(f"âŒ å‚æ•°é”™è¯¯: {e}")
            print_usage()
    else:
        # é»˜è®¤å¤„ç†å…¨éƒ¨æ•°æ®
        main(limit_stocks=None, start_date=None, end_date=None)