#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹parquetæ ¼å¼è‚¡ç¥¨æ•°æ®çš„è®­ç»ƒè„šæœ¬
é›†æˆæ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒåŠŸèƒ½
"""

import os
import sys
import yaml
import numpy as np
import pandas as pd
import json
import joblib
import logging
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple, List, Any, Optional
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from stock_data_processor import StockDataProcessor

# æŠ‘åˆ¶å¸¸è§çš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore', category=UserWarning, message='.*X does not have valid feature names.*')
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

import lightgbm as lgb
from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
from sklearn.feature_selection import SelectFromModel, RFE
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å­—ä½“é…ç½®æ¨¡å—
from font_config import setup_chinese_plot
setup_chinese_plot()  # è®¾ç½®ä¸­æ–‡å­—ä½“

try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("è­¦å‘Š: Optuna æœªå®‰è£…ï¼Œè¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("è­¦å‘Š: tqdm æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º")


class SuppressOutput:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå®Œå…¨æŠ‘åˆ¶stdoutå’Œstderrè¾“å‡º"""
    def __enter__(self):
        self.stdout = sys.stdout
        self.stderr = sys.stderr
        sys.stdout = StringIO()
        sys.stderr = StringIO()
        return self
    
    def __exit__(self, *args):
        sys.stdout = self.stdout
        sys.stderr = self.stderr


class StockLightGBMTrainer:
    """è‚¡ç¥¨æ•°æ®ä¸“ç”¨çš„LightGBMè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹
        self.training_folder = self._create_training_folder()
        
        self.setup_logging()
        
        # åˆå§‹åŒ–å˜é‡
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.feature_names = None
        self.scaler = None
        self.model = None
        self.stock_data_processor = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _create_training_folder(self) -> str:
        """åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹"""
        file_naming = self.config.get('output', {}).get('file_naming', {})
        identifier_type = file_naming.get('identifier_type', 'unique_id')
        folder_prefix = file_naming.get('folder_name_prefix', 'stock_training')
        
        if identifier_type == 'timestamp':
            identifier = datetime.now().strftime("%Y%m%d_%H%M%S")
        else:  # unique_id
            digits = file_naming.get('unique_id_digits', 3)
            # æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹ï¼Œç¡®å®šä¸‹ä¸€ä¸ªID
            model_dir = Path(self.config.get('output', {}).get('model_save', {}).get('save_dir', './models/lightgbm_stock'))
            existing_folders = []
            if model_dir.exists():
                existing_folders = [d.name for d in model_dir.iterdir() 
                                  if d.is_dir() and d.name.startswith(folder_prefix)]
            
            # æå–å·²å­˜åœ¨çš„IDå·
            max_id = 0
            for folder in existing_folders:
                try:
                    id_part = folder.replace(folder_prefix + '_', '')
                    if id_part.isdigit():
                        max_id = max(max_id, int(id_part))
                except:
                    continue
            
            identifier = str(max_id + 1).zfill(digits)
        
        training_folder = f"{folder_prefix}_{identifier}"
        return training_folder
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('output', {}).get('logging', {})
        log_level = log_config.get('log_level', 'INFO')
        log_file = log_config.get('log_file', './logs/lightgbm_stock_training.log')
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler() if log_config.get('console_output', True) else logging.NullHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸš€ å¼€å§‹è‚¡ç¥¨LightGBMè®­ç»ƒ: {self.training_folder}")
    
    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # æ¨¡å‹ä¿å­˜ç›®å½•
        model_save_config = self.config.get('output', {}).get('model_save', {})
        self.model_save_dir = Path(model_save_config.get('save_dir', './models/lightgbm_stock')) / self.training_folder
        self.model_save_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»“æœä¿å­˜ç›®å½•
        results_save_config = self.config.get('output', {}).get('results_save', {})
        self.results_save_dir = Path(results_save_config.get('save_dir', './results/lightgbm_stock')) / self.training_folder
        self.results_save_dir.mkdir(parents=True, exist_ok=True)
    
    def preprocess_stock_data(self) -> bool:
        """é¢„å¤„ç†è‚¡ç¥¨æ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            if direct_training.get('enabled', False):
                self.logger.info("ğŸ¯ ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼ï¼Œè·³è¿‡æ•°æ®é¢„å¤„ç†")
                return True
            
            self.logger.info("ğŸ“Š å¼€å§‹è‚¡ç¥¨æ•°æ®é¢„å¤„ç†...")
            
            # è·å–æ•°æ®é…ç½®
            source_data_config = data_config.get('source_data', {})
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨å¤„ç†æ•°æ®
            if source_data_config.get('auto_process', True):
                parquet_dir = source_data_config.get('parquet_dir', './data/professional_parquet')
                output_dir = data_config.get('data_dir', './data/processed_stock_data')
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†å¥½çš„æ•°æ®
                processed_dir = Path(output_dir)
                if processed_dir.exists() and any(processed_dir.glob('processed_*')):
                    self.logger.info("âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®ï¼Œè·³è¿‡é¢„å¤„ç†æ­¥éª¤")
                    return True
                
                # åˆ›å»ºè‚¡ç¥¨æ•°æ®å¤„ç†å™¨
                self.stock_data_processor = StockDataProcessor(
                    data_dir=parquet_dir,
                    output_dir=output_dir
                )
                
                # è·å–è‚¡ç¥¨ç‰¹å®šé…ç½®
                stock_config = data_config.get('stock_specific', {})
                time_series_config = stock_config.get('time_series', {})
                
                # è¿è¡Œæ•°æ®å¤„ç†æµç¨‹
                processed_path = self.stock_data_processor.run_full_pipeline(
                    lookback_days=time_series_config.get('lookback_days', 5),
                    target_days=time_series_config.get('target_days', 1)
                )
                
                if processed_path:
                    # æ›´æ–°é…ç½®ä¸­çš„æ•°æ®è·¯å¾„
                    self.config['data']['data_dir'] = processed_path
                    self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {processed_path}")
                    return True
                else:
                    self.logger.error("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
                    return False
            else:
                self.logger.info("â­ï¸ è·³è¿‡è‡ªåŠ¨æ•°æ®é¢„å¤„ç†")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def load_data(self) -> bool:
        """åŠ è½½æ•°æ®"""
        try:
            self.logger.info("ğŸ“‚ åŠ è½½è‚¡ç¥¨è®­ç»ƒæ•°æ®...")
            
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼
            if direct_training.get('enabled', False):
                return self._load_direct_data()
            
            # åŸæœ‰çš„åŠ è½½é€»è¾‘
            data_dir = Path(data_config.get('data_dir', './data/processed_stock_data'))
            
            # å¦‚æœdata_diræ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„processed_*æ–‡ä»¶å¤¹
            if data_dir.is_dir() and not (data_dir / data_config.get('X_features_file', 'X_features.csv')).exists():
                processed_folders = list(data_dir.glob('processed_*'))
                if processed_folders:
                    # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶å¤¹
                    data_dir = max(processed_folders, key=lambda x: x.stat().st_mtime)
                    self.logger.info(f"   ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶å¤¹: {data_dir}")
            
            # æ–‡ä»¶è·¯å¾„
            X_features_file = data_dir / data_config.get('X_features_file', 'X_features.csv')
            y_targets_file = data_dir / data_config.get('y_targets_file', 'y_targets.csv')
            full_data_file = data_dir / data_config.get('full_data_file', 'full_data.csv')
            
            loading_options = data_config.get('loading_options', {})
            prefer_full_data = loading_options.get('prefer_full_data', True)
            encoding = loading_options.get('encoding', 'utf-8')
            
            # åŠ è½½æ•°æ®
            if prefer_full_data and full_data_file.exists():
                self.logger.info("   åŠ è½½å®Œæ•´æ•°æ®æ–‡ä»¶...")
                full_data = pd.read_csv(full_data_file, encoding=encoding)
                
                # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
                exclude_cols = ['stock_code', 'date', 'target']
                feature_cols = [col for col in full_data.columns if col not in exclude_cols]
                
                self.X = full_data[feature_cols]
                self.y = full_data['target']
                self.stock_info = full_data[['stock_code']].copy() if 'stock_code' in full_data.columns else None
                
            else:
                self.logger.info("   åˆ†åˆ«åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ–‡ä»¶...")
                self.X = pd.read_csv(X_features_file, encoding=encoding)
                self.y = pd.read_csv(y_targets_file, encoding=encoding)
                if hasattr(self.y, 'iloc'):
                    self.y = self.y.iloc[:, 0]  # å–ç¬¬ä¸€åˆ—ä½œä¸ºç›®æ ‡
                self.stock_info = None
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = list(self.X.columns)
            
            self.logger.info(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ:")
            self.logger.info(f"     - ç‰¹å¾ç»´åº¦: {self.X.shape}")
            self.logger.info(f"     - ç›®æ ‡ç»´åº¦: {self.y.shape}")
            self.logger.info(f"     - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _load_direct_data(self) -> bool:
        """ç›´æ¥åŠ è½½parquetæ ¼å¼çš„è‚¡ç¥¨æ•°æ® - ç®€åŒ–ç‰ˆ"""
        try:
            self.logger.info("ğŸ“Š ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼åŠ è½½æ•°æ®...")
            
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            data_dir = Path(data_config.get('data_dir', './data/professional_parquet'))
            data_format = direct_training.get('data_format', 'parquet')
            target_column = direct_training.get('target_column', 'æ¶¨è·Œå¹…')
            exclude_columns = direct_training.get('exclude_columns', ['name', 'æ¶¨è·Œå¹…'])
            
            # åŠ è½½æ•°æ®æ–‡ä»¶
            if data_format == 'parquet':
                # æŸ¥æ‰¾parquetæ–‡ä»¶
                parquet_files = list(data_dir.glob("*.parquet"))
                if not parquet_files:
                    self.logger.error(f"âŒ åœ¨{data_dir}ä¸­æœªæ‰¾åˆ°parquetæ–‡ä»¶")
                    return False
                
                self.logger.info(f"   å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
                
                # ä½¿ç”¨ç®€å•é«˜æ•ˆçš„æ–‡ä»¶é…å¯¹æ–¹æ¡ˆ
                parquet_files = sorted(parquet_files)  # æŒ‰æ—¥æœŸæ’åº
                self.logger.info("   ğŸ“… ä½¿ç”¨æ–‡ä»¶é…å¯¹æ–¹æ¡ˆï¼šä»Šå¤©æ–‡ä»¶ â†’ æ˜å¤©ç›®æ ‡")
                
                features_list = []
                targets_list = []
                processed_pairs = 0
                
                # ç›¸é‚»æ–‡ä»¶é…å¯¹
                for i in range(len(parquet_files) - 1):
                    today_file = parquet_files[i]      # ä»Šå¤©çš„ç‰¹å¾
                    tomorrow_file = parquet_files[i+1]  # æ˜å¤©çš„ç›®æ ‡
                    
                    try:
                        # è¯»å–ä»Šå¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
                        today_data = pd.read_parquet(today_file)
                        # è¯»å–æ˜å¤©çš„æ•°æ®æå–ç›®æ ‡
                        tomorrow_data = pd.read_parquet(tomorrow_file)
                        
                        # æŒ‰è‚¡ç¥¨ä»£ç åŒ¹é…ï¼ˆå–äº¤é›†ï¼‰
                        common_stocks = today_data.index.intersection(tomorrow_data.index)
                        
                        if len(common_stocks) > 0:
                            # ä»Šå¤©çš„æ‰€æœ‰ä¿¡æ¯ä½œä¸ºç‰¹å¾
                            features_list.append(today_data.loc[common_stocks])
                            # æ˜å¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡
                            targets_list.append(tomorrow_data.loc[common_stocks, target_column])
                            processed_pairs += 1
                            
                        self.logger.info(f"   âœ… é…å¯¹: {today_file.name} â†’ {tomorrow_file.name}, è‚¡ç¥¨: {len(common_stocks)}")
                        
                    except Exception as e:
                        self.logger.warning(f"   è·³è¿‡é…å¯¹ {today_file.name} â†’ {tomorrow_file.name}: {e}")
                        continue
                
                if not features_list:
                    self.logger.error("âŒ æ²¡æœ‰æˆåŠŸé…å¯¹ä»»ä½•æ–‡ä»¶")
                    return False
                
                # åˆå¹¶æ‰€æœ‰é…å¯¹çš„æ•°æ®
                self.logger.info(f"   ğŸ”„ åˆå¹¶ {processed_pairs} ä¸ªæ–‡ä»¶é…å¯¹çš„æ•°æ®...")
                full_data = pd.concat(features_list, ignore_index=False)
                targets_data = pd.concat(targets_list, ignore_index=False)
                
                # æ·»åŠ ç›®æ ‡åˆ—
                full_data['next_day_target'] = targets_data
                
                self.logger.info(f"   âœ… æ–‡ä»¶é…å¯¹å®Œæˆ:")
                self.logger.info(f"   - å¤„ç†æ–‡ä»¶å¯¹: {processed_pairs}")
                self.logger.info(f"   - æœ€ç»ˆæ ·æœ¬æ•°: {len(full_data):,}")
                self.logger.info(f"   - ç‰¹å¾åˆ—æ•°: {len(full_data.columns)}")
                
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_format}")
                return False
            
            # æ£€æŸ¥æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
            if 'next_day_target' not in full_data.columns:
                self.logger.error(f"âŒ æœªæ‰¾åˆ°æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ— 'next_day_target'")
                return False
            
            # è®¾ç½®ç›®æ ‡å˜é‡ï¼ˆæ˜å¤©çš„æ¶¨è·Œå¹…ï¼‰
            self.y = full_data['next_day_target']
            actual_target_column = 'next_day_target'
            
            # æ’é™¤ç›®æ ‡åˆ—å’Œè¾…åŠ©åˆ—ï¼Œä¿ç•™ä»Šå¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç‰¹å¾
            exclude_columns = exclude_columns + ['next_day_target']
            self.logger.info(f"   ğŸ’¡ ä»Šå¤©çš„'{target_column}'ç”¨ä½œé¢„æµ‹æ˜å¤©æ¶¨è·Œå¹…çš„ç‰¹å¾")
            
            # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤æŒ‡å®šçš„åˆ—ï¼‰
            feature_columns = [col for col in full_data.columns if col not in exclude_columns]
            self.X = full_data[feature_columns]
            
            # åªä¿ç•™æ•°å€¼åˆ—ä½œä¸ºç‰¹å¾
            numeric_columns = self.X.select_dtypes(include=[np.number]).columns
            self.X = self.X[numeric_columns]
            
            self.logger.info(f"   ğŸ“‹ æ’é™¤çš„åˆ—: {exclude_columns}")
            self.logger.info(f"   ğŸ“Š æ•°å€¼ç‰¹å¾åˆ—æ•°: {len(numeric_columns)}")
            
            # å¤„ç†ç¼ºå¤±å€¼
            self.X = self.X.fillna(0)
            self.y = self.y.fillna(0)
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = list(self.X.columns)
            
            # ä¿å­˜è‚¡ç¥¨ä¿¡æ¯
            stock_name_column = direct_training.get('stock_name_column', 'name')
            if stock_name_column in full_data.columns:
                self.stock_info = full_data[[stock_name_column]].copy()
            else:
                self.stock_info = None
            
            self.logger.info(f"   âœ… æ¬¡æ—¥é¢„æµ‹æ•°æ®åŠ è½½å®Œæˆ:")
            self.logger.info(f"     - ç‰¹å¾ç»´åº¦: {self.X.shape}")
            self.logger.info(f"     - ç›®æ ‡ç»´åº¦: {self.y.shape}")
            self.logger.info(f"     - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            self.logger.info(f"     - ç›®æ ‡åˆ—: {actual_target_column}")
            self.logger.info(f"     - é¢„æµ‹ä»»åŠ¡: ä»Šå¤©ç‰¹å¾ â†’ æ˜å¤©æ¶¨è·Œå¹…")
            self.logger.info(f"     - ç›®æ ‡å€¼èŒƒå›´: [{self.y.min():.4f}, {self.y.max():.4f}]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False


    def split_data(self) -> bool:
        """åˆ†å‰²æ•°æ®"""
        try:
            self.logger.info("âœ‚ï¸ åˆ†å‰²è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®...")
            
            split_config = self.config.get('training', {}).get('data_split', {})
            test_size = split_config.get('test_size', 0.2)
            validation_size = split_config.get('validation_size', 0.1)
            random_state = split_config.get('random_state', 42)
            time_series_split = split_config.get('time_series_split', True)
            
            if time_series_split:
                # æ—¶åºåˆ†å‰²ï¼ˆè‚¡ç¥¨æ•°æ®çš„æ¨èæ–¹å¼ï¼‰
                n_samples = len(self.X)
                test_start = int(n_samples * (1 - test_size))
                val_start = int(n_samples * (1 - test_size - validation_size))
                
                # åˆ†å‰²æ•°æ®
                self.X_train = self.X.iloc[:val_start]
                self.X_val = self.X.iloc[val_start:test_start]
                self.X_test = self.X.iloc[test_start:]
                
                self.y_train = self.y.iloc[:val_start]
                self.y_val = self.y.iloc[val_start:test_start]
                self.y_test = self.y.iloc[test_start:]
                
                self.logger.info("   ä½¿ç”¨æ—¶åºåˆ†å‰²æ–¹å¼")
                
            else:
                # éšæœºåˆ†å‰²
                X_temp, self.X_test, y_temp, self.y_test = train_test_split(
                    self.X, self.y, test_size=test_size, random_state=random_state
                )
                
                val_size_adjusted = validation_size / (1 - test_size)
                self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
                    X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state
                )
                
                self.logger.info("   ä½¿ç”¨éšæœºåˆ†å‰²æ–¹å¼")
            
            self.logger.info(f"   âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
            self.logger.info(f"     - è®­ç»ƒé›†: {self.X_train.shape[0]} æ ·æœ¬")
            self.logger.info(f"     - éªŒè¯é›†: {self.X_val.shape[0]} æ ·æœ¬")
            self.logger.info(f"     - æµ‹è¯•é›†: {self.X_test.shape[0]} æ ·æœ¬")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åˆ†å‰²å¤±è´¥: {e}")
            return False
    
    def preprocess_features(self) -> bool:
        """ç‰¹å¾é¢„å¤„ç†"""
        try:
            self.logger.info("ğŸ”§ ç‰¹å¾é¢„å¤„ç†...")
            
            preprocessing_config = self.config.get('data', {}).get('preprocessing', {})
            normalization_config = preprocessing_config.get('normalization', {})
            outlier_config = preprocessing_config.get('outlier_handling', {})
            
            # æ•°æ®æ ‡å‡†åŒ–
            method = normalization_config.get('method', 'robust')
            if method:
                if method == 'standard':
                    self.scaler = StandardScaler()
                elif method == 'minmax':
                    self.scaler = MinMaxScaler()
                elif method == 'robust':
                    self.scaler = RobustScaler()
                else:
                    self.logger.warning(f"æœªçŸ¥çš„æ ‡å‡†åŒ–æ–¹æ³•: {method}ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                    self.scaler = None
                
                if self.scaler:
                    self.X_train = pd.DataFrame(
                        self.scaler.fit_transform(self.X_train),
                        columns=self.X_train.columns,
                        index=self.X_train.index
                    )
                    self.X_val = pd.DataFrame(
                        self.scaler.transform(self.X_val),
                        columns=self.X_val.columns,
                        index=self.X_val.index
                    )
                    self.X_test = pd.DataFrame(
                        self.scaler.transform(self.X_test),
                        columns=self.X_test.columns,
                        index=self.X_test.index
                    )
                    
                    self.logger.info(f"   âœ… ä½¿ç”¨ {method} æ ‡å‡†åŒ–")
            
            # å¼‚å¸¸å€¼å¤„ç†
            if outlier_config.get('enabled', False):
                method = outlier_config.get('method', 'winsorize')
                if method == 'winsorize':
                    limits = outlier_config.get('winsorize_limits', [0.01, 0.01])
                    from scipy.stats.mstats import winsorize
                    
                    # åªå¯¹è®­ç»ƒé›†è¿›è¡Œwinsorizeï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„é™åˆ¶
                    for col in self.X_train.columns:
                        self.X_train[col] = winsorize(self.X_train[col], limits=limits)
                    
                    self.logger.info(f"   âœ… ä½¿ç”¨ winsorize å¤„ç†å¼‚å¸¸å€¼ï¼Œé™åˆ¶: {limits}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾é¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def train_model(self) -> bool:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
            
            # è·å–æ¨¡å‹å‚æ•°
            lgb_config = self.config.get('lightgbm', {})
            basic_params = lgb_config.get('basic_params', {})
            advanced_params = lgb_config.get('advanced_params', {})
            fit_params = lgb_config.get('fit_params', {})
            
            # åˆå¹¶å‚æ•°
            model_params = {**basic_params, **advanced_params}
            
            # è®­ç»ƒå‚æ•°
            training_config = self.config.get('training', {}).get('training_params', {})
            early_stopping_rounds = training_config.get('early_stopping_rounds', 100)
            verbose = training_config.get('verbose', 100)
            eval_metric = training_config.get('eval_metric', ['rmse'])
            
            # åˆ›å»ºæ•°æ®é›†
            train_data = lgb.Dataset(self.X_train, label=self.y_train)
            val_data = lgb.Dataset(self.X_val, label=self.y_val, reference=train_data)
            
            # è®­ç»ƒæ¨¡å‹
            self.logger.info("   å¼€å§‹è®­ç»ƒ...")
            
            callbacks = [
                lgb.early_stopping(early_stopping_rounds),
                lgb.log_evaluation(verbose)
            ]
            
            self.model = lgb.train(
                model_params,
                train_data,
                valid_sets=[train_data, val_data],
                valid_names=['train', 'val'],
                num_boost_round=fit_params.get('num_boost_round', 1000),
                callbacks=callbacks
            )
            
            self.logger.info("   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def evaluate_model(self) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            self.logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è·å–é¢„æµ‹ç»“æœ
            y_train_pred = self.model.predict(self.X_train)
            y_val_pred = self.model.predict(self.X_val)
            y_test_pred = self.model.predict(self.X_test)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            eval_config = self.config.get('evaluation', {})
            metrics_list = eval_config.get('metrics', ['rmse', 'mae', 'r2_score'])
            
            results = {}
            
            for split, y_true, y_pred in [
                ('train', self.y_train, y_train_pred),
                ('val', self.y_val, y_val_pred),
                ('test', self.y_test, y_test_pred)
            ]:
                split_metrics = {}
                
                for metric in metrics_list:
                    if metric == 'rmse':
                        value = np.sqrt(mean_squared_error(y_true, y_pred))
                    elif metric == 'mae':
                        value = mean_absolute_error(y_true, y_pred)
                    elif metric == 'mape':
                        value = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                    elif metric == 'r2_score':
                        value = r2_score(y_true, y_pred)
                    elif metric == 'explained_variance':
                        value = explained_variance_score(y_true, y_pred)
                    elif metric == 'directional_accuracy':
                        # æ–¹å‘å‡†ç¡®ç‡ï¼ˆè‚¡ç¥¨é¢„æµ‹ç‰¹æœ‰æŒ‡æ ‡ï¼‰
                        direction_true = np.sign(y_true)
                        direction_pred = np.sign(y_pred)
                        value = np.mean(direction_true == direction_pred) * 100
                    else:
                        continue
                    
                    split_metrics[metric] = float(value)
                
                results[split] = split_metrics
            
            # è¾“å‡ºç»“æœ
            self.logger.info("   ğŸ“ˆ è¯„ä¼°ç»“æœ:")
            for split, metrics in results.items():
                self.logger.info(f"     {split.upper()}:")
                for metric, value in metrics.items():
                    if metric in ['mape', 'directional_accuracy']:
                        self.logger.info(f"       {metric}: {value:.2f}%")
                    else:
                        self.logger.info(f"       {metric}: {value:.6f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    def save_model(self) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            self.logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶...")
            
            model_config = self.config.get('output', {}).get('model_save', {})
            model_name = model_config.get('model_name', 'lightgbm_stock_model')
            save_formats = model_config.get('save_format', ['pkl'])
            
            # ä¿å­˜æ¨¡å‹
            for fmt in save_formats:
                if fmt == 'pkl':
                    model_path = self.model_save_dir / f"{model_name}.pkl"
                    joblib.dump(self.model, model_path)
                    self.logger.info(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
                elif fmt == 'txt':
                    model_path = self.model_save_dir / f"{model_name}.txt"
                    self.model.save_model(str(model_path))
                    self.logger.info(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            if self.scaler:
                scaler_path = self.model_save_dir / "scaler.pkl"
                joblib.dump(self.scaler, scaler_path)
                self.logger.info(f"   âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
            
            # ä¿å­˜ç‰¹å¾åç§°
            feature_info = {
                'feature_names': self.feature_names,
                'feature_count': len(self.feature_names),
                'training_config': self.config_path
            }
            
            with open(self.model_save_dir / "feature_names.json", 'w', encoding='utf-8') as f:
                json.dump(feature_info, f, ensure_ascii=False, indent=2)
            
            with open(self.model_save_dir / "feature_names_simple.txt", 'w', encoding='utf-8') as f:
                f.write('\n'.join(self.feature_names))
            
            self.logger.info(f"   âœ… ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def save_results(self, evaluation_results: Dict) -> bool:
        """ä¿å­˜ç»“æœ"""
        try:
            self.logger.info("ğŸ“Š ä¿å­˜è®­ç»ƒç»“æœ...")
            
            results_config = self.config.get('output', {}).get('results_save', {})
            
            # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
            if results_config.get('save_metrics', True):
                metrics_path = self.results_save_dir / "metrics.json"
                with open(metrics_path, 'w', encoding='utf-8') as f:
                    json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
                self.logger.info(f"   âœ… è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            if results_config.get('save_predictions', True):
                predictions = {
                    'y_train_true': self.y_train.tolist(),
                    'y_train_pred': self.model.predict(self.X_train).tolist(),
                    'y_val_true': self.y_val.tolist(),
                    'y_val_pred': self.model.predict(self.X_val).tolist(),
                    'y_test_true': self.y_test.tolist(),
                    'y_test_pred': self.model.predict(self.X_test).tolist()
                }
                
                pred_df = pd.DataFrame({
                    'split': (['train'] * len(self.y_train) + 
                             ['val'] * len(self.y_val) + 
                             ['test'] * len(self.y_test)),
                    'y_true': (self.y_train.tolist() + 
                              self.y_val.tolist() + 
                              self.y_test.tolist()),
                    'y_pred': (predictions['y_train_pred'] + 
                              predictions['y_val_pred'] + 
                              predictions['y_test_pred'])
                })
                
                pred_path = self.results_save_dir / "predictions.csv"
                pred_df.to_csv(pred_path, index=False, encoding='utf-8')
                self.logger.info(f"   âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_path}")
            
            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            if results_config.get('save_feature_importance', True):
                importance = self.model.feature_importance(importance_type='gain')
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                importance_path = self.results_save_dir / "feature_importance.csv"
                importance_df.to_csv(importance_path, index=False, encoding='utf-8')
                
                # ä¿å­˜å‰20ä¸ªé‡è¦ç‰¹å¾
                top_features_path = self.results_save_dir / "top_features.txt"
                with open(top_features_path, 'w', encoding='utf-8') as f:
                    f.write("å‰20ä¸ªæœ€é‡è¦ç‰¹å¾:\n")
                    f.write("=" * 50 + "\n")
                    for i, (_, row) in enumerate(importance_df.head(20).iterrows()):
                        f.write(f"{i+1:2d}. {row['feature']}: {row['importance']:.6f}\n")
                
                self.logger.info(f"   âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def run_training_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹è‚¡ç¥¨LightGBMè®­ç»ƒæµç¨‹...")
            
            # 1. æ•°æ®é¢„å¤„ç†
            if not self.preprocess_stock_data():
                return False
            
            # 2. åŠ è½½æ•°æ®
            if not self.load_data():
                return False
            
            # 3. åˆ†å‰²æ•°æ®
            if not self.split_data():
                return False
            
            # 4. ç‰¹å¾é¢„å¤„ç†
            if not self.preprocess_features():
                return False
            
            # 5. è®­ç»ƒæ¨¡å‹
            if not self.train_model():
                return False
            
            # 6. è¯„ä¼°æ¨¡å‹
            evaluation_results = self.evaluate_model()
            if not evaluation_results:
                return False
            
            # 7. ä¿å­˜æ¨¡å‹
            if not self.save_model():
                return False
            
            # 8. ä¿å­˜ç»“æœ
            if not self.save_results(evaluation_results):
                return False
            
            self.logger.info("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
            self.logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {self.model_save_dir}")
            self.logger.info(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {self.results_save_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LightGBMè‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--config', type=str, 
                       default='config/train/lightGBM_stock_train.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ LightGBMè‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = StockLightGBMTrainer(args.config)
    except Exception as e:
        print(f"âŒ åˆ›å»ºè®­ç»ƒå™¨å¤±è´¥: {e}")
        sys.exit(1)
    
    # è¿è¡Œè®­ç»ƒæµç¨‹
    success = trainer.run_training_pipeline()
    
    if success:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {trainer.model_save_dir}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {trainer.results_save_dir}")
        sys.exit(0)
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        sys.exit(1)


if __name__ == "__main__":
    main()