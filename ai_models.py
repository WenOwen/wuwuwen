# -*- coding: utf-8 -*-
"""
AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - æ¨¡å‹æ¶æ„æ¨¡å—
åŠŸèƒ½ï¼šå®ç°å¤šæ¨¡å‹èåˆçš„è‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import joblib
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ æ¡†æ¶
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Attention, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, RobustScaler

# æœºå™¨å­¦ä¹ æ¨¡å‹
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# æ¨¡å‹è¯„ä¼°
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error


class StockPredictionModel:
    """
    è‚¡ç¥¨é¢„æµ‹æ¨¡å‹åŸºç±»
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.scaler = None
        self.is_fitted = False
        
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒæ¨¡å‹"""
        raise NotImplementedError
        
    def predict(self, X):
        """é¢„æµ‹"""
        raise NotImplementedError
        
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        raise NotImplementedError


class LSTMModel(StockPredictionModel):
    """
    LSTMæ—¶åºé¢„æµ‹æ¨¡å‹
    """
    
    def __init__(self, sequence_length=60, n_features=50, 
                 lstm_units=128, dropout_rate=0.2):
        super().__init__("LSTM")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºLSTMæ¨¡å‹"""
        model = Sequential([
            LSTM(self.lstm_units, return_sequences=True, 
                 input_shape=(self.sequence_length, self.n_features)),
            Dropout(self.dropout_rate),
            
            LSTM(self.lstm_units // 2, return_sequences=True),
            Dropout(self.dropout_rate),
            
            LSTM(self.lstm_units // 4, return_sequences=False),
            Dropout(self.dropout_rate),
            
            Dense(64, activation='relu'),
            Dropout(self.dropout_rate),
            
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')  # äºŒåˆ†ç±»è¾“å‡º
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒLSTMæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†"""
        # é‡å¡‘æ•°æ®ä¸º2Dç”¨äºæ ‡å‡†åŒ–
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        # é‡å¡‘å›3D
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        # è¿”å›[neg_proba, pos_proba]æ ¼å¼
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class TransformerModel(StockPredictionModel):
    """
    Transformeræ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹
    """
    
    def __init__(self, sequence_length=60, n_features=50, 
                 d_model=128, num_heads=8, num_layers=4):
        super().__init__("Transformer")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        inputs = tf.keras.Input(shape=(self.sequence_length, self.n_features))
        
        # è¾“å…¥æŠ•å½±å±‚
        x = Dense(self.d_model)(inputs)
        
        # å¤šå±‚Transformerå—
        for _ in range(self.num_layers):
            # å¤šå¤´æ³¨æ„åŠ›
            attn_output = MultiHeadAttention(
                num_heads=self.num_heads, 
                key_dim=self.d_model // self.num_heads
            )(x, x)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = LayerNormalization()(x + attn_output)
            
            # å‰é¦ˆç½‘ç»œ
            ffn_output = Dense(self.d_model * 2, activation='relu')(x)
            ffn_output = Dense(self.d_model)(ffn_output)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = LayerNormalization()(x + ffn_output)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        
        # åˆ†ç±»å¤´
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.3)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs, outputs)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†ï¼ˆä¸LSTMç›¸åŒï¼‰
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†ï¼ˆä¸LSTMç›¸åŒï¼‰"""
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class XGBoostModel(StockPredictionModel):
    """
    XGBoostæ¢¯åº¦æå‡æ¨¡å‹
    """
    
    def __init__(self, **params):
        super().__init__("XGBoost")
        self.params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 1000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            **params
        }
        self.scaler = RobustScaler()
        
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # å¦‚æœè¾“å…¥æ˜¯3Dï¼ˆæ—¶åºæ•°æ®ï¼‰ï¼Œéœ€è¦flatten
        if len(X_train.shape) == 3:
            X_train_2d = X_train.reshape(X_train.shape[0], -1)
        else:
            X_train_2d = X_train
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train_2d)
        
        # éªŒè¯é›†å¤„ç†
        eval_set = None
        if X_val is not None and y_val is not None:
            if len(X_val.shape) == 3:
                X_val_2d = X_val.reshape(X_val.shape[0], -1)
            else:
                X_val_2d = X_val
            X_val_scaled = self.scaler.transform(X_val_2d)
            eval_set = [(X_val_scaled, y_val)]
        
        # è®­ç»ƒæ¨¡å‹
        self.model = xgb.XGBClassifier(**self.params)
        self.model.fit(
            X_train_scaled, y_train,
            eval_set=eval_set,
            early_stopping_rounds=50,
            verbose=False
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
        if len(X.shape) == 3:
            X_2d = X.reshape(X.shape[0], -1)
        else:
            X_2d = X
            
        X_scaled = self.scaler.transform(X_2d)
        return self.model.predict(X_scaled)
        
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
        if len(X.shape) == 3:
            X_2d = X.reshape(X.shape[0], -1)
        else:
            X_2d = X
            
        X_scaled = self.scaler.transform(X_2d)
        return self.model.predict_proba(X_scaled)


class CNNLSTMModel(StockPredictionModel):
    """
    CNN-LSTMæ··åˆæ¨¡å‹ï¼Œç”¨äºæ•æ‰å±€éƒ¨æ¨¡å¼å’Œé•¿æœŸä¾èµ–
    """
    
    def __init__(self, sequence_length=60, n_features=50):
        super().__init__("CNN-LSTM")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºCNN-LSTMæ¨¡å‹"""
        model = Sequential([
            # CNNå±‚æå–å±€éƒ¨ç‰¹å¾
            Conv1D(filters=64, kernel_size=3, activation='relu', 
                   input_shape=(self.sequence_length, self.n_features)),
            MaxPooling1D(pool_size=2),
            
            Conv1D(filters=128, kernel_size=3, activation='relu'),
            MaxPooling1D(pool_size=2),
            
            Conv1D(filters=64, kernel_size=3, activation='relu'),
            
            # LSTMå±‚æ•æ‰æ—¶åºä¾èµ–
            LSTM(100, return_sequences=True),
            Dropout(0.3),
            
            LSTM(50, return_sequences=False),
            Dropout(0.3),
            
            # å…¨è¿æ¥å±‚
            Dense(50, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†"""
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class EnsembleModel:
    """
    é›†æˆæ¨¡å‹ - å¤šæ¨¡å‹èåˆé¢„æµ‹ç³»ç»Ÿ
    """
    
    def __init__(self, model_weights: Dict[str, float] = None):
        """
        åˆå§‹åŒ–é›†æˆæ¨¡å‹
        
        Args:
            model_weights: æ¨¡å‹æƒé‡å­—å…¸ï¼Œå¦‚ {'LSTM': 0.4, 'XGBoost': 0.3, 'Transformer': 0.2, 'CNN-LSTM': 0.1}
        """
        self.models = {}
        self.model_weights = model_weights or {
            'LSTM': 0.4,
            'XGBoost': 0.3, 
            'Transformer': 0.2,
            'CNN-LSTM': 0.1
        }
        self.performance_history = []
        
    def add_model(self, model: StockPredictionModel):
        """æ·»åŠ æ¨¡å‹åˆ°é›†æˆ"""
        self.models[model.model_name] = model
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹...")
        
        # è®­ç»ƒå„ä¸ªå­æ¨¡å‹
        for model_name, model in self.models.items():
            print(f"\n--- è®­ç»ƒ {model_name} ---")
            model.fit(X_train, y_train, X_val, y_val, **kwargs)
        
        # åŠ¨æ€è°ƒæ•´æƒé‡ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½ï¼‰
        if X_val is not None and y_val is not None:
            self._adjust_weights(X_val, y_val)
        
        print("âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
    def _adjust_weights(self, X_val, y_val):
        """åŸºäºéªŒè¯é›†æ€§èƒ½åŠ¨æ€è°ƒæ•´æ¨¡å‹æƒé‡"""
        print("\nğŸ”§ åŸºäºéªŒè¯é›†æ€§èƒ½è°ƒæ•´æ¨¡å‹æƒé‡...")
        
        model_scores = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                y_pred = model.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)
                model_scores[model_name] = accuracy
                print(f"{model_name} éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åŸºäºæ€§èƒ½é‡æ–°åˆ†é…æƒé‡
        total_score = sum(model_scores.values())
        if total_score > 0:
            for model_name in self.model_weights.keys():
                if model_name in model_scores:
                    self.model_weights[model_name] = model_scores[model_name] / total_score
        
        print("è°ƒæ•´åçš„æ¨¡å‹æƒé‡:")
        for model_name, weight in self.model_weights.items():
            print(f"  {model_name}: {weight:.3f}")
    
    def predict(self, X):
        """é›†æˆé¢„æµ‹"""
        if not self.models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        # æ”¶é›†å„æ¨¡å‹é¢„æµ‹
        predictions = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                predictions[model_name] = model.predict(X)
        
        if not predictions:
            raise ValueError("æ²¡æœ‰å·²è®­ç»ƒçš„æ¨¡å‹")
        
        # åŠ æƒå¹³å‡ï¼ˆç¡¬æŠ•ç¥¨ï¼‰
        ensemble_pred = np.zeros(len(predictions[list(predictions.keys())[0]]))
        
        for model_name, pred in predictions.items():
            weight = self.model_weights.get(model_name, 0)
            ensemble_pred += weight * pred
        
        return (ensemble_pred > 0.5).astype(int)
    
    def predict_proba(self, X):
        """é›†æˆæ¦‚ç‡é¢„æµ‹"""
        if not self.models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        # æ”¶é›†å„æ¨¡å‹æ¦‚ç‡é¢„æµ‹
        prob_predictions = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                prob_predictions[model_name] = model.predict_proba(X)
        
        if not prob_predictions:
            raise ValueError("æ²¡æœ‰å·²è®­ç»ƒçš„æ¨¡å‹")
        
        # åŠ æƒå¹³å‡æ¦‚ç‡
        first_pred = list(prob_predictions.values())[0]
        ensemble_proba = np.zeros_like(first_pred)
        
        for model_name, proba in prob_predictions.items():
            weight = self.model_weights.get(model_name, 0)
            ensemble_proba += weight * proba
        
        return ensemble_proba
    
    def evaluate(self, X_test, y_test):
        """è¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # å„å­æ¨¡å‹æ€§èƒ½
        results = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1]
                
                accuracy = accuracy_score(y_test, y_pred)
                results[model_name] = {
                    'accuracy': accuracy,
                    'predictions': y_pred,
                    'probabilities': y_proba
                }
                
                print(f"{model_name} æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # é›†æˆæ¨¡å‹æ€§èƒ½
        ensemble_pred = self.predict(X_test)
        ensemble_proba = self.predict_proba(X_test)[:, 1]
        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
        
        results['Ensemble'] = {
            'accuracy': ensemble_accuracy,
            'predictions': ensemble_pred,
            'probabilities': ensemble_proba
        }
        
        print(f"é›†æˆæ¨¡å‹ æµ‹è¯•å‡†ç¡®ç‡: {ensemble_accuracy:.4f}")
        
        # è¯¦ç»†æŠ¥å‘Š
        print("\né›†æˆæ¨¡å‹åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, ensemble_pred))
        
        return results
    
    def save_models(self, save_dir: str):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        for model_name, model in self.models.items():
            if model.is_fitted:
                if hasattr(model.model, 'save'):  # Kerasæ¨¡å‹
                    model.model.save(f"{save_dir}/{model_name}_model.h5")
                    joblib.dump(model.scaler, f"{save_dir}/{model_name}_scaler.pkl")
                else:  # sklearnæ¨¡å‹
                    joblib.dump(model.model, f"{save_dir}/{model_name}_model.pkl")
                    joblib.dump(model.scaler, f"{save_dir}/{model_name}_scaler.pkl")
        
        # ä¿å­˜æƒé‡
        joblib.dump(self.model_weights, f"{save_dir}/model_weights.pkl")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")


def create_ensemble_model(sequence_length=60, n_features=50) -> EnsembleModel:
    """
    åˆ›å»ºå®Œæ•´çš„é›†æˆæ¨¡å‹
    
    Args:
        sequence_length: æ—¶åºé•¿åº¦
        n_features: ç‰¹å¾æ•°é‡
        
    Returns:
        EnsembleModelå®ä¾‹
    """
    
    # åˆ›å»ºé›†æˆæ¨¡å‹
    ensemble = EnsembleModel()
    
    # æ·»åŠ å„ä¸ªå­æ¨¡å‹
    lstm_model = LSTMModel(sequence_length, n_features)
    xgb_model = XGBoostModel()
    transformer_model = TransformerModel(sequence_length, n_features)
    cnn_lstm_model = CNNLSTMModel(sequence_length, n_features)
    
    ensemble.add_model(lstm_model)
    ensemble.add_model(xgb_model)
    ensemble.add_model(transformer_model)
    ensemble.add_model(cnn_lstm_model)
    
    return ensemble


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•AIæ¨¡å‹æ¶æ„...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    n_samples, sequence_length, n_features = 1000, 60, 50
    X = np.random.randn(n_samples, sequence_length, n_features)
    y = np.random.randint(0, 2, n_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # åˆ›å»ºå’Œè®­ç»ƒé›†æˆæ¨¡å‹
    ensemble = create_ensemble_model(sequence_length, n_features)
    ensemble.fit(X_train, y_train, X_test, y_test, epochs=5, batch_size=32)
    
    # è¯„ä¼°æ¨¡å‹
    results = ensemble.evaluate(X_test, y_test)
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")