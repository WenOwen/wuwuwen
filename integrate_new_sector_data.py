# -*- coding: utf-8 -*-
"""
æ–°æ¿å—æ•°æ®æ•´åˆè„šæœ¬
å°†è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csvä¸datas_emä¸­çš„è‚¡ç¥¨æ•°æ®æ•´åˆï¼Œå¹¶ç”Ÿæˆè®­ç»ƒç”¨çš„æ¿å—æ˜ å°„æ–‡ä»¶
"""

import pandas as pd
import os
import json
from typing import Dict, List
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def integrate_sector_data():
    """æ•´åˆæ–°çš„æ¿å—æ•°æ®"""
    
    # æ•°æ®è·¯å¾„
    sector_mapping_file = "/home/wangkai/6tdisk/wht/wuwuwen/data/datas_sector/è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨.csv"
    datas_em_dir = "/home/wangkai/6tdisk/wht/wuwuwen/data/datas_em"
    
    # å…¶ä»–æ¿å—æ•°æ®æ–‡ä»¶
    industry_data_file = "/home/wangkai/6tdisk/wht/wuwuwen/data/datas_sector/è¡Œä¸šæ¿å—æ•°æ®.csv"
    concept_data_file = "/home/wangkai/6tdisk/wht/wuwuwen/data/datas_sector/æ¦‚å¿µæ¿å—æ•°æ®.csv"
    hot_concepts_file = "/home/wangkai/6tdisk/wht/wuwuwen/data/datas_sector/çƒ­é—¨æ¦‚å¿µæ’è¡Œ.csv"
    
    # è¾“å‡ºè·¯å¾„
    output_dir = "/home/wangkai/6tdisk/wht/wuwuwen/data"
    
    logger.info("ğŸš€ å¼€å§‹æ•´åˆæ–°çš„æ¿å—æ•°æ®...")
    
    # 1. è¯»å–è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨
    logger.info("ğŸ“Š è¯»å–è‚¡ç¥¨æ¿å—æ˜ å°„è¡¨...")
    df_mapping = pd.read_csv(sector_mapping_file, encoding='utf-8-sig')
    logger.info(f"âœ… æ¿å—æ˜ å°„è¡¨: {len(df_mapping)}åªè‚¡ç¥¨")
    
    # 2. æ£€æŸ¥datas_emä¸­å¯¹åº”çš„è‚¡ç¥¨æ•°æ®ï¼Œå¹¶æ‰¾åˆ°é¢å¤–çš„è‚¡ç¥¨
    logger.info("ğŸ“Š æ£€æŸ¥datas_emä¸­çš„è‚¡ç¥¨æ•°æ®...")
    available_stocks = []
    missing_stocks = []
    extra_stocks = []
    
    # æ£€æŸ¥æ˜ å°„è¡¨ä¸­çš„è‚¡ç¥¨
    mapping_stocks = set(df_mapping['è‚¡ç¥¨ä»£ç '].values)
    
    for _, row in df_mapping.iterrows():
        stock_code = row['è‚¡ç¥¨ä»£ç ']
        stock_file = os.path.join(datas_em_dir, f"{stock_code}.csv")
        
        if os.path.exists(stock_file):
            available_stocks.append(stock_code)
        else:
            missing_stocks.append(stock_code)
    
    # æ£€æŸ¥datas_emä¸­æ˜¯å¦æœ‰é¢å¤–çš„è‚¡ç¥¨
    if os.path.exists(datas_em_dir):
        all_files = [f for f in os.listdir(datas_em_dir) if f.endswith('.csv')]
        all_stock_codes = [f.replace('.csv', '') for f in all_files]
        
        for stock_code in all_stock_codes:
            if stock_code not in mapping_stocks:
                extra_stocks.append(stock_code)
    
    logger.info(f"âœ… æ˜ å°„è¡¨ä¸­æœ‰æ•°æ®çš„è‚¡ç¥¨: {len(available_stocks)}åª")
    logger.info(f"ğŸ“Š datas_emä¸­é¢å¤–çš„è‚¡ç¥¨: {len(extra_stocks)}åª")
    if missing_stocks:
        logger.warning(f"âš ï¸  æ˜ å°„è¡¨ä¸­ç¼ºå¤±æ•°æ®çš„è‚¡ç¥¨: {len(missing_stocks)}åª")
        logger.warning(f"ç¼ºå¤±è‚¡ç¥¨åˆ—è¡¨: {missing_stocks[:10]}...")  # åªæ˜¾ç¤ºå‰10åª
    if extra_stocks:
        logger.info(f"ğŸ“‹ é¢å¤–è‚¡ç¥¨ç¤ºä¾‹: {extra_stocks[:10]}...")  # åªæ˜¾ç¤ºå‰10åª
    
    # 3. åˆ›å»ºè®­ç»ƒç”¨çš„æ¿å—æ˜ å°„æ–‡ä»¶
    logger.info("ğŸ“Š åˆ›å»ºè®­ç»ƒç”¨çš„æ¿å—æ˜ å°„æ–‡ä»¶...")
    
    # é¦–å…ˆé‡å‘½ååŸæœ‰æ•°æ®çš„åˆ—
    df_mapping_renamed = df_mapping.rename(columns={
        'è‚¡ç¥¨ä»£ç ': 'stock_code',
        'è‚¡ç¥¨åç§°': 'stock_name', 
        'æ‰€å±è¡Œä¸š': 'industry',
        'æ¦‚å¿µæ¿å—': 'all_concepts',
        'åœ°åŒº': 'region'
    })
    
    # å¤„ç†æ¦‚å¿µæ¿å—æ•°æ® - æå–ä¸»è¦æ¦‚å¿µ
    def extract_primary_concept(concepts_str):
        if pd.isna(concepts_str) or concepts_str == '':
            return 'æ— æ¦‚å¿µ'
        concepts = concepts_str.split(',')
        # è¿‡æ»¤æ‰ä¸€äº›é€šç”¨æ¦‚å¿µï¼Œä¿ç•™ç‰¹è‰²æ¦‚å¿µ
        filtered_concepts = [c.strip() for c in concepts 
                           if c.strip() not in ['èèµ„èåˆ¸', 'æ²ªè‚¡é€š', 'å¯Œæ—¶ç½—ç´ ', 'æ ‡å‡†æ™®å°”', 'MSCIä¸­å›½']]
        return filtered_concepts[0] if filtered_concepts else concepts[0].strip()
    
    # æ·»åŠ ä¸»è¦æ¦‚å¿µåˆ—
    df_mapping_renamed['primary_concept'] = df_mapping_renamed['all_concepts'].apply(extract_primary_concept)
    
    # è¿‡æ»¤å‡ºæœ‰æ•°æ®çš„è‚¡ç¥¨
    df_training = df_mapping_renamed[df_mapping_renamed['stock_code'].isin(available_stocks)].copy()
    
    # 4. ä¸ºé¢å¤–çš„è‚¡ç¥¨åˆ›å»ºé»˜è®¤æ˜ å°„
    if extra_stocks:
        logger.info(f"ğŸ“Š ä¸º{len(extra_stocks)}åªé¢å¤–è‚¡ç¥¨åˆ›å»ºé»˜è®¤æ¿å—æ˜ å°„...")
        
        extra_rows = []
        for stock_code in extra_stocks:
            # æ ¹æ®è‚¡ç¥¨ä»£ç æ¨æ–­åŸºæœ¬ä¿¡æ¯
            if stock_code.startswith('sh'):
                if stock_code.startswith('sh688'):
                    industry = 'ç§‘åˆ›æ¿'
                    primary_concept = 'ç§‘åˆ›æ¿'
                elif stock_code.startswith('sh601'):
                    industry = 'å¤§ç›˜è‚¡'
                    primary_concept = 'è“ç­¹è‚¡'
                else:
                    industry = 'æ²ªå¸‚ä¸»æ¿'
                    primary_concept = 'ä¸»æ¿'
                region = 'ä¸Šæµ·'
            elif stock_code.startswith('sz'):
                if stock_code.startswith('sz300'):
                    industry = 'åˆ›ä¸šæ¿'
                    primary_concept = 'åˆ›ä¸šæ¿'
                elif stock_code.startswith('sz301'):
                    industry = 'åˆ›ä¸šæ¿'
                    primary_concept = 'åˆ›ä¸šæ¿æ³¨å†Œåˆ¶'
                elif stock_code.startswith('sz002'):
                    industry = 'ä¸­å°æ¿'
                    primary_concept = 'ä¸­å°æ¿'
                else:
                    industry = 'æ·±å¸‚ä¸»æ¿'
                    primary_concept = 'ä¸»æ¿'
                region = 'æ·±åœ³'
            else:
                industry = 'å…¶ä»–'
                primary_concept = 'å…¶ä»–'
                region = 'æœªçŸ¥'
            
            extra_rows.append({
                'stock_code': stock_code,
                'stock_name': f'è‚¡ç¥¨{stock_code}',
                'industry': industry,
                'primary_concept': primary_concept,
                'all_concepts': primary_concept,
                'region': region
            })
        
        # æ·»åŠ é¢å¤–è‚¡ç¥¨åˆ°è®­ç»ƒæ•°æ®
        df_extra = pd.DataFrame(extra_rows)
        df_training = pd.concat([df_training, df_extra], ignore_index=True)
        
        logger.info(f"âœ… å·²æ·»åŠ {len(extra_stocks)}åªé¢å¤–è‚¡ç¥¨çš„é»˜è®¤æ˜ å°„")
    
    # ä¿å­˜è®­ç»ƒç”¨æ˜ å°„æ–‡ä»¶
    training_file = os.path.join(output_dir, "è‚¡ç¥¨æ¿å—æ˜ å°„_è®­ç»ƒç”¨.csv")
    df_training[['stock_code', 'stock_name', 'industry', 'primary_concept', 'all_concepts', 'region']].to_csv(
        training_file, index=False, encoding='utf-8-sig'
    )
    logger.info(f"âœ… è®­ç»ƒç”¨æ¿å—æ˜ å°„å·²ä¿å­˜: {training_file}")
    
    # 4. è¯»å–å¹¶æ•´åˆå…¶ä»–æ¿å—æ•°æ®ï¼ˆç”¨ä½œç‰¹å¾ï¼‰
    logger.info("ğŸ“Š æ•´åˆå…¶ä»–æ¿å—æ•°æ®ä½œä¸ºç‰¹å¾...")
    
    additional_features = {}
    
    # è¡Œä¸šæ¿å—æ•°æ®
    if os.path.exists(industry_data_file):
        df_industry = pd.read_csv(industry_data_file, encoding='utf-8-sig')
        industry_features = {}
        for _, row in df_industry.iterrows():
            industry_features[row['è¡Œä¸šåç§°']] = {
                'industry_return': row['æ¶¨è·Œå¹…'],
                'industry_volume': row['æˆäº¤é‡'],
                'industry_amount': row['æˆäº¤é¢'],
                'industry_net_inflow': row['ä¸»åŠ›å‡€æµå…¥'],
                'industry_net_inflow_ratio': row['ä¸»åŠ›å‡€æµå…¥å æ¯”']
            }
        additional_features['industry_features'] = industry_features
        logger.info(f"âœ… è¡Œä¸šç‰¹å¾: {len(industry_features)}ä¸ªè¡Œä¸š")
    
    # æ¦‚å¿µæ¿å—æ•°æ®
    if os.path.exists(concept_data_file):
        df_concept = pd.read_csv(concept_data_file, encoding='utf-8-sig')
        concept_features = {}
        for _, row in df_concept.iterrows():
            concept_features[row['æ¦‚å¿µåç§°']] = {
                'concept_return': row['æ¶¨è·Œå¹…'],
                'concept_volume': row['æˆäº¤é‡'],
                'concept_amount': row['æˆäº¤é¢'],
                'concept_net_inflow': row['ä¸»åŠ›å‡€æµå…¥'],
                'concept_net_inflow_ratio': row['ä¸»åŠ›å‡€æµå…¥å æ¯”']
            }
        additional_features['concept_features'] = concept_features
        logger.info(f"âœ… æ¦‚å¿µç‰¹å¾: {len(concept_features)}ä¸ªæ¦‚å¿µ")
    
    # çƒ­é—¨æ¦‚å¿µæ’è¡Œ
    if os.path.exists(hot_concepts_file):
        df_hot = pd.read_csv(hot_concepts_file, encoding='utf-8-sig')
        hot_concepts = {}
        for idx, row in df_hot.iterrows():
            hot_concepts[row['æ¦‚å¿µåç§°']] = {
                'hot_rank': idx + 1,
                'hot_return': row['æ¶¨è·Œå¹…'],
                'hot_net_inflow': row['ä¸»åŠ›å‡€æµå…¥'],
                'up_count': row['ä¸Šæ¶¨å®¶æ•°'],
                'down_count': row['ä¸‹è·Œå®¶æ•°']
            }
        additional_features['hot_concepts'] = hot_concepts
        logger.info(f"âœ… çƒ­é—¨æ¦‚å¿µ: {len(hot_concepts)}ä¸ªæ¦‚å¿µ")
    
    # ä¿å­˜é¢å¤–ç‰¹å¾æ•°æ®
    features_file = os.path.join(output_dir, "æ¿å—ç‰¹å¾æ•°æ®.json")
    with open(features_file, 'w', encoding='utf-8') as f:
        json.dump(additional_features, f, ensure_ascii=False, indent=2, default=str)
    logger.info(f"âœ… æ¿å—ç‰¹å¾æ•°æ®å·²ä¿å­˜: {features_file}")
    
    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    logger.info("ğŸ“Š ç”Ÿæˆæ•´åˆæŠ¥å‘Š...")
    
    # ç»Ÿè®¡å„è¡Œä¸šè‚¡ç¥¨æ•°é‡
    industry_stats = df_training['industry'].value_counts()
    logger.info(f"ğŸ“ˆ è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡:")
    for industry, count in industry_stats.head(10).items():
        logger.info(f"  {industry}: {count}åª")
    
    # ç»Ÿè®¡å„åœ°åŒºè‚¡ç¥¨æ•°é‡
    region_stats = df_training['region'].value_counts()
    logger.info(f"ğŸ“ˆ åœ°åŒºåˆ†å¸ƒç»Ÿè®¡:")
    for region, count in region_stats.head(10).items():
        logger.info(f"  {region}: {count}åª")
    
    # ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
    summary_file = os.path.join(output_dir, "æ¿å—æ•°æ®æ•´åˆæ‘˜è¦.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("æ–°æ¿å—æ•°æ®æ•´åˆæ‘˜è¦\n")
        f.write("=" * 50 + "\n")
        f.write(f"æ•´åˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"æ•°æ®ç»Ÿè®¡:\n")
        f.write(f"- æ€»è‚¡ç¥¨æ•°: {len(df_training)}åª\n")
        f.write(f"- è¡Œä¸šæ•°: {df_training['industry'].nunique()}ä¸ª\n")
        f.write(f"- åœ°åŒºæ•°: {df_training['region'].nunique()}ä¸ª\n")
        f.write(f"- åŸæœ‰æ˜ å°„è‚¡ç¥¨: {len(available_stocks)}åª\n")
        f.write(f"- é¢å¤–è¡¥å……è‚¡ç¥¨: {len(extra_stocks)}åª\n")
        f.write(f"- ç¼ºå¤±æ•°æ®è‚¡ç¥¨: {len(missing_stocks)}åª\n\n")
        
        f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
        f.write(f"- è‚¡ç¥¨æ¿å—æ˜ å°„_è®­ç»ƒç”¨.csv: è®­ç»ƒç”¨çš„ä¸»è¦æ˜ å°„æ–‡ä»¶\n")
        f.write(f"- æ¿å—ç‰¹å¾æ•°æ®.json: è¡Œä¸šå’Œæ¦‚å¿µçš„é¢å¤–ç‰¹å¾\n")
        f.write(f"- æ¿å—æ•°æ®æ•´åˆæ‘˜è¦.txt: æœ¬æ–‡ä»¶\n\n")
        
        f.write("è¡Œä¸šåˆ†å¸ƒ (å‰10å):\n")
        for industry, count in industry_stats.head(10).items():
            f.write(f"- {industry}: {count}åª\n")
        
        f.write("\nåœ°åŒºåˆ†å¸ƒ (å‰10å):\n")
        for region, count in region_stats.head(10).items():
            f.write(f"- {region}: {count}åª\n")
    
    logger.info(f"âœ… æ•´åˆæ‘˜è¦å·²ä¿å­˜: {summary_file}")
    logger.info("ğŸ‰ æ–°æ¿å—æ•°æ®æ•´åˆå®Œæˆï¼")
    
    return {
        'training_file': training_file,
        'features_file': features_file,
        'summary_file': summary_file,
        'available_stocks': len(available_stocks),
        'extra_stocks': len(extra_stocks),
        'missing_stocks': len(missing_stocks),
        'total_stocks': len(df_training),
        'total_industries': df_training['industry'].nunique(),
        'total_regions': df_training['region'].nunique()
    }

if __name__ == "__main__":
    result = integrate_sector_data()
    print("\næ•´åˆç»“æœ:")
    print(f"âœ… åŸæœ‰æ˜ å°„è‚¡ç¥¨: {result['available_stocks']}åª")
    print(f"ğŸ“Š é¢å¤–è¡¥å……è‚¡ç¥¨: {result['extra_stocks']}åª")
    print(f"ğŸ¯ æ€»è‚¡ç¥¨æ•°: {result['total_stocks']}åª")
    print(f"âš ï¸  ç¼ºå¤±è‚¡ç¥¨: {result['missing_stocks']}åª") 
    print(f"ğŸ“Š è¡Œä¸šæ•°: {result['total_industries']}ä¸ª")
    print(f"ğŸ“Š åœ°åŒºæ•°: {result['total_regions']}ä¸ª")
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"- {result['training_file']}")
    print(f"- {result['features_file']}")
    print(f"- {result['summary_file']}")