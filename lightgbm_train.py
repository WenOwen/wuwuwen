#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºæ—¶åºç‰¹å¾æ•°æ®è¿›è¡Œè‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒ
"""

import os
import sys
import yaml
import numpy as np
import pandas as pd
import json
import joblib
import logging
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple, List, Any, Optional
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO

# æŠ‘åˆ¶å¸¸è§çš„è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¿›åº¦æ¡æ¸…æ™°
warnings.filterwarnings('ignore', category=UserWarning, message='.*X does not have valid feature names.*')
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

import lightgbm as lgb
from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
from sklearn.feature_selection import SelectFromModel, RFE
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å­—ä½“é…ç½®æ¨¡å—
from font_config import setup_chinese_plot
setup_chinese_plot()  # è®¾ç½®ä¸­æ–‡å­—ä½“

try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("è­¦å‘Š: Optuna æœªå®‰è£…ï¼Œè¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("è­¦å‘Š: tqdm æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º")


class SuppressOutput:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå®Œå…¨æŠ‘åˆ¶stdoutå’Œstderrè¾“å‡º"""
    def __enter__(self):
        self.stdout = sys.stdout
        self.stderr = sys.stderr
        sys.stdout = StringIO()
        sys.stderr = StringIO()
        return self
    
    def __exit__(self, *args):
        sys.stdout = self.stdout
        sys.stderr = self.stderr


class LightGBMTrainer:
    """LightGBM è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹
        self.training_folder = self._create_training_folder()
        
        self.setup_logging()
        
        # åˆå§‹åŒ–å˜é‡
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.feature_names = None
        self.scaler = None
        self.model = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _create_training_folder(self) -> str:
        """åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹"""
        file_naming_config = self.config['output'].get('file_naming', {})
        identifier_type = file_naming_config.get('identifier_type', 'unique_id')
        folder_name_prefix = file_naming_config.get('folder_name_prefix', 'training')
        
        # æ ¹æ®é…ç½®ç”Ÿæˆæ–‡ä»¶å¤¹æ ‡è¯†ç¬¦
        if identifier_type == 'timestamp':
            folder_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        else:
            digits = file_naming_config.get('unique_id_digits', 3)
            # æ£€æŸ¥æ¨¡å‹å’Œç»“æœç›®å½•ï¼Œé€‰æ‹©æ›´å¤§çš„ID
            model_dir = self.config['output']['model_save']['save_dir']
            results_dir = self.config['output']['results_save']['save_dir']
            
            # è·å–ç°æœ‰è®­ç»ƒæ–‡ä»¶å¤¹çš„æœ€å¤§ID
            max_id = 0
            for base_dir in [model_dir, results_dir]:
                if os.path.exists(base_dir):
                    for folder in os.listdir(base_dir):
                        if folder.startswith(folder_name_prefix + '_'):
                            folder_id_part = folder.replace(folder_name_prefix + '_', '')
                            if folder_id_part.isdigit() and len(folder_id_part) == digits:
                                max_id = max(max_id, int(folder_id_part))
            
            folder_id = f"{max_id + 1:0{digits}d}"
        
        folder_name = f"{folder_name_prefix}_{folder_id}"
        return folder_name
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('output', {}).get('logging', {})
        log_level = getattr(logging, log_config.get('log_level', 'INFO'))
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_file = log_config.get('log_file', './logs/lightgbm_training.log')
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler() if log_config.get('console_output', True) else logging.NullHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # åˆ›å»ºåŸºç¡€ç›®å½•
        base_dirs = [
            self.config['output']['model_save']['save_dir'],
            self.config['output']['results_save']['save_dir'],
            os.path.dirname(self.config['output']['logging']['log_file'])
        ]
        
        for dir_path in base_dirs:
            os.makedirs(dir_path, exist_ok=True)
        
        # åˆ›å»ºå½“å‰è®­ç»ƒçš„ä¸“ç”¨æ–‡ä»¶å¤¹
        self.model_save_dir = os.path.join(self.config['output']['model_save']['save_dir'], self.training_folder)
        self.results_save_dir = os.path.join(self.config['output']['results_save']['save_dir'], self.training_folder)
        
        os.makedirs(self.model_save_dir, exist_ok=True)
        os.makedirs(self.results_save_dir, exist_ok=True)
        
        print(f"ğŸ“ åˆ›å»ºè®­ç»ƒæ–‡ä»¶å¤¹: {self.training_folder}")
        print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {self.model_save_dir}")
        print(f"   ç»“æœä¿å­˜è·¯å¾„: {self.results_save_dir}")
    
    def load_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        åŠ è½½æ•°æ®
        
        Returns:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
        """
        self.logger.info("å¼€å§‹åŠ è½½æ•°æ®...")
        
        data_config = self.config['data']
        data_dir = data_config['data_dir']
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_dir):
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
        # è·å–åŠ è½½é€‰é¡¹
        loading_options = data_config.get('loading_options', {})
        prefer_full_data = loading_options.get('prefer_full_data', True)
        encoding = loading_options.get('encoding', 'utf-8')
        validate_data = loading_options.get('validate_data', True)
        
        # ä¼˜å…ˆä½¿ç”¨å®Œæ•´æ•°æ®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ†åˆ«åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ–‡ä»¶
        full_data_file = data_config.get('full_data_file')
        if prefer_full_data and full_data_file and os.path.exists(os.path.join(data_dir, full_data_file)):
            self.logger.info("ä½¿ç”¨å®Œæ•´æ•°æ®æ–‡ä»¶åŠ è½½æ•°æ®...")
            full_data_path = os.path.join(data_dir, full_data_file)
            
            # æ·»åŠ æ•°æ®åŠ è½½è¿›åº¦æç¤º
            if TQDM_AVAILABLE:
                self.logger.info("æ­£åœ¨è¯»å–CSVæ–‡ä»¶...")
                with tqdm(desc="è¯»å–æ•°æ®æ–‡ä»¶", unit="è¡Œ", leave=False) as pbar:
                    df = pd.read_csv(full_data_path, encoding=encoding)
                    pbar.update(len(df))
            else:
                df = pd.read_csv(full_data_path, encoding=encoding)
            
            # æ—¶åºçª—å£æ•°æ®éªŒè¯å’Œæ•´ç†
            self.logger.info("å¤„ç†æ—¶åºçª—å£æ•°æ®...")
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºæ—¶åºçª—å£æ ¼å¼
            step_columns = [col for col in df.columns if col.startswith('step_')]
            if step_columns:
                # åˆ†ææ—¶åºçª—å£ç»“æ„
                step_numbers = set()
                feature_types = set()
                
                for col in step_columns:
                    parts = col.split('_')
                    if len(parts) >= 3:
                        step_numbers.add(parts[1])  # step_XXéƒ¨åˆ†
                        feature_types.add('_'.join(parts[2:]))  # ç‰¹å¾åéƒ¨åˆ†
                
                max_step = max([int(s) for s in step_numbers if s.isdigit()])
                self.logger.info(f"æ£€æµ‹åˆ°æ—¶åºçª—å£æ•°æ®: {max_step + 1} ä¸ªæ—¶é—´æ­¥, {len(feature_types)} ç§ç‰¹å¾ç±»å‹")
                self.logger.info(f"æ—¶é—´çª—å£èŒƒå›´: step_00 åˆ° step_{max_step:02d}")
                
                # éªŒè¯çª—å£å®Œæ•´æ€§
                expected_steps = [f"{i:02d}" for i in range(max_step + 1)]
                missing_steps = set(expected_steps) - step_numbers
                if missing_steps:
                    self.logger.warning(f"å‘ç°ç¼ºå¤±çš„æ—¶é—´æ­¥: {missing_steps}")
                
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
                if 'stock_code' in df.columns:
                    stock_groups = df.groupby('stock_code').size()
                    self.logger.info(f"æ•°æ®åŒ…å« {len(stock_groups)} åªè‚¡ç¥¨")
                    self.logger.info(f"æ¯åªè‚¡ç¥¨çš„æ ·æœ¬æ•°é‡ - æœ€å°‘: {stock_groups.min()}, æœ€å¤š: {stock_groups.max()}, å¹³å‡: {stock_groups.mean():.1f}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„çª—å£æ•°æ®
                    incomplete_stocks = stock_groups[stock_groups < 5].index.tolist()  # å°‘äº5ä¸ªæ ·æœ¬å¯èƒ½æœ‰é—®é¢˜
                    if incomplete_stocks:
                        self.logger.warning(f"å‘ç°æ ·æœ¬æ•°é‡è¾ƒå°‘çš„è‚¡ç¥¨ (< 5ä¸ªæ ·æœ¬): {incomplete_stocks[:5]}..." if len(incomplete_stocks) > 5 else incomplete_stocks)
                
                # ä¿å­˜æ—¶åºçª—å£ä¿¡æ¯
                self.timeseries_info = {
                    'window_size': max_step + 1,
                    'feature_types': list(feature_types),
                    'step_columns': step_columns,
                    'is_timeseries_window': True
                }
                
            else:
                self.logger.info("æœªæ£€æµ‹åˆ°æ—¶åºçª—å£æ ¼å¼ï¼ŒæŒ‰ä¼ ç»Ÿæ•°æ®å¤„ç†")
                self.timeseries_info = {'is_timeseries_window': False}
            
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
            if 'target' in df.columns and 'stock_code' in df.columns:
                # ç§»é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬æ—¥æœŸåˆ—ï¼‰
                excluded_cols = ['target', 'stock_code']
                if 'date' in df.columns:
                    excluded_cols.append('date')
                if 'Date' in df.columns:
                    excluded_cols.append('Date')
                    
                feature_cols = [col for col in df.columns if col not in excluded_cols]
                X = df[feature_cols].values
                y = df['target'].values
                self.stock_codes = df['stock_code'].tolist()
                
                # ä¿å­˜çœŸå®çš„ç‰¹å¾åç§°
                self.original_feature_names = feature_cols
                self.logger.info(f"æå–ç‰¹å¾åˆ— {len(feature_cols)} ä¸ª: {feature_cols[:5]}..." if len(feature_cols) > 5 else f"æå–ç‰¹å¾åˆ—: {feature_cols}")
            else:
                raise ValueError("å®Œæ•´æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: 'target' æˆ– 'stock_code'")
                
        else:
            self.logger.info("åˆ†åˆ«åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ–‡ä»¶...")
            # åˆ†åˆ«åŠ è½½ç‰¹å¾æ•°æ®å’Œç›®æ ‡æ•°æ®
            X_path = os.path.join(data_dir, data_config['X_features_file'])
            y_path = os.path.join(data_dir, data_config['y_targets_file'])
            
            if not os.path.exists(X_path):
                raise FileNotFoundError(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {X_path}")
            if not os.path.exists(y_path):
                raise FileNotFoundError(f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {y_path}")
            
            # åŠ è½½ç‰¹å¾æ•°æ®
            if TQDM_AVAILABLE:
                with tqdm(desc="è¯»å–ç‰¹å¾æ–‡ä»¶", leave=False) as pbar:
                    X_df = pd.read_csv(X_path, encoding=encoding)
                    pbar.update(len(X_df))
            else:
                X_df = pd.read_csv(X_path, encoding=encoding)
            
            # åŠ è½½ç›®æ ‡æ•°æ®ï¼ˆéœ€è¦åŒæ—¶å¤„ç†ä¸¤ä¸ªæ–‡ä»¶çš„æ—¥æœŸè¿‡æ»¤ï¼‰
            if TQDM_AVAILABLE:
                with tqdm(desc="è¯»å–ç›®æ ‡æ–‡ä»¶", leave=False) as pbar:
                    y_df = pd.read_csv(y_path, encoding=encoding)
                    pbar.update(len(y_df))
            else:
                y_df = pd.read_csv(y_path, encoding=encoding)
            
            # æ—¶åºçª—å£æ•°æ®éªŒè¯å’Œæ•´ç†ï¼ˆåˆ†æ–‡ä»¶æ¨¡å¼ï¼‰
            self.logger.info("å¤„ç†æ—¶åºçª—å£æ•°æ®...")
            
            # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶æ˜¯å¦ä¸ºæ—¶åºçª—å£æ ¼å¼
            step_columns = [col for col in X_df.columns if col.startswith('step_')]
            if step_columns:
                # åˆ†ææ—¶åºçª—å£ç»“æ„
                step_numbers = set()
                feature_types = set()
                
                for col in step_columns:
                    parts = col.split('_')
                    if len(parts) >= 3:
                        step_numbers.add(parts[1])  # step_XXéƒ¨åˆ†
                        feature_types.add('_'.join(parts[2:]))  # ç‰¹å¾åéƒ¨åˆ†
                
                max_step = max([int(s) for s in step_numbers if s.isdigit()])
                self.logger.info(f"æ£€æµ‹åˆ°æ—¶åºçª—å£æ•°æ®: {max_step + 1} ä¸ªæ—¶é—´æ­¥, {len(feature_types)} ç§ç‰¹å¾ç±»å‹")
                
                # éªŒè¯Xå’Œyæ–‡ä»¶çš„æ•°æ®ä¸€è‡´æ€§
                if len(X_df) != len(y_df):
                    self.logger.error(f"ç‰¹å¾æ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„è¡Œæ•°ä¸åŒ¹é…: {len(X_df)} vs {len(y_df)}")
                    raise ValueError("ç‰¹å¾æ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„æ•°æ®é‡ä¸åŒ¹é…")
                
                # å¦‚æœyæ–‡ä»¶ä¹Ÿæœ‰è‚¡ç¥¨ä»£ç ï¼ŒéªŒè¯é¡ºåºä¸€è‡´æ€§
                if 'stock_code' in X_df.columns and 'stock_code' in y_df.columns:
                    if not X_df['stock_code'].equals(y_df['stock_code']):
                        self.logger.warning("ç‰¹å¾æ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„è‚¡ç¥¨ä»£ç é¡ºåºä¸ä¸€è‡´ï¼Œæ­£åœ¨å¯¹é½...")
                        # æŒ‰è‚¡ç¥¨ä»£ç å¯¹é½
                        y_df = y_df.set_index('stock_code').loc[X_df['stock_code']].reset_index()
                
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
                if 'stock_code' in X_df.columns:
                    stock_groups = X_df.groupby('stock_code').size()
                    self.logger.info(f"æ•°æ®åŒ…å« {len(stock_groups)} åªè‚¡ç¥¨")
                    self.logger.info(f"æ¯åªè‚¡ç¥¨çš„æ ·æœ¬æ•°é‡ - æœ€å°‘: {stock_groups.min()}, æœ€å¤š: {stock_groups.max()}")
                
                # ä¿å­˜æ—¶åºçª—å£ä¿¡æ¯
                self.timeseries_info = {
                    'window_size': max_step + 1,
                    'feature_types': list(feature_types),
                    'step_columns': step_columns,
                    'is_timeseries_window': True
                }
                
            else:
                self.logger.info("æœªæ£€æµ‹åˆ°æ—¶åºçª—å£æ ¼å¼ï¼ŒæŒ‰ä¼ ç»Ÿæ•°æ®å¤„ç†")
                self.timeseries_info = {'is_timeseries_window': False}
            
            # æå–è‚¡ç¥¨ä»£ç å’Œç‰¹å¾
            if 'stock_code' in X_df.columns:
                self.stock_codes = X_df['stock_code'].tolist()
                # ç§»é™¤è‚¡ç¥¨ä»£ç åˆ—å’Œæ—¥æœŸåˆ—ï¼Œåªä¿ç•™ç‰¹å¾
                excluded_cols = ['stock_code']
                if date_col:
                    excluded_cols.append(date_col)
                feature_cols = [col for col in X_df.columns if col not in excluded_cols]
                X = X_df[feature_cols].values
            else:
                # ä»ç›®æ ‡æ–‡ä»¶ä¸­è·å–è‚¡ç¥¨ä»£ç 
                if 'stock_code' in y_df.columns:
                    self.stock_codes = y_df['stock_code'].tolist()
                else:
                    self.stock_codes = [f'stock_{i}' for i in range(len(X_df))]
                
                # è·å–ç‰¹å¾åˆ—
                excluded_cols = []
                if date_col:
                    excluded_cols.append(date_col)
                feature_cols = [col for col in X_df.columns if col not in excluded_cols]
                X = X_df[feature_cols].values
            
            # ä¿å­˜çœŸå®çš„ç‰¹å¾åç§°
            self.original_feature_names = feature_cols
            self.logger.info(f"æå–ç‰¹å¾åˆ— {len(feature_cols)} ä¸ª: {feature_cols[:5]}..." if len(feature_cols) > 5 else f"æå–ç‰¹å¾åˆ—: {feature_cols}")
            
            # æå–ç›®æ ‡æ•°æ®ï¼ˆy_dfå·²ç»åŠ è½½è¿‡äº†ï¼‰
            if 'target' in y_df.columns:
                y = y_df['target'].values
            else:
                # å¦‚æœæ²¡æœ‰targetåˆ—ï¼Œå‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡
                excluded_cols = ['stock_code']
                if date_col and date_col in y_df.columns:
                    excluded_cols.append(date_col)
                target_cols = [col for col in y_df.columns if col not in excluded_cols]
                if target_cols:
                    y = y_df[target_cols[-1]].values  # ä½¿ç”¨æœ€åä¸€ä¸ªéæ’é™¤åˆ—
                else:
                    y = y_df.iloc[:, -1].values
        
        self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: X.shape={X.shape}, y.shape={y.shape}")
        
        # æ•°æ®éªŒè¯å’Œæ¸…ç†
        if validate_data:
            X, y = self._validate_loaded_data(X, y)
        
        # åŠ è½½è‚¡ç¥¨ä»£ç ä¿¡æ¯ï¼ˆå¦‚æœå•ç‹¬çš„jsonæ–‡ä»¶å­˜åœ¨ï¼‰
        stock_codes_path = os.path.join(data_dir, data_config['stock_codes_file'])
        if os.path.exists(stock_codes_path):
            with open(stock_codes_path, 'r', encoding='utf-8') as f:
                stock_codes_from_json = json.load(f)
                # å¦‚æœCSVä¸­æ²¡æœ‰è‚¡ç¥¨ä»£ç ï¼Œä½¿ç”¨JSONä¸­çš„
                if not hasattr(self, 'stock_codes') or not self.stock_codes:
                    self.stock_codes = stock_codes_from_json
                    
        # åŠ è½½æ•°æ®ä¿¡æ¯
        data_info_path = os.path.join(data_dir, data_config['data_info_file'])
        if os.path.exists(data_info_path):
            with open(data_info_path, 'r', encoding='utf-8') as f:
                self.data_info = json.load(f)
                self.logger.info(f"æ•°æ®ä¿¡æ¯: {self.data_info.get('output_format', 'Unknown')} æ ¼å¼")
        else:
            self.data_info = {}
            
        return X, y
    
    def _validate_loaded_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """éªŒè¯å’Œæ¸…ç†åŠ è½½çš„æ•°æ®"""
        self.logger.info("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        
        # æ£€æŸ¥åŸºæœ¬å½¢çŠ¶
        if len(X) != len(y):
            raise ValueError(f"ç‰¹å¾æ•°æ®å’Œç›®æ ‡æ•°æ®é•¿åº¦ä¸åŒ¹é…: {len(X)} vs {len(y)}")
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼å’Œæ— ç©·å€¼
        X_nan_count = np.isnan(X).sum()
        y_nan_count = np.isnan(y).sum()
        X_inf_count = np.isinf(X).sum()
        y_inf_count = np.isinf(y).sum()
        
        if X_nan_count > 0:
            self.logger.warning(f"ç‰¹å¾æ•°æ®ä¸­å‘ç° {X_nan_count} ä¸ª NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º0")
        if y_nan_count > 0:
            self.logger.warning(f"ç›®æ ‡æ•°æ®ä¸­å‘ç° {y_nan_count} ä¸ª NaN å€¼ï¼Œå°†æ›¿æ¢ä¸º0")
        if X_inf_count > 0:
            self.logger.warning(f"ç‰¹å¾æ•°æ®ä¸­å‘ç° {X_inf_count} ä¸ªæ— ç©·å€¼ï¼Œå°†æ›¿æ¢ä¸º0")
        if y_inf_count > 0:
            self.logger.warning(f"ç›®æ ‡æ•°æ®ä¸­å‘ç° {y_inf_count} ä¸ªæ— ç©·å€¼ï¼Œå°†æ›¿æ¢ä¸º0")
        
        # å¦‚æœå‘ç°é—®é¢˜æ•°æ®ï¼Œè¿›è¡Œæ¸…ç†
        total_issues = X_nan_count + y_nan_count + X_inf_count + y_inf_count
        if total_issues > 0:
            self.logger.info(f"å¼€å§‹æ¸…ç†æ•°æ®ä¸­çš„ {total_issues} ä¸ªé—®é¢˜å€¼...")
            X, y = self._clean_raw_data(X, y)
        
        # æ£€æŸ¥è‚¡ç¥¨ä»£ç æ•°é‡
        if hasattr(self, 'stock_codes') and self.stock_codes:
            if len(self.stock_codes) != len(X):
                self.logger.warning(f"è‚¡ç¥¨ä»£ç æ•°é‡ä¸æ ·æœ¬æ•°é‡ä¸åŒ¹é…: {len(self.stock_codes)} vs {len(X)}")
        
        self.logger.info("æ•°æ®éªŒè¯å®Œæˆ")
        return X, y
    
    def _clean_raw_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æ¸…ç†åŸå§‹æ•°æ®ä¸­çš„NaNå€¼å’Œæ— ç©·å€¼"""
        X_clean = np.copy(X)
        y_clean = np.copy(y)
        
        # å¤„ç†ç‰¹å¾æ•°æ®ä¸­çš„æ— ç©·å€¼å’ŒNaNå€¼
        X_clean[np.isnan(X_clean)] = 0
        X_clean[np.isinf(X_clean)] = 0
        
        # å¤„ç†ç›®æ ‡æ•°æ®ä¸­çš„æ— ç©·å€¼å’ŒNaNå€¼
        y_clean[np.isnan(y_clean)] = 0
        y_clean[np.isinf(y_clean)] = 0
        
        # éªŒè¯æ¸…ç†ç»“æœ
        X_issues_after = np.isnan(X_clean).sum() + np.isinf(X_clean).sum()
        y_issues_after = np.isnan(y_clean).sum() + np.isinf(y_clean).sum()
        
        if X_issues_after == 0 and y_issues_after == 0:
            self.logger.info("åŸå§‹æ•°æ®æ¸…ç†å®Œæˆï¼Œæ‰€æœ‰é—®é¢˜å€¼å·²å¤„ç†")
        else:
            self.logger.warning(f"æ¸…ç†åä»æœ‰é—®é¢˜ï¼šç‰¹å¾æ•°æ® {X_issues_after} ä¸ªï¼Œç›®æ ‡æ•°æ® {y_issues_after} ä¸ª")
        
        return X_clean, y_clean
    
    def preprocess_timeseries_data(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„å¤„ç†æ—¶åºæ•°æ®
        
        Args:
            X: åŸå§‹æ•°æ®ï¼Œå¯èƒ½æ˜¯2D [samples, features] æˆ– 3D [samples, timesteps, features]
            
        Returns:
            å¤„ç†åçš„2Dæ•°æ® [samples, features]
        """
        self.logger.info("å¼€å§‹é¢„å¤„ç†æ—¶åºæ•°æ®...")
        self.logger.info(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {X.shape}")
        
        # æ£€æŸ¥æ•°æ®ç»´åº¦å’Œæ—¶åºçª—å£ç±»å‹
        if len(X.shape) == 2:
            # æ•°æ®å·²ç»æ˜¯2Dï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ—¶åºçª—å£æ•°æ®
            if hasattr(self, 'timeseries_info') and self.timeseries_info.get('is_timeseries_window', False):
                self.logger.info("æ£€æµ‹åˆ°2Dæ—¶åºçª—å£æ•°æ®ï¼Œè¿›è¡Œç‰¹å¾åç§°å¤„ç†...")
                X_processed = X
                
                # ä½¿ç”¨step_æ ¼å¼çš„ç‰¹å¾åç§°
                if hasattr(self, 'original_feature_names') and self.original_feature_names:
                    # éªŒè¯ç‰¹å¾åç§°æ˜¯å¦ä¸ºstepæ ¼å¼
                    step_features = [name for name in self.original_feature_names if name.startswith('step_')]
                    if step_features:
                        self.feature_names = self.original_feature_names.copy()
                        self.logger.info(f"ä½¿ç”¨æ—¶åºçª—å£ç‰¹å¾åç§°: {len(step_features)} ä¸ªstepç‰¹å¾")
                        
                        # åˆ†ææ—¶åºçª—å£ç»“æ„
                        window_size = self.timeseries_info.get('window_size', 30)
                        feature_types = self.timeseries_info.get('feature_types', [])
                        self.logger.info(f"æ—¶åºçª—å£ç»“æ„: {window_size} ä¸ªæ—¶é—´æ­¥ Ã— {len(feature_types)} ç§ç‰¹å¾")
                    else:
                        self.feature_names = self.original_feature_names.copy()
                        self.logger.info(f"ä½¿ç”¨åŸå§‹ç‰¹å¾åç§°: {len(self.feature_names)} ä¸ªç‰¹å¾")
                else:
                    self.feature_names = [f'feature_{i}' for i in range(X_processed.shape[1])]
                    self.logger.warning("æœªæ‰¾åˆ°åŸå§‹ç‰¹å¾åç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°")
            else:
                # ä¼ ç»Ÿ2Dæ•°æ®å¤„ç†
                self.logger.info("æ£€æµ‹åˆ°2Dæ•°æ®ï¼Œè·³è¿‡æ—¶åºé¢„å¤„ç†")
                X_processed = X
                
                if hasattr(self, 'original_feature_names') and self.original_feature_names:
                    self.feature_names = self.original_feature_names.copy()
                    self.logger.info(f"ä½¿ç”¨åŸå§‹ç‰¹å¾åç§°: {len(self.feature_names)} ä¸ªç‰¹å¾")
                else:
                    self.feature_names = [f'feature_{i}' for i in range(X_processed.shape[1])]
                    self.logger.warning("æœªæ‰¾åˆ°åŸå§‹ç‰¹å¾åç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°")
            
        elif len(X.shape) == 3:
            # 3Dæ—¶åºæ•°æ®ï¼Œéœ€è¦æŒ‰é…ç½®å¤„ç†
            method = self.config['data']['preprocessing']['timeseries_method']
            
            if method == 'flatten':
                # ç›´æ¥å±•å¹³
                X_processed = X.reshape(X.shape[0], -1)
                # åŸºäºåŸå§‹ç‰¹å¾åç§°ç”Ÿæˆå±•å¹³åçš„ç‰¹å¾åç§°
                if hasattr(self, 'original_feature_names') and self.original_feature_names:
                    base_names = self.original_feature_names
                else:
                    base_names = [f'feature_{f}' for f in range(X.shape[2])]
                    
                self.feature_names = [f'timestep_{t}_{name}' 
                                    for t in range(X.shape[1]) 
                                    for name in base_names]
                
            elif method == 'statistical':
                # æå–ç»Ÿè®¡ç‰¹å¾
                stat_features = self.config['data']['preprocessing']['feature_engineering']['statistical_features']
                X_processed = self._extract_statistical_features(X, stat_features)
                
            elif method == 'last_step':
                # åªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                X_processed = X[:, -1, :]
                # åŸºäºåŸå§‹ç‰¹å¾åç§°ç”Ÿæˆç‰¹å¾åç§°
                if hasattr(self, 'original_feature_names') and self.original_feature_names:
                    self.feature_names = [f'last_step_{name}' for name in self.original_feature_names]
                else:
                    self.feature_names = [f'last_step_feature_{i}' for i in range(X_processed.shape[1])]
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ—¶åºå¤„ç†æ–¹æ³•: {method}")
                
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç»´åº¦: {len(X.shape)}Dï¼ŒæœŸæœ›2Dæˆ–3D")
        
        self.logger.info(f"æ—¶åºæ•°æ®é¢„å¤„ç†å®Œæˆ: {X.shape} -> {X_processed.shape}")
        return X_processed
    
    def _extract_statistical_features(self, X: np.ndarray, stat_features: List[str]) -> np.ndarray:
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        features_list = []
        feature_names = []
        
        # åŸºäºåŸå§‹ç‰¹å¾åç§°ç”ŸæˆåŸºç¡€åç§°
        if hasattr(self, 'original_feature_names') and self.original_feature_names:
            base_feature_names = self.original_feature_names
        else:
            base_feature_names = [f'feature_{j}' for j in range(X.shape[2])]
        
        for i, base_name in enumerate(base_feature_names):
            feature_data = X[:, :, i]  # [samples, timesteps]
            
            for stat in stat_features:
                if stat == 'mean':
                    features_list.append(np.mean(feature_data, axis=1))
                elif stat == 'std':
                    features_list.append(np.std(feature_data, axis=1))
                elif stat == 'max':
                    features_list.append(np.max(feature_data, axis=1))
                elif stat == 'min':
                    features_list.append(np.min(feature_data, axis=1))
                elif stat == 'median':
                    features_list.append(np.median(feature_data, axis=1))
                elif stat == 'skew':
                    features_list.append(stats.skew(feature_data, axis=1))
                elif stat == 'kurtosis':
                    features_list.append(stats.kurtosis(feature_data, axis=1))
                
                feature_names.append(f'{base_name}_{stat}')
        
        self.feature_names = feature_names
        self.logger.info(f"ç”Ÿæˆç»Ÿè®¡ç‰¹å¾åç§°: {len(feature_names)} ä¸ªï¼Œç¤ºä¾‹: {feature_names[:5]}...")
        return np.column_stack(features_list)
    
    def add_technical_features(self, X: np.ndarray, raw_X: np.ndarray) -> np.ndarray:
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        if not self.config['data']['preprocessing']['feature_engineering']['technical_features']['enabled']:
            return X
        
        # å¯¹äºæ—¶åºçª—å£æ•°æ®ï¼Œå¯ä»¥è®¡ç®—ä¸€äº›åŸºäºçª—å£çš„æŠ€æœ¯æŒ‡æ ‡
        if hasattr(self, 'timeseries_info') and self.timeseries_info.get('is_timeseries_window', False):
            self.logger.info("ä¸ºæ—¶åºçª—å£æ•°æ®è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
            
            try:
                tech_features = []
                tech_feature_names = []
                
                # ä»æ—¶åºçª—å£æ•°æ®ä¸­æå–æŠ€æœ¯æŒ‡æ ‡
                window_size = self.timeseries_info.get('window_size', 30)
                feature_types = self.timeseries_info.get('feature_types', [])
                
                # å¯»æ‰¾ä»·æ ¼ç›¸å…³çš„ç‰¹å¾
                price_features = [ft for ft in feature_types if any(keyword in ft for keyword in ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·'])]
                volume_features = [ft for ft in feature_types if 'æˆäº¤é‡' in ft or 'æˆäº¤é¢' in ft]
                
                self.logger.info(f"å‘ç° {len(price_features)} ç§ä»·æ ¼ç‰¹å¾, {len(volume_features)} ç§æˆäº¤é‡ç‰¹å¾")
                
                if price_features:
                    # è®¡ç®—åŸºäºæ—¶åºçª—å£çš„æŠ€æœ¯æŒ‡æ ‡
                    window_tech_features = self._calculate_window_technical_features(X, window_size, feature_types)
                    tech_features.extend(window_tech_features['features'])
                    tech_feature_names.extend(window_tech_features['names'])
                
                if tech_features:
                    tech_array = np.column_stack(tech_features)
                    X_enhanced = np.column_stack([X, tech_array])
                    self.feature_names.extend(tech_feature_names)
                    self.logger.info(f"æ—¶åºçª—å£æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æ·»åŠ å®Œæˆ: {X.shape} -> {X_enhanced.shape}")
                    return X_enhanced
                
            except Exception as e:
                self.logger.warning(f"æ—¶åºçª—å£æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡æŠ€æœ¯ç‰¹å¾")
                
            return X
        
        # ä¼ ç»Ÿçš„3Dæ•°æ®æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
        if len(raw_X.shape) == 2:
            self.logger.info("åŸå§‹æ•°æ®å·²ä¸º2Dæ ¼å¼ï¼Œè·³è¿‡ä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡è®¡ç®—")
            return X
            
        elif len(raw_X.shape) != 3:
            self.logger.warning(f"åŸå§‹æ•°æ®ç»´åº¦å¼‚å¸¸: {raw_X.shape}ï¼Œè·³è¿‡æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾è®¡ç®—")
            return X
            
        self.logger.info("æ·»åŠ ä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
        
        tech_features = []
        tech_feature_names = []
        
        try:
            # è®¡ç®—åŠ¨é‡ç‰¹å¾
            momentum_features = self._calculate_momentum_features(raw_X)
            tech_features.extend(momentum_features)
            tech_feature_names.extend([f'momentum_{i}' for i in range(len(momentum_features))])
            
            # è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾
            volatility_features = self._calculate_volatility_features(raw_X)
            tech_features.extend(volatility_features)
            tech_feature_names.extend([f'volatility_{i}' for i in range(len(volatility_features))])
            
            # è®¡ç®—è¶‹åŠ¿ç‰¹å¾
            trend_features = self._calculate_trend_features(raw_X)
            tech_features.extend(trend_features)
            tech_feature_names.extend([f'trend_{i}' for i in range(len(trend_features))])
            
            # åˆå¹¶ç‰¹å¾
            if tech_features:
                tech_array = np.column_stack(tech_features)
                X_enhanced = np.column_stack([X, tech_array])
                self.feature_names.extend(tech_feature_names)
                self.logger.info(f"æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æ·»åŠ å®Œæˆ: {X.shape} -> {X_enhanced.shape}")
                return X_enhanced
                
        except Exception as e:
            self.logger.warning(f"æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡æŠ€æœ¯ç‰¹å¾")
            
        return X
    
    def _calculate_window_technical_features(self, X: np.ndarray, window_size: int, feature_types: List[str]) -> Dict:
        """è®¡ç®—åŸºäºæ—¶åºçª—å£çš„æŠ€æœ¯æŒ‡æ ‡"""
        tech_features = []
        tech_feature_names = []
        
        # å¯»æ‰¾ä»·æ ¼ç‰¹å¾çš„åˆ—ç´¢å¼•
        price_cols = {}
        for i, feature_name in enumerate(self.feature_names):
            for price_type in ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·']:
                if price_type in feature_name:
                    step_match = feature_name.split('_')[1] if feature_name.startswith('step_') else None
                    if step_match:
                        if price_type not in price_cols:
                            price_cols[price_type] = []
                        price_cols[price_type].append((i, int(step_match)))
        
        # å¯¹æ¯ç§ä»·æ ¼ç±»å‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        for price_type, col_info in price_cols.items():
            if len(col_info) >= window_size:
                # æŒ‰æ—¶é—´æ­¥æ’åº
                col_info.sort(key=lambda x: x[1])
                price_indices = [col[0] for col in col_info]
                
                # æå–è¯¥ä»·æ ¼ç±»å‹çš„æ—¶åºæ•°æ®
                price_series = X[:, price_indices]  # [samples, time_steps]
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                # 1. ç§»åŠ¨å¹³å‡ï¼ˆä¸åŒçª—å£å¤§å°ï¼‰
                for ma_window in [5, 10, 20]:
                    if ma_window <= window_size:
                        ma_values = []
                        for i in range(len(price_series)):
                            series = price_series[i]
                            # è®¡ç®—æœ€åma_windowä¸ªæ•°æ®çš„ç§»åŠ¨å¹³å‡
                            if len(series) >= ma_window:
                                window_data = series[-ma_window:]
                                # è¿‡æ»¤æ‰NaNå€¼
                                valid_data = window_data[~np.isnan(window_data)]
                                if len(valid_data) > 0:
                                    ma = np.mean(valid_data)
                                    ma_values.append(ma)
                                else:
                                    ma_values.append(0)  # å¦‚æœå…¨æ˜¯NaNï¼Œä½¿ç”¨0
                            else:
                                ma_values.append(0)
                        
                        tech_features.append(np.array(ma_values))
                        tech_feature_names.append(f'MA{ma_window}_{price_type}')
                
                # 2. ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡RSIçš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆåŸºäºæœ€è¿‘å‡ å¤©çš„æ¶¨è·Œï¼‰
                rsi_values = []
                for i in range(len(price_series)):
                    series = price_series[i]
                    if len(series) >= 14:
                        # è®¡ç®—ä»·æ ¼å˜åŒ–
                        price_changes = np.diff(series[-14:])
                        gains = price_changes[price_changes > 0]
                        losses = -price_changes[price_changes < 0]
                        
                        avg_gain = np.mean(gains) if len(gains) > 0 else 0
                        avg_loss = np.mean(losses) if len(losses) > 0 else 0
                        
                        if avg_loss != 0 and not np.isnan(avg_loss) and not np.isnan(avg_gain):
                            rs = avg_gain / avg_loss
                            rsi = 100 - (100 / (1 + rs))
                            # ç¡®ä¿RSIåœ¨0-100èŒƒå›´å†…
                            rsi = np.clip(rsi, 0, 100)
                        else:
                            rsi = 100 if avg_gain > 0 else 50
                        
                        rsi_values.append(rsi)
                    else:
                        rsi_values.append(50)  # é»˜è®¤ä¸­æ€§å€¼
                
                tech_features.append(np.array(rsi_values))
                tech_feature_names.append(f'RSI_{price_type}')
                
                # 3. ä»·æ ¼æ³¢åŠ¨ç‡ï¼ˆæ ‡å‡†å·®ï¼‰
                volatility_values = []
                for i in range(len(price_series)):
                    series = price_series[i]
                    vol = np.std(series) if len(series) > 1 else 0
                    volatility_values.append(vol)
                
                tech_features.append(np.array(volatility_values))
                tech_feature_names.append(f'Volatility_{price_type}')
                
                # 4. åŠ¨é‡æŒ‡æ ‡ï¼ˆæœ€æ–°ä»·æ ¼ç›¸å¯¹äºNå¤©å‰çš„å˜åŒ–ç‡ï¼‰
                for momentum_period in [5, 10]:
                    if momentum_period < window_size:
                        momentum_values = []
                        for i in range(len(price_series)):
                            series = price_series[i]
                            if len(series) > momentum_period:
                                prev_price = series[-momentum_period-1]
                                curr_price = series[-1]
                                
                                # é˜²æ­¢é™¤é›¶é”™è¯¯
                                if prev_price != 0 and not np.isnan(prev_price) and not np.isnan(curr_price):
                                    momentum = (curr_price - prev_price) / abs(prev_price) * 100
                                    # é™åˆ¶åŠ¨é‡å€¼èŒƒå›´ï¼Œé˜²æ­¢æç«¯å€¼
                                    momentum = np.clip(momentum, -1000, 1000)
                                else:
                                    momentum = 0
                                
                                momentum_values.append(momentum)
                            else:
                                momentum_values.append(0)
                        
                        tech_features.append(np.array(momentum_values))
                        tech_feature_names.append(f'Momentum{momentum_period}D_{price_type}')
        
        # æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— ç©·å€¼å’ŒNaNå€¼
        cleaned_features = []
        for i, feature_array in enumerate(tech_features):
            # æ›¿æ¢æ— ç©·å€¼å’ŒNaNå€¼
            clean_feature = np.copy(feature_array)
            
            # å¤„ç†æ— ç©·å€¼
            clean_feature[np.isinf(clean_feature)] = 0
            
            # å¤„ç†NaNå€¼
            clean_feature[np.isnan(clean_feature)] = 0
            
            # å¤„ç†æç«¯å€¼ï¼ˆè¶…è¿‡10ä¸ªæ ‡å‡†å·®çš„å€¼ï¼‰
            if len(clean_feature) > 1:
                std_val = np.std(clean_feature)
                mean_val = np.mean(clean_feature)
                if std_val > 0:
                    # é™åˆ¶åœ¨Â±10ä¸ªæ ‡å‡†å·®èŒƒå›´å†…
                    outlier_mask = np.abs(clean_feature - mean_val) > 10 * std_val
                    clean_feature[outlier_mask] = mean_val
            
            cleaned_features.append(clean_feature)
        
        return {
            'features': cleaned_features,
            'names': tech_feature_names
        }
    
    def _calculate_momentum_features(self, X: np.ndarray) -> List[np.ndarray]:
        """è®¡ç®—åŠ¨é‡ç‰¹å¾"""
        features = []
        
        # ç®€å•åŠ¨é‡ (æœ€åå€¼ - ç¬¬ä¸€å€¼)
        momentum = X[:, -1, :] - X[:, 0, :]
        for i in range(momentum.shape[1]):
            features.append(momentum[:, i])
            
        return features
    
    def _calculate_volatility_features(self, X: np.ndarray) -> List[np.ndarray]:
        """è®¡ç®—æ³¢åŠ¨ç‡ç‰¹å¾"""
        features = []
        
        # æ ‡å‡†å·®ä½œä¸ºæ³¢åŠ¨ç‡
        volatility = np.std(X, axis=1)
        for i in range(volatility.shape[1]):
            features.append(volatility[:, i])
            
        return features
    
    def _calculate_trend_features(self, X: np.ndarray) -> List[np.ndarray]:
        """è®¡ç®—è¶‹åŠ¿ç‰¹å¾"""
        features = []
        
        # çº¿æ€§è¶‹åŠ¿æ–œç‡
        time_steps = np.arange(X.shape[1])
        for feat_idx in range(X.shape[2]):
            slopes = []
            for sample_idx in range(X.shape[0]):
                slope, _, _, _, _ = stats.linregress(time_steps, X[sample_idx, :, feat_idx])
                slopes.append(slope)
            features.append(np.array(slopes))
            
        return features
    
    def _clean_data_for_scaling(self, X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """æ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å€¼ã€NaNå€¼å’Œæç«¯å€¼ï¼Œä¸ºæ ‡å‡†åŒ–åšå‡†å¤‡"""
        def clean_array(X: np.ndarray, array_name: str) -> np.ndarray:
            X_clean = np.copy(X)
            
            # ç»Ÿè®¡é—®é¢˜æ•°æ®
            inf_count = np.isinf(X_clean).sum()
            nan_count = np.isnan(X_clean).sum()
            
            if inf_count > 0 or nan_count > 0:
                self.logger.warning(f"{array_name}: å‘ç° {inf_count} ä¸ªæ— ç©·å€¼, {nan_count} ä¸ªNaNå€¼")
            
            # å¤„ç†æ— ç©·å€¼ï¼šæ›¿æ¢ä¸º0
            X_clean[np.isinf(X_clean)] = 0
            
            # å¤„ç†NaNå€¼ï¼šæ›¿æ¢ä¸º0  
            X_clean[np.isnan(X_clean)] = 0
            
            # å¤„ç†æç«¯å€¼ï¼šæ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å¤„ç†
            for feature_idx in range(X_clean.shape[1]):
                feature_values = X_clean[:, feature_idx]
                if len(feature_values) > 1:
                    # è®¡ç®—è¯¥ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
                    q75 = np.percentile(feature_values, 75)
                    q25 = np.percentile(feature_values, 25)
                    iqr = q75 - q25
                    
                    if iqr > 0:
                        # ä½¿ç”¨IQRæ–¹æ³•è¯†åˆ«æç«¯å€¼
                        lower_bound = q25 - 3.0 * iqr
                        upper_bound = q75 + 3.0 * iqr
                        
                        # é™åˆ¶æç«¯å€¼
                        outlier_mask = (feature_values < lower_bound) | (feature_values > upper_bound)
                        if outlier_mask.any():
                            # ç”¨ä¸­ä½æ•°æ›¿æ¢æç«¯å€¼
                            median_val = np.median(feature_values[~outlier_mask]) if (~outlier_mask).any() else np.median(feature_values)
                            X_clean[outlier_mask, feature_idx] = median_val
            
            return X_clean
        
        # æ¸…ç†æ‰€æœ‰æ•°æ®é›†
        X_train_clean = clean_array(X_train, "è®­ç»ƒé›†")
        X_val_clean = clean_array(X_val, "éªŒè¯é›†")
        X_test_clean = clean_array(X_test, "æµ‹è¯•é›†")
        
        # æœ€ç»ˆæ£€æŸ¥
        for name, data in [("è®­ç»ƒé›†", X_train_clean), ("éªŒè¯é›†", X_val_clean), ("æµ‹è¯•é›†", X_test_clean)]:
            if np.isinf(data).any() or np.isnan(data).any():
                self.logger.error(f"{name}ä»åŒ…å«æ— ç©·å€¼æˆ–NaNå€¼ï¼")
            else:
                self.logger.info(f"{name}æ•°æ®æ¸…ç†å®Œæˆ")
        
        return X_train_clean, X_val_clean, X_test_clean
    
    def normalize_data(self, X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """æ•°æ®æ ‡å‡†åŒ–"""
        norm_config = self.config['data']['preprocessing']['normalization']
        
        if norm_config['method'] is None:
            return X_train, X_val, X_test
        
        # æ•°æ®æ¸…ç†ï¼šåœ¨æ ‡å‡†åŒ–å‰å¤„ç†æ— ç©·å€¼å’ŒNaNå€¼
        self.logger.info("æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— ç©·å€¼å’ŒNaNå€¼...")
        X_train, X_val, X_test = self._clean_data_for_scaling(X_train, X_val, X_test)
            
        self.logger.info(f"ä½¿ç”¨ {norm_config['method']} æ–¹æ³•è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...")
        
        if norm_config['method'] == 'standard':
            self.scaler = StandardScaler()
        elif norm_config['method'] == 'minmax':
            self.scaler = MinMaxScaler()
        elif norm_config['method'] == 'robust':
            self.scaler = RobustScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {norm_config['method']}")
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_val_scaled, X_test_scaled
    
    def handle_outliers(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """å¢å¼ºçš„å¼‚å¸¸å€¼å¤„ç†"""
        outlier_config = self.config['data']['preprocessing']['outlier_handling']
        
        if not outlier_config['enabled']:
            return X, y
            
        self.logger.info("å¤„ç†å¼‚å¸¸å€¼...")
        
        method = outlier_config['method']
        
        if method == 'multi_stage':
            # å¤šé˜¶æ®µå¼‚å¸¸å€¼å¤„ç†ï¼ˆæ–°å¢ï¼‰
            return self._handle_outliers_multi_stage(X, y, outlier_config)
        elif method == 'iqr':
            # ä½¿ç”¨IQRæ–¹æ³•
            threshold = outlier_config['threshold']
            Q1 = np.percentile(y, 25)
            Q3 = np.percentile(y, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            mask = (y >= lower_bound) & (y <= upper_bound)
            
        elif method == 'zscore':
            # ä½¿ç”¨Z-scoreæ–¹æ³•
            threshold = outlier_config['threshold']
            z_scores = np.abs(stats.zscore(y))
            mask = z_scores < threshold
            
        else:
            self.logger.warning(f"ä¸æ”¯æŒçš„å¼‚å¸¸å€¼å¤„ç†æ–¹æ³•: {method}")
            return X, y
        
        X_clean = X[mask]
        y_clean = y[mask]
        
        removed_count = len(y) - len(y_clean)
        self.logger.info(f"å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼Œç§»é™¤äº† {removed_count} ä¸ªæ ·æœ¬")
        
        return X_clean, y_clean
    
    def _handle_outliers_multi_stage(self, X: np.ndarray, y: np.ndarray, config: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """å¤šé˜¶æ®µå¼‚å¸¸å€¼å¤„ç†"""
        extreme_threshold = config.get('extreme_threshold', 5.0)
        mild_threshold = config.get('mild_threshold', 2.5)
        strategy = config.get('strategy', 'winsorize')
        winsorize_limits = config.get('winsorize_limits', [0.01, 0.01])
        
        original_count = len(y)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†æç«¯å¼‚å¸¸å€¼
        z_scores = np.abs(stats.zscore(y))
        extreme_mask = z_scores < extreme_threshold
        
        if strategy == 'remove':
            X_clean = X[extreme_mask]
            y_clean = y[extreme_mask]
            extreme_removed = original_count - len(y_clean)
            self.logger.info(f"ç¬¬ä¸€é˜¶æ®µï¼šç§»é™¤äº† {extreme_removed} ä¸ªæç«¯å¼‚å¸¸å€¼")
            
        elif strategy == 'winsorize':
            # ä½¿ç”¨Winsorizeæ–¹æ³•æˆªæ–­æç«¯å€¼
            from scipy.stats import mstats
            y_clean = mstats.winsorize(y, limits=winsorize_limits)
            X_clean = X.copy()
            extreme_modified = np.sum(y != y_clean)
            self.logger.info(f"ç¬¬ä¸€é˜¶æ®µï¼šè°ƒæ•´äº† {extreme_modified} ä¸ªæç«¯å¼‚å¸¸å€¼")
            
        elif strategy == 'clip':
            # ä½¿ç”¨clipæ–¹æ³•æˆªæ–­
            lower_percentile = winsorize_limits[0] * 100
            upper_percentile = (1 - winsorize_limits[1]) * 100
            lower_bound = np.percentile(y, lower_percentile)
            upper_bound = np.percentile(y, upper_percentile)
            y_clean = np.clip(y, lower_bound, upper_bound)
            X_clean = X.copy()
            extreme_modified = np.sum(y != y_clean)
            self.logger.info(f"ç¬¬ä¸€é˜¶æ®µï¼šæˆªæ–­äº† {extreme_modified} ä¸ªæç«¯å¼‚å¸¸å€¼")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æ¸©å’Œå¼‚å¸¸å€¼ï¼ˆä»…åœ¨removeç­–ç•¥ä¸‹æ‰§è¡Œï¼‰
        if strategy == 'remove':
            z_scores_clean = np.abs(stats.zscore(y_clean))
            mild_mask = z_scores_clean < mild_threshold
            X_final = X_clean[mild_mask]
            y_final = y_clean[mild_mask]
            mild_removed = len(y_clean) - len(y_final)
            self.logger.info(f"ç¬¬äºŒé˜¶æ®µï¼šç§»é™¤äº† {mild_removed} ä¸ªæ¸©å’Œå¼‚å¸¸å€¼")
        else:
            X_final = X_clean
            y_final = y_clean
        
        total_processed = original_count - len(y_final) if strategy == 'remove' else np.sum(y != y_final)
        self.logger.info(f"å¤šé˜¶æ®µå¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç†äº† {total_processed} ä¸ªæ ·æœ¬")
        
        return X_final, y_final
    
    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, ...]:
        """æ•°æ®åˆ†å‰²"""
        self.logger.info("åˆ†å‰²æ•°æ®...")
        
        split_config = self.config['training']['data_split']
        
        # å…ˆåˆ†ç¦»æµ‹è¯•é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=split_config['test_size'],
            random_state=split_config['random_state'],
            stratify=None if not split_config['stratify'] else y
        )
        
        # å†åˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_size = split_config['validation_size'] / (1 - split_config['test_size'])
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size,
            random_state=split_config['random_state'],
            stratify=None if not split_config['stratify'] else y_temp
        )
        
        self.logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ: è®­ç»ƒé›†={len(X_train)}, éªŒè¯é›†={len(X_val)}, æµ‹è¯•é›†={len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def feature_selection(self, X_train: np.ndarray, y_train: np.ndarray, 
                         X_val: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, ...]:
        """å¢å¼ºçš„ç‰¹å¾é€‰æ‹©"""
        fs_config = self.config['feature_selection']
        
        if not fs_config['enabled']:
            return X_train, X_val, X_test
            
        self.logger.info("è¿›è¡Œç‰¹å¾é€‰æ‹©...")
        original_features = X_train.shape[1]
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¸è¿›å¼ç‰¹å¾é€‰æ‹©
        progressive_config = self.config.get('progressive_feature_selection', {})
        if progressive_config.get('enabled', False):
            return self._progressive_feature_selection(
                X_train, X_val, X_test, y_train, progressive_config
            )
        
        # ä¼ ç»Ÿç‰¹å¾é€‰æ‹©
        # åŸºäºé‡è¦æ€§çš„é€‰æ‹©
        if fs_config['methods']['importance_based']['enabled']:
            X_train, X_val, X_test = self._importance_based_selection(
                X_train, X_val, X_test, y_train, fs_config['methods']['importance_based']
            )
            
        # åŸºäºç›¸å…³æ€§çš„é€‰æ‹©
        if fs_config['methods']['correlation_based']['enabled']:
            X_train, X_val, X_test = self._correlation_based_selection(
                X_train, X_val, X_test, fs_config['methods']['correlation_based']
            )
            
        # é€’å½’ç‰¹å¾æ¶ˆé™¤
        if fs_config['methods']['rfe']['enabled']:
            X_train, X_val, X_test = self._rfe_selection(
                X_train, X_val, X_test, y_train, fs_config['methods']['rfe']
            )
        
        final_features = X_train.shape[1]
        self.logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼š{original_features} -> {final_features} ä¸ªç‰¹å¾")
        
        return X_train, X_val, X_test
    
    def _progressive_feature_selection(self, X_train, X_val, X_test, y_train, config):
        """æ¸è¿›å¼ç‰¹å¾é€‰æ‹©"""
        self.logger.info("å¼€å§‹æ¸è¿›å¼ç‰¹å¾é€‰æ‹©...")
        original_features = X_train.shape[1]
        
        stages = config.get('stages', [])
        current_features = X_train.shape[1]
        
        for stage in stages:
            stage_name = stage['name']
            method = stage['method']
            threshold = stage['threshold']
            
            self.logger.info(f"æ‰§è¡Œé˜¶æ®µ: {stage_name}")
            
            if method == 'variance_threshold':
                # ç§»é™¤ä½æ–¹å·®ç‰¹å¾
                X_train, X_val, X_test = self._variance_threshold_selection(
                    X_train, X_val, X_test, threshold
                )
                
            elif method == 'correlation':
                # ç§»é™¤é«˜ç›¸å…³ç‰¹å¾
                X_train, X_val, X_test = self._correlation_based_selection(
                    X_train, X_val, X_test, {'threshold': threshold}
                )
                
            elif method == 'importance':
                # åŸºäºé‡è¦æ€§é€‰æ‹©
                X_train, X_val, X_test = self._importance_based_selection(
                    X_train, X_val, X_test, y_train, 
                    {'threshold': threshold, 'method': 'gain'}
                )
            
            new_features = X_train.shape[1]
            removed = current_features - new_features
            self.logger.info(f"{stage_name}å®Œæˆï¼šç§»é™¤äº†{removed}ä¸ªç‰¹å¾ï¼Œå‰©ä½™{new_features}ä¸ª")
            current_features = new_features
        
        final_features = X_train.shape[1]
        self.logger.info(f"æ¸è¿›å¼ç‰¹å¾é€‰æ‹©å®Œæˆï¼š{original_features} -> {final_features} ä¸ªç‰¹å¾")
        
        return X_train, X_val, X_test
    
    def _variance_threshold_selection(self, X_train, X_val, X_test, threshold):
        """åŸºäºæ–¹å·®é˜ˆå€¼çš„ç‰¹å¾é€‰æ‹©"""
        from sklearn.feature_selection import VarianceThreshold
        
        selector = VarianceThreshold(threshold=threshold)
        X_train_selected = selector.fit_transform(X_train)
        X_val_selected = selector.transform(X_val)
        X_test_selected = selector.transform(X_test)
        
        # æ›´æ–°ç‰¹å¾åç§°
        selected_features = selector.get_support()
        self.feature_names = [name for name, selected in zip(self.feature_names, selected_features) if selected]
        
        return X_train_selected, X_val_selected, X_test_selected
    
    def _importance_based_selection(self, X_train, X_val, X_test, y_train, config):
        """åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©"""
        threshold = config['threshold']
        method = config.get('method', 'split')
        
        # è®­ç»ƒä¸´æ—¶æ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
        temp_params = {
            'random_state': 42,
            'verbose': -1,
            'n_estimators': 100,  # å‡å°‘ä¼°è®¡å™¨æ•°é‡ï¼ŒåŠ å¿«é€Ÿåº¦
            'importance_type': method
        }
        temp_model = lgb.LGBMRegressor(**temp_params)
        
        with SuppressOutput():  # é™é»˜è®­ç»ƒ
            temp_model.fit(X_train, y_train)
        
        selector = SelectFromModel(temp_model, threshold=threshold)
        X_train_selected = selector.fit_transform(X_train, y_train)
        X_val_selected = selector.transform(X_val)
        X_test_selected = selector.transform(X_test)
        
        selected_features = selector.get_support()
        self.feature_names = [name for name, selected in zip(self.feature_names, selected_features) if selected]
        
        selected_count = X_train_selected.shape[1]
        self.logger.info(f"åŸºäºé‡è¦æ€§é€‰æ‹©äº† {selected_count} ä¸ªç‰¹å¾")
        
        return X_train_selected, X_val_selected, X_test_selected
    
    def _correlation_based_selection(self, X_train, X_val, X_test, config):
        """åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©"""
        threshold = config['threshold']
        
        # è®¡ç®—ç‰¹å¾é—´ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = np.corrcoef(X_train.T)
        corr_matrix = np.abs(corr_matrix)
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
        upper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
        high_corr_pairs = np.where((corr_matrix > threshold) & upper_triangle)
        
        # é€‰æ‹©è¦ç§»é™¤çš„ç‰¹å¾ï¼ˆä¿ç•™æ–¹å·®è¾ƒå¤§çš„ï¼‰
        features_to_remove = set()
        for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):
            var_i = np.var(X_train[:, i])
            var_j = np.var(X_train[:, j])
            # ç§»é™¤æ–¹å·®è¾ƒå°çš„ç‰¹å¾
            if var_i < var_j:
                features_to_remove.add(i)
            else:
                features_to_remove.add(j)
        
        # åˆ›å»ºä¿ç•™ç‰¹å¾çš„æ©ç 
        features_to_keep = [i for i in range(X_train.shape[1]) if i not in features_to_remove]
        
        X_train_selected = X_train[:, features_to_keep]
        X_val_selected = X_val[:, features_to_keep]
        X_test_selected = X_test[:, features_to_keep]
        
        # æ›´æ–°ç‰¹å¾åç§°
        self.feature_names = [self.feature_names[i] for i in features_to_keep]
        
        removed_count = len(features_to_remove)
        self.logger.info(f"åŸºäºç›¸å…³æ€§ç§»é™¤äº† {removed_count} ä¸ªé«˜ç›¸å…³ç‰¹å¾")
        
        return X_train_selected, X_val_selected, X_test_selected
    
    def _rfe_selection(self, X_train, X_val, X_test, y_train, config):
        """é€’å½’ç‰¹å¾æ¶ˆé™¤"""
        n_features_to_select = config['n_features_to_select']
        step = config.get('step', 0.1)
        
        # åˆ›å»ºåŸºç¡€ä¼°è®¡å™¨
        estimator = lgb.LGBMRegressor(
            random_state=42,
            verbose=-1,
            n_estimators=50  # å‡å°‘ä¼°è®¡å™¨æ•°é‡ï¼ŒåŠ å¿«RFEé€Ÿåº¦
        )
        
        # åˆ›å»ºRFEé€‰æ‹©å™¨
        selector = RFE(
            estimator=estimator,
            n_features_to_select=n_features_to_select,
            step=step,
            verbose=0
        )
        
        self.logger.info(f"å¼€å§‹é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼Œç›®æ ‡ç‰¹å¾æ•°: {n_features_to_select}")
        
        # æ‰§è¡Œç‰¹å¾é€‰æ‹©ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
        with SuppressOutput():
            X_train_selected = selector.fit_transform(X_train, y_train)
        
        X_val_selected = selector.transform(X_val)
        X_test_selected = selector.transform(X_test)
        
        # æ›´æ–°ç‰¹å¾åç§°
        selected_features = selector.get_support()
        self.feature_names = [name for name, selected in zip(self.feature_names, selected_features) if selected]
        
        self.logger.info(f"RFEé€‰æ‹©äº† {X_train_selected.shape[1]} ä¸ªç‰¹å¾")
        
        return X_train_selected, X_val_selected, X_test_selected
    
    def train_model(self, X_train: np.ndarray, y_train: np.ndarray, 
                   X_val: np.ndarray, y_val: np.ndarray) -> lgb.LGBMRegressor:
        """è®­ç»ƒæ¨¡å‹"""
        # è·å–æ¨¡å‹å‚æ•°
        lgb_config = self.config['lightgbm']
        model_params = {**lgb_config['basic_params'], **lgb_config['advanced_params']}
        fit_params = lgb_config['fit_params']
        train_params = self.config['training']['training_params']
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä¸åŒ…å«early_stopping_roundsï¼Œè¿™ä¸ªå‚æ•°åœ¨fitä¸­ä½¿ç”¨ï¼‰
        self.model = lgb.LGBMRegressor(**model_params)
        
        # è®¾ç½®å›è°ƒå‡½æ•°
        callbacks = [
            lgb.early_stopping(train_params['early_stopping_rounds'])
        ]
        
        # æ ¹æ®æ˜¯å¦æœ‰tqdmè®¾ç½®ä¸åŒçš„å›è°ƒ
        if TQDM_AVAILABLE:
            # æœ‰tqdmæ—¶ï¼Œç›´æ¥é™é»˜è®­ç»ƒï¼Œç”¨ç®€å•çš„è¿›åº¦æŒ‡ç¤º
            self.logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            callbacks.append(lgb.log_evaluation(0))  # å®Œå…¨å…³é—­æ—¥å¿—
        else:
            # æ²¡æœ‰tqdmæ—¶ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ—¥å¿—è¾“å‡º
            callbacks.append(lgb.log_evaluation(train_params.get('verbose', 100)))
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆé™é»˜è®­ç»ƒï¼‰
        print("ğŸš€ æ­£åœ¨è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        if TQDM_AVAILABLE:
            # é™é»˜è®­ç»ƒï¼Œé¿å…å†—é•¿æ—¥å¿—
            with SuppressOutput():
                self.model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=callbacks
                )
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        else:
            # æ²¡æœ‰tqdmæ—¶ï¼Œæ­£å¸¸æ˜¾ç¤ºLightGBMè¾“å‡º
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=callbacks
            )
        
        self.logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.model
    
    def hyperparameter_tuning(self, X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """è¶…å‚æ•°ä¼˜åŒ–"""
        tuning_config = self.config['hyperparameter_tuning']
        
        if not tuning_config['enabled'] or not OPTUNA_AVAILABLE:
            return self.config['lightgbm']['basic_params']
            
        n_trials = tuning_config['optuna_config']['n_trials']
        
        # è®¾ç½®Optunaæ—¥å¿—çº§åˆ«ï¼ŒæŠ‘åˆ¶è¯¦ç»†è¾“å‡º
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        # åˆ›å»ºç ”ç©¶å¯¹è±¡ï¼Œä½¿ç”¨TPEé‡‡æ ·å™¨
        sampler = TPESampler(seed=42) if OPTUNA_AVAILABLE else None
        study = optuna.create_study(direction='minimize', sampler=sampler)
        
        # åˆ›å»ºæ¸…æ™°çš„è¿›åº¦æ¡
        print(f"\nğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ– (å…±{n_trials}æ¬¡å°è¯•)...")
        if TQDM_AVAILABLE:
            pbar = tqdm(
                total=n_trials, 
                desc="ğŸ” ä¼˜åŒ–è¿›åº¦", 
                unit="æ¬¡",
                bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
                leave=True
            )
            
        # ç”¨äºå­˜å‚¨æœ€ä½³å€¼çš„å˜é‡
        best_score = float('inf')
        
        def objective(trial):
            nonlocal best_score
            
            # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
            param_ranges = tuning_config['optuna_config']['param_ranges']
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'boosting_type': 'gbdt',
                'verbose': -1,
                'random_state': 42,
                'num_leaves': trial.suggest_int('num_leaves', *param_ranges['num_leaves']),
                'learning_rate': trial.suggest_float('learning_rate', *param_ranges['learning_rate']),
                'feature_fraction': trial.suggest_float('feature_fraction', *param_ranges['feature_fraction']),
                'bagging_fraction': trial.suggest_float('bagging_fraction', *param_ranges['bagging_fraction']),
                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', *param_ranges['min_data_in_leaf']),
                'lambda_l1': trial.suggest_float('lambda_l1', *param_ranges['lambda_l1']),
                'lambda_l2': trial.suggest_float('lambda_l2', *param_ranges['lambda_l2']),
            }
            
            # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
            cv_config = self.config['training']['cross_validation']
            if cv_config['enabled']:
                scores = []
                
                if cv_config['cv_strategy'] == 'time_series':
                    tscv = TimeSeriesSplit(n_splits=cv_config['cv_folds'])
                    cv_splits = list(tscv.split(X_train))
                else:
                    from sklearn.model_selection import KFold
                    kf = KFold(n_splits=cv_config['cv_folds'], shuffle=True, random_state=42)
                    cv_splits = list(kf.split(X_train))
                
                # å®Œå…¨é™é»˜æ‰§è¡Œäº¤å‰éªŒè¯
                for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):
                    X_tr, X_val = X_train[train_idx], X_train[val_idx]
                    y_tr, y_val = y_train[train_idx], y_train[val_idx]
                    
                    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®Œå…¨æŠ‘åˆ¶è¾“å‡º
                    with SuppressOutput():
                        model = lgb.LGBMRegressor(**params)
                        model.fit(
                            X_tr, y_tr, 
                            eval_set=[(X_val, y_val)],
                            callbacks=[
                                lgb.early_stopping(50, verbose=False),
                                lgb.log_evaluation(0)  # å®Œå…¨å…³é—­æ—¥å¿—è¾“å‡º
                            ]
                        )
                        
                        y_pred = model.predict(X_val)
                    
                    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
                    scores.append(rmse)
                
                current_score = np.mean(scores)
                
                # æ›´æ–°æœ€ä½³åˆ†æ•°
                if current_score < best_score:
                    best_score = current_score
                
                # æ›´æ–°ä¸»è¿›åº¦æ¡
                if TQDM_AVAILABLE:
                    pbar.update(1)
                    pbar.set_postfix({
                        'æœ€ä½³': f"{best_score:.4f}",
                        'å½“å‰': f"{current_score:.4f}"
                    })
                
                return current_score
        
        try:
            # æ‰§è¡Œä¼˜åŒ–
            study.optimize(
                objective, 
                n_trials=n_trials,
                timeout=tuning_config['optuna_config']['timeout'],
                show_progress_bar=False  # æˆ‘ä»¬ä½¿ç”¨è‡ªå·±çš„è¿›åº¦æ¡
            )
        finally:
            if TQDM_AVAILABLE:
                pbar.close()
        
        best_params = {**self.config['lightgbm']['basic_params'], **study.best_params}
        
        # ç¾åŒ–æœ€ä½³å‚æ•°è¾“å‡º
        print(f"\n{'='*60}")
        print(f"ğŸ¯ è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"{'='*60}")
        print(f"æœ€ä½³RMSE: {study.best_value:.4f}")
        print(f"æœ€ä½³å‚æ•°:")
        print(f"{'-'*40}")
        for param, value in study.best_params.items():
            if isinstance(value, float):
                print(f"  {param:<20}: {value:.6f}")
            else:
                print(f"  {param:<20}: {value}")
        print(f"{'='*60}\n")
        
        return best_params
    
    def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.logger.info("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        y_pred = self.model.predict(X_test)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {}
        eval_config = self.config['evaluation']['metrics']
        
        if 'rmse' in eval_config:
            metrics['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))
        if 'mae' in eval_config:
            metrics['mae'] = mean_absolute_error(y_test, y_pred)
        if 'r2_score' in eval_config:
            metrics['r2_score'] = r2_score(y_test, y_pred)
        if 'explained_variance' in eval_config:
            metrics['explained_variance'] = explained_variance_score(y_test, y_pred)
        if 'mape' in eval_config:
            # é¿å…é™¤é›¶é”™è¯¯ï¼šè¿‡æ»¤æ‰y_testä¸­çš„é›¶å€¼
            non_zero_mask = np.abs(y_test) > 1e-8  # ä½¿ç”¨å°é˜ˆå€¼è€Œä¸æ˜¯ä¸¥æ ¼çš„é›¶
            if non_zero_mask.any():
                y_test_filtered = y_test[non_zero_mask]
                y_pred_filtered = y_pred[non_zero_mask]
                metrics['mape'] = np.mean(np.abs((y_test_filtered - y_pred_filtered) / y_test_filtered)) * 100
                
                # å¦‚æœæœ‰æ•°æ®è¢«è¿‡æ»¤ï¼Œè®°å½•ä¿¡æ¯
                if not non_zero_mask.all():
                    filtered_count = len(y_test) - non_zero_mask.sum()
                    self.logger.info(f"MAPEè®¡ç®—æ—¶è¿‡æ»¤äº†{filtered_count}ä¸ªæ¥è¿‘é›¶å€¼çš„æ ·æœ¬")
            else:
                # å¦‚æœæ‰€æœ‰çœŸå®å€¼éƒ½æ¥è¿‘é›¶ï¼ŒMAPEæ— æ„ä¹‰
                metrics['mape'] = np.inf
                self.logger.warning("æ‰€æœ‰ç›®æ ‡å€¼éƒ½æ¥è¿‘é›¶ï¼ŒMAPEè®¾ä¸ºæ— ç©·å¤§")
        
        # ç¾åŒ–è¯„ä¼°ç»“æœè¾“å‡º
        print(f"\n{'='*50}")
        print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
        print(f"{'='*50}")
        for metric, value in metrics.items():
            if np.isinf(value):
                print(f"  {metric.upper():<20}: âˆ (æ— ç©·å¤§)")
            elif np.isnan(value):
                print(f"  {metric.upper():<20}: NaN")
            else:
                print(f"  {metric.upper():<20}: {value:.4f}")
        print(f"{'='*50}\n")
        
        return metrics, y_pred
    

    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        self.logger.info("ä¿å­˜æ¨¡å‹...")
        
        save_config = self.config['output']['model_save']
        model_name = save_config['model_name']
        
        for format_type in save_config['save_format']:
            if format_type == 'pkl':
                model_path = os.path.join(self.model_save_dir, f"{model_name}.pkl")
                joblib.dump(self.model, model_path)
                
                # ä¿å­˜é¢„å¤„ç†å™¨
                if self.scaler is not None:
                    scaler_path = os.path.join(self.model_save_dir, "scaler.pkl")
                    joblib.dump(self.scaler, scaler_path)
                
                # ä¿å­˜ç‰¹å¾åç§°ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰
                feature_info = {
                    'feature_names': self.feature_names,
                    'original_feature_names': getattr(self, 'original_feature_names', []),
                    'feature_count': len(self.feature_names),
                    'timeseries_method': self.config['data']['preprocessing']['timeseries_method'],
                    'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'feature_mapping': {}
                }
                
                # åˆ›å»ºç‰¹å¾æ˜ å°„ï¼ˆåŸå§‹ç‰¹å¾ -> å¤„ç†åç‰¹å¾ï¼‰
                if hasattr(self, 'original_feature_names') and self.original_feature_names:
                    for i, original_name in enumerate(self.original_feature_names):
                        related_features = [fname for fname in self.feature_names 
                                          if original_name in fname or fname.endswith(f'_{i}')]
                        feature_info['feature_mapping'][original_name] = related_features
                
                feature_names_path = os.path.join(self.model_save_dir, "feature_names.json")
                with open(feature_names_path, 'w', encoding='utf-8') as f:
                    json.dump(feature_info, f, ensure_ascii=False, indent=2)
                
                # ä¹Ÿä¿å­˜ç®€å•çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰
                simple_names_path = os.path.join(self.model_save_dir, "feature_names_simple.txt")
                with open(simple_names_path, 'w', encoding='utf-8') as f:
                    f.write("ç‰¹å¾åç§°åˆ—è¡¨:\n")
                    f.write("=" * 50 + "\n")
                    for i, name in enumerate(self.feature_names):
                        f.write(f"{i+1:4d}. {name}\n")
                    
            elif format_type == 'txt':
                model_path = os.path.join(self.model_save_dir, f"{model_name}.txt")
                self.model.booster_.save_model(model_path)
        
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_save_dir}")
    
    def save_results(self, metrics: Dict, y_pred: np.ndarray, y_test: np.ndarray):
        """ä¿å­˜ç»“æœ"""
        save_config = self.config['output']['results_save']
        
        # ä¿å­˜æŒ‡æ ‡
        if save_config['save_metrics']:
            metrics_path = os.path.join(self.results_save_dir, "metrics.json")
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if save_config['save_predictions']:
            predictions_path = os.path.join(self.results_save_dir, "predictions.csv")
            results_df = pd.DataFrame({
                'y_true': y_test,
                'y_pred': y_pred,
                'residual': y_test - y_pred
            })
            results_df.to_csv(predictions_path, index=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if save_config['save_feature_importance'] and hasattr(self.model, 'feature_importances_'):
            # åˆ›å»ºè¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
            feature_names_used = self.feature_names[:len(self.model.feature_importances_)]
            importance_df = pd.DataFrame({
                'feature_name': feature_names_used,
                'importance': self.model.feature_importances_,
                'feature_index': range(len(self.model.feature_importances_))
            }).sort_values('importance', ascending=False)
            
            # æ·»åŠ ç‰¹å¾æ’å
            importance_df['importance_rank'] = range(1, len(importance_df) + 1)
            
            # æ·»åŠ ç›¸å¯¹é‡è¦æ€§ï¼ˆç™¾åˆ†æ¯”ï¼‰
            total_importance = importance_df['importance'].sum()
            importance_df['importance_percent'] = (importance_df['importance'] / total_importance * 100).round(4)
            
            # æ·»åŠ ç´¯ç§¯é‡è¦æ€§ç™¾åˆ†æ¯”
            importance_df['cumulative_percent'] = importance_df['importance_percent'].cumsum().round(4)
            
            importance_path = os.path.join(self.results_save_dir, "feature_importance.csv")
            importance_df.to_csv(importance_path, index=False)
            
            # ä¿å­˜å‰20ä¸ªæœ€é‡è¦ç‰¹å¾çš„å¯è§†åŒ–æ–‡æœ¬
            top_features_path = os.path.join(self.results_save_dir, "top_features.txt")
            with open(top_features_path, 'w', encoding='utf-8') as f:
                f.write("å‰20ä¸ªæœ€é‡è¦ç‰¹å¾:\n")
                f.write("=" * 80 + "\n")
                f.write(f"{'æ’å':<4} {'ç‰¹å¾åç§°':<40} {'é‡è¦æ€§':<12} {'å æ¯”(%)':<10} {'ç´¯ç§¯(%)':<10}\n")
                f.write("-" * 80 + "\n")
                for idx, row in importance_df.head(20).iterrows():
                    f.write(f"{row['importance_rank']:<4} {row['feature_name']:<40} "
                           f"{row['importance']:<12.6f} {row['importance_percent']:<10.2f} "
                           f"{row['cumulative_percent']:<10.2f}\n")
            
            self.logger.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ï¼Œå‰5ä¸ªé‡è¦ç‰¹å¾: {list(importance_df.head(5)['feature_name'])}")
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.results_save_dir}")
    
    def run_training_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("å¼€å§‹LightGBMè®­ç»ƒæµç¨‹...")
            
            # å®šä¹‰è®­ç»ƒæ­¥éª¤
            steps = [
                ("åŠ è½½æ•°æ®", self._step_load_data),
                ("å¤„ç†å¼‚å¸¸å€¼", self._step_handle_outliers),
                ("é¢„å¤„ç†æ•°æ®", self._step_preprocess_data),
                ("æ·»åŠ æŠ€æœ¯ç‰¹å¾", self._step_add_features),
                ("åˆ†å‰²æ•°æ®", self._step_split_data),
                ("æ ‡å‡†åŒ–æ•°æ®", self._step_normalize_data),
                ("ç‰¹å¾é€‰æ‹©", self._step_feature_selection),
                ("è¶…å‚æ•°ä¼˜åŒ–", self._step_hyperparameter_tuning),
                ("è®­ç»ƒæ¨¡å‹", self._step_train_model),
                ("è¯„ä¼°æ¨¡å‹", self._step_evaluate_model),
                ("ä¿å­˜ç»“æœ", self._step_save_results)
            ]
            
            # ä½¿ç”¨ç®€åŒ–çš„è¿›åº¦æ˜¾ç¤º
            print(f"\n{'='*60}")
            print(f"ğŸš€ å¼€å§‹LightGBMè®­ç»ƒæµç¨‹")
            print(f"{'='*60}")
            
            # å­˜å‚¨ä¸­é—´ç»“æœçš„å˜é‡
            results = {}
            
            for i, (step_name, step_func) in enumerate(steps):
                # æ¸…æ™°çš„æ­¥éª¤æŒ‡ç¤º
                print(f"\nğŸ“ [{i+1}/{len(steps)}] {step_name}...")
                
                # æ‰§è¡Œæ­¥éª¤
                step_result = step_func(results)
                if step_result is not None:
                    results.update(step_result)
                
                print(f"âœ… {step_name} å®Œæˆ")
            
            # å®Œæˆæç¤º
            print(f"\n{'='*60}")
            print(f"ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
            print(f"{'='*60}")
            print(f"ğŸ“‚ è®­ç»ƒç»“æœå·²ä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®:")
            print(f"   ğŸ”§ æ¨¡å‹æ–‡ä»¶: {self.model_save_dir}")
            print(f"   ğŸ“Š ç»“æœæ–‡ä»¶: {self.results_save_dir}")
            print(f"   ğŸ“ è®­ç»ƒæ–‡ä»¶å¤¹: {self.training_folder}")
            print(f"{'='*60}\n")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    # å°†åŸæ¥çš„æ­¥éª¤æ‹†åˆ†ä¸ºç‹¬ç«‹çš„æ–¹æ³•ï¼ˆä¼˜åŒ–æ—¥å¿—è¾“å‡ºï¼‰
    def _step_load_data(self, results):
        """é™é»˜åŠ è½½æ•°æ®"""
        # ä¸´æ—¶è°ƒæ•´æ—¥å¿—çº§åˆ«
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X_raw, y = self.load_data()
        
        # æ¢å¤æ—¥å¿—çº§åˆ«
        self.logger.setLevel(original_level)
        return {"X_raw": X_raw, "y": y}
    
    def _step_handle_outliers(self, results):
        """é™é»˜å¤„ç†å¼‚å¸¸å€¼"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X_raw, y = self.handle_outliers(results["X_raw"], results["y"])
        
        self.logger.setLevel(original_level)
        return {"X_raw": X_raw, "y": y}
    
    def _step_preprocess_data(self, results):
        """é™é»˜é¢„å¤„ç†æ•°æ®"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X = self.preprocess_timeseries_data(results["X_raw"])
        
        self.logger.setLevel(original_level)
        return {"X": X}
    
    def _step_add_features(self, results):
        """é™é»˜æ·»åŠ æŠ€æœ¯ç‰¹å¾"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X = self.add_technical_features(results["X"], results["X_raw"])
        
        self.logger.setLevel(original_level)
        return {"X": X}
    
    def _step_split_data(self, results):
        """é™é»˜åˆ†å‰²æ•°æ®"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(results["X"], results["y"])
        
        self.logger.setLevel(original_level)
        return {
            "X_train": X_train, "X_val": X_val, "X_test": X_test,
            "y_train": y_train, "y_val": y_val, "y_test": y_test
        }
    
    def _step_normalize_data(self, results):
        """é™é»˜æ ‡å‡†åŒ–æ•°æ®"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X_train, X_val, X_test = self.normalize_data(
            results["X_train"], results["X_val"], results["X_test"]
        )
        
        self.logger.setLevel(original_level)
        return {"X_train": X_train, "X_val": X_val, "X_test": X_test}
    
    def _step_feature_selection(self, results):
        """é™é»˜ç‰¹å¾é€‰æ‹©"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        X_train, X_val, X_test = self.feature_selection(
            results["X_train"], results["y_train"], results["X_val"], results["X_test"]
        )
        
        self.logger.setLevel(original_level)
        return {"X_train": X_train, "X_val": X_val, "X_test": X_test}
    
    def _step_hyperparameter_tuning(self, results):
        """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        if self.config['hyperparameter_tuning']['enabled']:
            best_params = self.hyperparameter_tuning(results["X_train"], results["y_train"])
            self.config['lightgbm']['basic_params'].update(best_params)
        else:
            print("âš ï¸  è¶…å‚æ•°ä¼˜åŒ–å·²ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return None
    
    def _step_train_model(self, results):
        """è®­ç»ƒæ¨¡å‹"""
        self.model = self.train_model(
            results["X_train"], results["y_train"], results["X_val"], results["y_val"]
        )
        return None
    
    def _step_evaluate_model(self, results):
        """é™é»˜è¯„ä¼°æ¨¡å‹"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        metrics, y_pred = self.evaluate_model(results["X_test"], results["y_test"])
        
        self.logger.setLevel(original_level)
        return {"metrics": metrics, "y_pred": y_pred}
    
    def _step_save_results(self, results):
        """é™é»˜ä¿å­˜ç»“æœ"""
        original_level = self.logger.level
        self.logger.setLevel(logging.WARNING)
        
        self.save_model()
        self.save_results(results["metrics"], results["y_pred"], results["y_test"])
        
        self.logger.setLevel(original_level)
        return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LightGBM è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config/train/lightGBM_train.yaml',
                      help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    trainer = LightGBMTrainer(args.config)
    trainer.run_training_pipeline()


if __name__ == '__main__':
    main()