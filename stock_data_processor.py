#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®é¢„å¤„ç†è„šæœ¬
ä¸“é—¨ç”¨äºå¤„ç†professional_parquetæ–‡ä»¶å¤¹ä¸­çš„è‚¡ç¥¨æ•°æ®
è½¬æ¢ä¸ºé€‚åˆlightGBMè®­ç»ƒçš„æ ¼å¼
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import warnings
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import gc

warnings.filterwarnings('ignore')


class StockDataProcessor:
    """è‚¡ç¥¨æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/professional_parquet", 
                 output_dir: str = "data/processed_stock_data"):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            data_dir: è¾“å…¥æ•°æ®ç›®å½• (professional_parquet)
            output_dir: è¾“å‡ºæ•°æ®ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.stock_data = {}
        self.feature_data = None
        self.target_data = None
        self.stock_codes = []
        self.feature_names = []
        
        print(f"ğŸ“ æ•°æ®è¾“å…¥è·¯å¾„: {self.data_dir}")
        print(f"ğŸ“ æ•°æ®è¾“å‡ºè·¯å¾„: {self.output_dir}")
    
    def load_parquet_data(self) -> bool:
        """åŠ è½½parquetæ ¼å¼çš„è‚¡ç¥¨æ•°æ®"""
        try:
            print("ğŸ“Š åŠ è½½parquetè‚¡ç¥¨æ•°æ®...")
            
            # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.data_dir.exists():
                print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
                return False
            
            # è·å–æ‰€æœ‰parquetæ–‡ä»¶
            parquet_files = list(self.data_dir.glob("*.parquet"))
            if not parquet_files:
                print(f"âŒ åœ¨{self.data_dir}ä¸­æœªæ‰¾åˆ°parquetæ–‡ä»¶")
                return False
            
            print(f"   å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
            
            # åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰parquetæ–‡ä»¶
            all_data = []
            for file_path in tqdm(parquet_files, desc="åŠ è½½parquetæ–‡ä»¶"):
                try:
                    df = pd.read_parquet(file_path)
                    # æ·»åŠ æ–‡ä»¶åä¿¡æ¯ï¼ˆå¯èƒ½åŒ…å«æ—¥æœŸç­‰ä¿¡æ¯ï¼‰
                    df['data_source'] = file_path.stem
                    all_data.append(df)
                except Exception as e:
                    print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
                    continue
            
            if not all_data:
                print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
                return False
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            self.stock_data = pd.concat(all_data, ignore_index=True)
            print(f"   âœ… æˆåŠŸåŠ è½½æ•°æ®: {self.stock_data.shape}")
            print(f"   ğŸ“Š æ•°æ®åˆ—: {list(self.stock_data.columns)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def analyze_data_structure(self):
        """åˆ†ææ•°æ®ç»“æ„"""
        print("\nğŸ“Š æ•°æ®ç»“æ„åˆ†æ:")
        print(f"   æ€»è¡Œæ•°: {len(self.stock_data):,}")
        print(f"   æ€»åˆ—æ•°: {len(self.stock_data.columns)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨ä»£ç åˆ—
        code_columns = [col for col in self.stock_data.columns 
                       if any(keyword in col.lower() for keyword in ['code', 'ä»£ç ', 'symbol'])]
        print(f"   å¯èƒ½çš„è‚¡ç¥¨ä»£ç åˆ—: {code_columns}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸåˆ—
        date_columns = [col for col in self.stock_data.columns 
                       if any(keyword in col.lower() for keyword in ['date', 'æ—¥æœŸ', 'time', 'æ—¶é—´'])]
        print(f"   å¯èƒ½çš„æ—¥æœŸåˆ—: {date_columns}")
        
        # æ£€æŸ¥æ•°å€¼åˆ—
        numeric_columns = self.stock_data.select_dtypes(include=[np.number]).columns.tolist()
        print(f"   æ•°å€¼åˆ—æ•°é‡: {len(numeric_columns)}")
        
        # æ˜¾ç¤ºå‰å‡ åˆ—æ ·ä¾‹
        print(f"\n   å‰5åˆ—æ•°æ®é¢„è§ˆ:")
        print(self.stock_data.head().iloc[:, :5])
        
        return {
            'total_rows': len(self.stock_data),
            'total_columns': len(self.stock_data.columns),
            'code_columns': code_columns,
            'date_columns': date_columns,
            'numeric_columns': numeric_columns
        }
    
    def create_features_and_targets(self, target_column: str = None, 
                                   code_column: str = None,
                                   date_column: str = None,
                                   lookback_days: int = 5,
                                   target_days: int = 1) -> bool:
        """
        åˆ›å»ºç‰¹å¾å’Œç›®æ ‡æ•°æ®
        
        Args:
            target_column: ç›®æ ‡åˆ—åï¼ˆå¦‚è‚¡ä»·æ¶¨è·Œå¹…ï¼‰
            code_column: è‚¡ç¥¨ä»£ç åˆ—å
            date_column: æ—¥æœŸåˆ—å
            lookback_days: å›çœ‹å¤©æ•°ï¼ˆç”¨äºåˆ›å»ºæ»åç‰¹å¾ï¼‰
            target_days: é¢„æµ‹å¤©æ•°
        """
        try:
            print(f"\nğŸ”§ åˆ›å»ºç‰¹å¾å’Œç›®æ ‡æ•°æ®...")
            
            # è‡ªåŠ¨è¯†åˆ«å…³é”®åˆ—
            if code_column is None:
                code_columns = [col for col in self.stock_data.columns 
                               if any(keyword in col.lower() for keyword in ['code', 'ä»£ç ', 'symbol'])]
                code_column = code_columns[0] if code_columns else self.stock_data.columns[0]
                print(f"   è‡ªåŠ¨é€‰æ‹©è‚¡ç¥¨ä»£ç åˆ—: {code_column}")
            
            if date_column is None:
                date_columns = [col for col in self.stock_data.columns 
                               if any(keyword in col.lower() for keyword in ['date', 'æ—¥æœŸ', 'time', 'æ—¶é—´'])]
                if date_columns:
                    date_column = date_columns[0]
                    print(f"   è‡ªåŠ¨é€‰æ‹©æ—¥æœŸåˆ—: {date_column}")
            
            if target_column is None:
                # å¯»æ‰¾æ¶¨è·Œå¹…ç›¸å…³åˆ—
                pct_columns = [col for col in self.stock_data.columns 
                              if any(keyword in col.lower() for keyword in ['pct', 'æ¶¨è·Œ', 'return', 'æ”¶ç›Š'])]
                if pct_columns:
                    target_column = pct_columns[0]
                    print(f"   è‡ªåŠ¨é€‰æ‹©ç›®æ ‡åˆ—: {target_column}")
                else:
                    print("âŒ æ— æ³•è‡ªåŠ¨è¯†åˆ«ç›®æ ‡åˆ—ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š")
                    return False
            
            # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºdatetimeæ ¼å¼
            if date_column and date_column in self.stock_data.columns:
                self.stock_data[date_column] = pd.to_datetime(self.stock_data[date_column])
                self.stock_data = self.stock_data.sort_values([code_column, date_column])
            
            # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            self.stock_codes = self.stock_data[code_column].unique().tolist()
            print(f"   è‚¡ç¥¨æ•°é‡: {len(self.stock_codes)}")
            
            # æ„å»ºç‰¹å¾å’Œç›®æ ‡æ•°æ®
            features_list = []
            targets_list = []
            
            for stock_code in tqdm(self.stock_codes, desc="å¤„ç†è‚¡ç¥¨æ•°æ®"):
                stock_df = self.stock_data[self.stock_data[code_column] == stock_code].copy()
                
                if len(stock_df) < lookback_days + target_days:
                    continue  # æ•°æ®ä¸è¶³ï¼Œè·³è¿‡
                
                # åˆ›å»ºæ»åç‰¹å¾
                numeric_cols = stock_df.select_dtypes(include=[np.number]).columns.tolist()
                
                for i in range(lookback_days, len(stock_df) - target_days + 1):
                    # ç‰¹å¾ï¼šè¿‡å»lookback_dayså¤©çš„æ•°æ®
                    feature_row = {}
                    feature_row['stock_code'] = stock_code
                    
                    if date_column:
                        feature_row['date'] = stock_df.iloc[i + target_days - 1][date_column]
                    
                    # æ·»åŠ å†å²ç‰¹å¾
                    for day in range(lookback_days):
                        day_data = stock_df.iloc[i - lookback_days + day]
                        for col in numeric_cols:
                            if col == target_column:
                                continue
                            feature_row[f'{col}_lag_{lookback_days - day}'] = day_data[col]
                    
                    # æ·»åŠ ç»Ÿè®¡ç‰¹å¾
                    window_data = stock_df.iloc[i - lookback_days:i]
                    for col in numeric_cols:
                        if col == target_column:
                            continue
                        values = window_data[col]
                        feature_row[f'{col}_mean'] = values.mean()
                        feature_row[f'{col}_std'] = values.std()
                        feature_row[f'{col}_max'] = values.max()
                        feature_row[f'{col}_min'] = values.min()
                    
                    features_list.append(feature_row)
                    
                    # ç›®æ ‡ï¼šæœªæ¥target_dayså¤©çš„æ”¶ç›Š
                    target_value = stock_df.iloc[i + target_days - 1][target_column]
                    targets_list.append({
                        'stock_code': stock_code,
                        'target': target_value
                    })
            
            # è½¬æ¢ä¸ºDataFrame
            self.feature_data = pd.DataFrame(features_list)
            self.target_data = pd.DataFrame(targets_list)
            
            print(f"   âœ… ç‰¹å¾æ•°æ®shape: {self.feature_data.shape}")
            print(f"   âœ… ç›®æ ‡æ•°æ®shape: {self.target_data.shape}")
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = [col for col in self.feature_data.columns 
                                 if col not in ['stock_code', 'date']]
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºç‰¹å¾å’Œç›®æ ‡æ•°æ®å¤±è´¥: {e}")
            return False
    
    def save_processed_data(self) -> bool:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        try:
            print(f"\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
            
            # åˆ›å»ºæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_folder = self.output_dir / f"processed_{timestamp}"
            output_folder.mkdir(exist_ok=True)
            
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
            X_features = self.feature_data[self.feature_names]
            y_targets = self.target_data['target']
            
            # ä¿å­˜CSVæ–‡ä»¶
            X_features.to_csv(output_folder / "X_features.csv", index=False, encoding='utf-8')
            y_targets.to_csv(output_folder / "y_targets.csv", index=False, encoding='utf-8')
            
            # ä¿å­˜å®Œæ•´æ•°æ®ï¼ˆåŒ…å«è‚¡ç¥¨ä»£ç ç­‰ä¿¡æ¯ï¼‰
            full_data = self.feature_data.copy()
            full_data['target'] = y_targets
            full_data.to_csv(output_folder / "full_data.csv", index=False, encoding='utf-8')
            
            # ä¿å­˜è‚¡ç¥¨ä»£ç ä¿¡æ¯
            stock_info = {
                'stock_codes': self.stock_codes,
                'total_stocks': len(self.stock_codes),
                'total_samples': len(self.feature_data),
                'feature_count': len(self.feature_names)
            }
            with open(output_folder / "stock_codes.json", 'w', encoding='utf-8') as f:
                json.dump(stock_info, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ•°æ®å¤„ç†ä¿¡æ¯
            data_info = {
                'processing_time': timestamp,
                'input_data_dir': str(self.data_dir),
                'output_data_dir': str(output_folder),
                'total_samples': len(self.feature_data),
                'feature_count': len(self.feature_names),
                'feature_names': self.feature_names[:10],  # åªä¿å­˜å‰10ä¸ªç‰¹å¾åä½œä¸ºæ ·ä¾‹
                'data_shape': {
                    'features': list(X_features.shape),
                    'targets': list(y_targets.shape)
                }
            }
            with open(output_folder / "data_info.json", 'w', encoding='utf-8') as f:
                json.dump(data_info, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_folder}")
            print(f"   ğŸ“ X_features.csv: {X_features.shape}")
            print(f"   ğŸ“ y_targets.csv: {y_targets.shape}")
            print(f"   ğŸ“ full_data.csv: {full_data.shape}")
            print(f"   ğŸ“ stock_codes.json: {len(self.stock_codes)} åªè‚¡ç¥¨")
            print(f"   ğŸ“ data_info.json: æ•°æ®å¤„ç†ä¿¡æ¯")
            
            # è¿”å›è¾“å‡ºè·¯å¾„ä¾›åç»­ä½¿ç”¨
            self.processed_data_path = output_folder
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def run_full_pipeline(self, target_column: str = None, 
                         code_column: str = None,
                         date_column: str = None,
                         lookback_days: int = 5,
                         target_days: int = 1) -> str:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
        
        Returns:
            str: å¤„ç†åæ•°æ®çš„è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹è‚¡ç¥¨æ•°æ®å¤„ç†æµç¨‹...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_parquet_data():
            return None
        
        # 2. åˆ†ææ•°æ®ç»“æ„
        self.analyze_data_structure()
        
        # 3. åˆ›å»ºç‰¹å¾å’Œç›®æ ‡
        if not self.create_features_and_targets(
            target_column=target_column,
            code_column=code_column,
            date_column=date_column,
            lookback_days=lookback_days,
            target_days=target_days
        ):
            return None
        
        # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®
        if not self.save_processed_data():
            return None
        
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®æ¦‚å†µ:")
        print(f"   - æ ·æœ¬æ•°é‡: {len(self.feature_data):,}")
        print(f"   - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
        print(f"   - è‚¡ç¥¨æ•°é‡: {len(self.stock_codes)}")
        print(f"   - æ•°æ®è·¯å¾„: {self.processed_data_path}")
        
        return str(self.processed_data_path)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨æ•°æ®é¢„å¤„ç†')
    parser.add_argument('--data_dir', type=str, default='data/professional_parquet',
                       help='è¾“å…¥æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='data/processed_stock_data',
                       help='è¾“å‡ºæ•°æ®ç›®å½•')
    parser.add_argument('--target_column', type=str, default=None,
                       help='ç›®æ ‡åˆ—å')
    parser.add_argument('--code_column', type=str, default=None,
                       help='è‚¡ç¥¨ä»£ç åˆ—å')
    parser.add_argument('--date_column', type=str, default=None,
                       help='æ—¥æœŸåˆ—å')
    parser.add_argument('--lookback_days', type=int, default=5,
                       help='å›çœ‹å¤©æ•°')
    parser.add_argument('--target_days', type=int, default=1,
                       help='é¢„æµ‹å¤©æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = StockDataProcessor(
        data_dir=args.data_dir,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œå¤„ç†æµç¨‹
    result_path = processor.run_full_pipeline(
        target_column=args.target_column,
        code_column=args.code_column,
        date_column=args.date_column,
        lookback_days=args.lookback_days,
        target_days=args.target_days
    )
    
    if result_path:
        print(f"\nğŸ‰ å¤„ç†æˆåŠŸ! æ•°æ®å·²ä¿å­˜åˆ°: {result_path}")
        print("\nğŸ“ ä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬:")
        print(f"python lightgbm_train.py --config config/train/lightGBM_train.yaml")
    else:
        print("\nâŒ æ•°æ®å¤„ç†å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()