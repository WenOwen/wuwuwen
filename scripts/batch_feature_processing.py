# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç‰¹å¾å¤„ç†è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç¼“å­˜ç³»ç»Ÿé«˜æ•ˆå¤„ç†å¤§é‡è‚¡ç¥¨
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from core.feature_engineering import FeatureEngineering
from core.feature_cache import BatchFeatureProcessor
import logging
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_stock_data(stock_code: str, data_dir: str = "data/datas_em") -> pd.DataFrame:
    """
    åŠ è½½è‚¡ç¥¨æ•°æ®
    """
    try:
        file_path = os.path.join(data_dir, f"{stock_code}.csv")
        if not os.path.exists(file_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
        if 'äº¤æ˜“æ—¥æœŸ' in df.columns:
            df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
        
        # ç¡®ä¿åŸºæœ¬åˆ—å­˜åœ¨
        required_cols = ['æ”¶ç›˜ä»·', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æˆäº¤é‡']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"è‚¡ç¥¨ {stock_code} ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return None
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        if len(df) < 60:  # è‡³å°‘éœ€è¦60å¤©æ•°æ®
            logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®é‡ä¸è¶³: {len(df)} å¤©")
            return None
        
        return df.sort_values('äº¤æ˜“æ—¥æœŸ').reset_index(drop=True)
        
    except Exception as e:
        logger.error(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ•°æ®å¤±è´¥: {e}")
        return None

def get_stock_list(data_dir: str = "data/datas_em", limit: int = None) -> list:
    """
    è·å–è‚¡ç¥¨åˆ—è¡¨
    """
    try:
        if not os.path.exists(data_dir):
            logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return []
        
        csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
        stock_codes = [f.replace('.csv', '') for f in csv_files]
        
        if limit:
            stock_codes = stock_codes[:limit]
        
        logger.info(f"æ‰¾åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
        return stock_codes
        
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        return []

def demo_traditional_processing(stock_codes: list, data_dir: str = "data/datas_em"):
    """
    æ¼”ç¤ºä¼ ç»Ÿå¤„ç†æ–¹å¼ï¼ˆæ— ç¼“å­˜ï¼‰
    """
    print("\nğŸŒ ä¼ ç»Ÿå¤„ç†æ–¹å¼ (æ— ç¼“å­˜):")
    
    fe = FeatureEngineering(enable_cache=False)
    start_time = time.time()
    
    processed_count = 0
    for i, stock_code in enumerate(stock_codes):
        try:
            if i % 5 == 0:
                print(f"è¿›åº¦: {i+1}/{len(stock_codes)} ({(i+1)/len(stock_codes)*100:.1f}%)")
            
            df = load_stock_data(stock_code, data_dir)
            if df is None:
                continue
            
            df_features = fe.create_all_features(df, stock_code)
            processed_count += 1
            
        except Exception as e:
            logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_code} å¤±è´¥: {e}")
            continue
    
    end_time = time.time()
    
    print(f"âœ… ä¼ ç»Ÿæ–¹å¼å®Œæˆ:")
    print(f"  å¤„ç†è‚¡ç¥¨: {processed_count}")
    print(f"  æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"  å¹³å‡æ¯åª: {(end_time - start_time) / max(processed_count, 1):.2f} ç§’")

def demo_cached_processing(stock_codes: list, data_dir: str = "data/datas_em"):
    """
    æ¼”ç¤ºç¼“å­˜å¤„ç†æ–¹å¼
    """
    print("\nğŸš€ ç¼“å­˜å¤„ç†æ–¹å¼:")
    
    fe = FeatureEngineering(enable_cache=True)
    processor = BatchFeatureProcessor(fe)
    
    start_time = time.time()
    
    # ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆå»ºç«‹ç¼“å­˜ï¼‰
    def data_loader(stock_code):
        return load_stock_data(stock_code, data_dir)
    
    results = processor.process_stocks_with_cache(stock_codes, data_loader)
    
    first_run_time = time.time()
    
    print(f"\nâœ… ç¬¬ä¸€æ¬¡å¤„ç†å®Œæˆ (å»ºç«‹ç¼“å­˜):")
    print(f"  å¤„ç†è‚¡ç¥¨: {len(results)}")
    print(f"  æ€»è€—æ—¶: {first_run_time - start_time:.2f} ç§’")
    print(f"  å¹³å‡æ¯åª: {(first_run_time - start_time) / max(len(results), 1):.2f} ç§’")
    
    # ç¬¬äºŒæ¬¡å¤„ç†ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\nğŸš€ ç¬¬äºŒæ¬¡å¤„ç† (ä½¿ç”¨ç¼“å­˜):")
    second_start_time = time.time()
    
    results2 = processor.process_stocks_with_cache(stock_codes, data_loader)
    
    second_end_time = time.time()
    
    print(f"\nâœ… ç¬¬äºŒæ¬¡å¤„ç†å®Œæˆ (ä½¿ç”¨ç¼“å­˜):")
    print(f"  å¤„ç†è‚¡ç¥¨: {len(results2)}")
    print(f"  æ€»è€—æ—¶: {second_end_time - second_start_time:.2f} ç§’")
    print(f"  å¹³å‡æ¯åª: {(second_end_time - second_start_time) / max(len(results2), 1):.2f} ç§’")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    if len(results) > 0:
        speedup = (first_run_time - start_time) / (second_end_time - second_start_time)
        print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.1f}x")
        print(f"  æ—¶é—´èŠ‚çœ: {((first_run_time - start_time) - (second_end_time - second_start_time)):.2f} ç§’")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ“Š æ‰¹é‡ç‰¹å¾å¤„ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆé™åˆ¶æ•°é‡ç”¨äºæ¼”ç¤ºï¼‰
    stock_codes = get_stock_list(limit=20)  # å…ˆç”¨20åªè‚¡ç¥¨æµ‹è¯•
    
    if len(stock_codes) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨æ•°æ®")
        return
    
    print(f"ğŸ“‹ å°†å¤„ç† {len(stock_codes)} åªè‚¡ç¥¨:")
    print(f"  {', '.join(stock_codes[:10])}{'...' if len(stock_codes) > 10 else ''}")
    
    # æ¼”ç¤ºç¼“å­˜å¤„ç†æ–¹å¼
    demo_cached_processing(stock_codes)
    
    # æ¸…ç†ç¼“å­˜å¹¶é‡ç½®ï¼ˆå¯é€‰ï¼‰
    # print("\nğŸ—‘ï¸ æ¸…ç†ç¼“å­˜...")
    # fe = FeatureEngineering(enable_cache=True)
    # if fe.cache:
    #     fe.cache.clear_all_cache()
    
    print("\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()