#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM è‚¡ç¥¨é¢„æµ‹è„šæœ¬ - äº¤äº’å¼ç‰ˆæœ¬
æ”¯æŒè‡ªåŠ¨æ‰«ææ¨¡å‹å’Œæ•°æ®ï¼Œå¿«æ·é”®é€‰æ‹©
"""

import os
import glob
import yaml
import numpy as np
import pandas as pd
import json
import joblib
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple, List, Any, Optional
import argparse

import lightgbm as lgb
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


class InteractiveLightGBMPredictor:
    """äº¤äº’å¼ LightGBM é¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é¢„æµ‹å™¨"""
        self.models_dir = "models"
        self.data_dir = "data/datas_predict"
        self.config_path = "config/train/lightGBM_train.yaml"
        
        # æ‰«æå¯ç”¨çš„æ¨¡å‹å’Œæ•°æ®
        self.available_models = self._scan_models()
        self.available_data = self._scan_prediction_data()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )
        self.logger = logging.getLogger(__name__)
    
    def _scan_models(self) -> List[Dict]:
        """æ‰«æå¯ç”¨çš„æ¨¡å‹"""
        models = []
        
        if not os.path.exists(self.models_dir):
            return models
            
        # æ‰«ææ‰€æœ‰æ¨¡å‹ç±»å‹æ–‡ä»¶å¤¹
        model_types = ['lightgbm', 'lightgbm_extended', 'lightgbm_fine_tuned', 'lightgbm_optimized']
        
        for model_type in model_types:
            model_type_dir = os.path.join(self.models_dir, model_type)
            if not os.path.exists(model_type_dir):
                continue
                
            # æ‰«æè®­ç»ƒæ–‡ä»¶å¤¹
            training_dirs = glob.glob(os.path.join(model_type_dir, "training*"))
            training_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰é¢
            
            for training_dir in training_dirs:
                # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                pkl_files = glob.glob(os.path.join(training_dir, "*model*.pkl"))
                txt_files = glob.glob(os.path.join(training_dir, "*model*.txt"))
                
                for model_file in pkl_files + txt_files:
                    # æŸ¥æ‰¾é…å¥—æ–‡ä»¶
                    scaler_file = None
                    feature_names_file = None
                    
                    scaler_path = os.path.join(training_dir, "scaler.pkl")
                    if os.path.exists(scaler_path):
                        scaler_file = scaler_path
                        
                    feature_names_path = os.path.join(training_dir, "feature_names.json")
                    if os.path.exists(feature_names_path):
                        feature_names_file = feature_names_path
                    
                    # è·å–æ¨¡å‹ä¿¡æ¯
                    model_name = f"{model_type}_{os.path.basename(training_dir)}"
                    
                    models.append({
                        'name': model_name,
                        'type': model_type,
                        'training_dir': os.path.basename(training_dir),
                        'model_file': model_file,
                        'scaler_file': scaler_file,
                        'feature_names_file': feature_names_file,
                        'full_path': training_dir
                    })
        
        return models
    
    def _scan_prediction_data(self) -> List[Dict]:
        """æ‰«æå¯ç”¨çš„é¢„æµ‹æ•°æ®"""
        data_list = []
        
        if not os.path.exists(self.data_dir):
            return data_list
            
        # æ‰«ææ‰€æœ‰å¤„ç†è¿‡çš„æ•°æ®æ–‡ä»¶å¤¹
        processed_dirs = glob.glob(os.path.join(self.data_dir, "processed_*"))
        processed_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰é¢
        
        for data_dir in processed_dirs:
            # æŸ¥æ‰¾ç‰¹å¾æ–‡ä»¶
            x_features_file = os.path.join(data_dir, "X_features.csv")
            full_data_file = os.path.join(data_dir, "full_data.csv")
            
            if os.path.exists(x_features_file):
                # æŸ¥æ‰¾é…å¥—æ–‡ä»¶
                stock_codes_file = None
                data_info_file = None
                feature_names_file = None
                
                stock_codes_path = os.path.join(data_dir, "stock_codes.json")
                if os.path.exists(stock_codes_path):
                    stock_codes_file = stock_codes_path
                    
                data_info_path = os.path.join(data_dir, "data_info.json")
                if os.path.exists(data_info_path):
                    data_info_file = data_info_path
                    
                feature_names_path = os.path.join(data_dir, "feature_names.json")
                if os.path.exists(feature_names_path):
                    feature_names_file = feature_names_path
                
                # è·å–æ•°æ®ä¿¡æ¯
                data_name = os.path.basename(data_dir)
                
                # è¯»å–æ•°æ®åŸºæœ¬ä¿¡æ¯
                try:
                    df = pd.read_csv(x_features_file)
                    sample_count = len(df)
                    feature_count = len(df.columns)
                except:
                    sample_count = "æœªçŸ¥"
                    feature_count = "æœªçŸ¥"
                
                data_list.append({
                    'name': data_name,
                    'data_dir': data_dir,
                    'x_features_file': x_features_file,
                    'full_data_file': full_data_file if os.path.exists(full_data_file) else None,
                    'stock_codes_file': stock_codes_file,
                    'data_info_file': data_info_file,
                    'feature_names_file': feature_names_file,
                    'sample_count': sample_count,
                    'feature_count': feature_count
                })
        
        return data_list
    
    def display_models(self):
        """æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹"""
        print("\n" + "="*80)
        print("ğŸ¤– å¯ç”¨çš„ LightGBM æ¨¡å‹:")
        print("="*80)
        
        if not self.available_models:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
            return
            
        for i, model in enumerate(self.available_models[:9], 1):  # æœ€å¤šæ˜¾ç¤º9ä¸ª
            print(f"[{i}] {model['name']}")
            print(f"    ğŸ“ ç±»å‹: {model['type']}")
            print(f"    ğŸ“‚ è®­ç»ƒ: {model['training_dir']}")
            print(f"    ğŸ“„ æ¨¡å‹: {os.path.basename(model['model_file'])}")
            print(f"    ğŸ”§ é¢„å¤„ç†å™¨: {'âœ…' if model['scaler_file'] else 'âŒ'}")
            print(f"    ğŸ“‹ ç‰¹å¾åç§°: {'âœ…' if model['feature_names_file'] else 'âŒ'}")
            print()
    
    def display_data(self):
        """æ˜¾ç¤ºå¯ç”¨çš„é¢„æµ‹æ•°æ®"""
        print("\n" + "="*80)
        print("ğŸ“Š å¯ç”¨çš„é¢„æµ‹æ•°æ®:")
        print("="*80)
        
        if not self.available_data:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®æ–‡ä»¶")
            return
            
        for i, data in enumerate(self.available_data[:9], 1):  # æœ€å¤šæ˜¾ç¤º9ä¸ª
            print(f"[{i}] {data['name']}")
            print(f"    ğŸ“ˆ æ ·æœ¬æ•°é‡: {data['sample_count']}")
            print(f"    ğŸ”¢ ç‰¹å¾æ•°é‡: {data['feature_count']}")
            print(f"    ğŸ“„ ç‰¹å¾æ–‡ä»¶: X_features.csv")
            print(f"    ğŸ·ï¸  è‚¡ç¥¨ä»£ç : {'âœ…' if data['stock_codes_file'] else 'âŒ'}")
            print(f"    â„¹ï¸  æ•°æ®ä¿¡æ¯: {'âœ…' if data['data_info_file'] else 'âŒ'}")
            print()
    
    def get_user_choice(self, prompt: str, max_choice: int) -> int:
        """è·å–ç”¨æˆ·é€‰æ‹©"""
        while True:
            try:
                choice = input(f"\n{prompt} (1-{max_choice}, æˆ– 'q' é€€å‡º): ").strip()
                
                if choice.lower() == 'q':
                    print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                    exit(0)
                
                choice_num = int(choice)
                if 1 <= choice_num <= max_choice:
                    return choice_num - 1  # è½¬æ¢ä¸ºç´¢å¼•
                else:
                    print(f"âŒ è¯·è¾“å…¥ 1-{max_choice} ä¹‹é—´çš„æ•°å­—")
                    
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    def load_model_and_components(self, model_info: Dict):
        """åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        self.logger.info(f"åŠ è½½æ¨¡å‹: {model_info['name']}")
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            # é»˜è®¤é…ç½®
            self.config = {
                'data': {
                    'preprocessing': {
                        'timeseries_method': 'flatten',
                        'feature_engineering': {
                            'technical_features': {'enabled': False},
                            'statistical_features': ['mean', 'std', 'max', 'min']
                        }
                    }
                }
            }
        
        # åŠ è½½æ¨¡å‹
        model_file = model_info['model_file']
        if model_file.endswith('.pkl'):
            self.model = joblib.load(model_file)
        elif model_file.endswith('.txt'):
            self.model = lgb.Booster(model_file=model_file)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ–‡ä»¶æ ¼å¼: {model_file}")
        
        # åŠ è½½é¢„å¤„ç†å™¨
        self.scaler = None
        if model_info['scaler_file'] and os.path.exists(model_info['scaler_file']):
            self.scaler = joblib.load(model_info['scaler_file'])
            self.logger.info("âœ… é¢„å¤„ç†å™¨åŠ è½½å®Œæˆ")
        
        # åŠ è½½ç‰¹å¾åç§°
        self.feature_names = None
        if model_info['feature_names_file'] and os.path.exists(model_info['feature_names_file']):
            with open(model_info['feature_names_file'], 'r', encoding='utf-8') as f:
                self.feature_names = json.load(f)
            self.logger.info(f"âœ… ç‰¹å¾åç§°åŠ è½½å®Œæˆï¼Œå…± {len(self.feature_names)} ä¸ªç‰¹å¾")
        
        self.logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_prediction_data(self, data_info: Dict) -> Tuple[np.ndarray, Optional[List], List[str], Optional[np.ndarray]]:
        """åŠ è½½é¢„æµ‹æ•°æ®ï¼ˆåŒ…æ‹¬çœŸå®ç›®æ ‡å€¼ï¼‰"""
        self.logger.info(f"åŠ è½½é¢„æµ‹æ•°æ®: {data_info['name']}")
        
        # åŠ è½½æ•°æ®ä¿¡æ¯
        if data_info['data_info_file']:
            with open(data_info['data_info_file'], 'r', encoding='utf-8') as f:
                self.data_info = json.load(f)
        
        # åŠ è½½ç‰¹å¾æ•°æ®
        df = pd.read_csv(data_info['x_features_file'])
        
        # è·å–ç‰¹å¾åç§°
        feature_names = df.columns.tolist()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‚¡ç¥¨ä»£ç 
        if 'stock_code' in df.columns:
            stock_codes = df['stock_code'].tolist()
            X = df.drop('stock_code', axis=1).values
            feature_names.remove('stock_code')
        else:
            X = df.values
            stock_codes = None
            
        # å¦‚æœæ²¡æœ‰è‚¡ç¥¨ä»£ç ï¼Œå°è¯•ä»JSONæ–‡ä»¶åŠ è½½
        if stock_codes is None and data_info['stock_codes_file']:
            with open(data_info['stock_codes_file'], 'r', encoding='utf-8') as f:
                stock_codes = json.load(f)
        
        # å°è¯•åŠ è½½çœŸå®ç›®æ ‡å€¼
        y_true = None
        y_targets_file = os.path.join(data_info['data_dir'], 'y_targets.csv')
        if os.path.exists(y_targets_file):
            try:
                y_df = pd.read_csv(y_targets_file)
                if 'target' in y_df.columns:
                    y_true = y_df['target'].values
                elif 'y' in y_df.columns:
                    y_true = y_df['y'].values
                elif len(y_df.columns) == 1:
                    y_true = y_df.iloc[:, 0].values
                else:
                    # å¦‚æœæœ‰å¤šåˆ—ï¼Œå°è¯•é€‰æ‹©æ•°å€¼åˆ—
                    numeric_cols = y_df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        y_true = y_df[numeric_cols[0]].values
                
                if y_true is not None and len(y_true) == X.shape[0]:
                    self.logger.info(f"âœ… çœŸå®ç›®æ ‡å€¼åŠ è½½å®Œæˆ: {y_true.shape}")
                else:
                    self.logger.warning(f"ç›®æ ‡å€¼æ•°é‡ä¸åŒ¹é…: y={len(y_true) if y_true is not None else 0}, X={X.shape[0]}")
                    y_true = None
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç›®æ ‡å€¼å¤±è´¥: {e}")
                y_true = None
        else:
            self.logger.info("æœªæ‰¾åˆ°y_targets.csvæ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹æ•ˆæœå¯¹æ¯”")
        
        # å°è¯•ä»ç‰¹å¾åç§°æ–‡ä»¶åŠ è½½æ›´å‡†ç¡®çš„ç‰¹å¾åç§°
        if data_info['feature_names_file']:
            try:
                with open(data_info['feature_names_file'], 'r', encoding='utf-8') as f:
                    loaded_feature_names = json.load(f)
                if isinstance(loaded_feature_names, list) and len(loaded_feature_names) == len(feature_names):
                    feature_names = loaded_feature_names
                    self.logger.info("ä½¿ç”¨ç‰¹å¾åç§°æ–‡ä»¶ä¸­çš„ç‰¹å¾åç§°")
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç‰¹å¾åç§°æ–‡ä»¶å¤±è´¥: {e}")
        
        self.logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {X.shape}, ç‰¹å¾æ•°é‡: {len(feature_names)}")
        return X, stock_codes, feature_names, y_true
    
    def preprocess_data(self, X: np.ndarray, data_feature_names: List[str]) -> np.ndarray:
        """é¢„å¤„ç†æ•°æ®ï¼ˆæŒ‰ç…§è®­ç»ƒæ—¶çš„é¡ºåºï¼‰"""
        self.logger.info("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
        
        # ç¬¬ä¸€æ­¥ï¼šç‰¹å¾å¯¹é½å’Œç»Ÿè®¡ç‰¹å¾æå–ï¼ˆç”Ÿæˆå®Œæ•´çš„ç‰¹å¾å‘é‡ï¼‰
        if self.feature_names is not None:
            X = self._align_and_expand_features(X, data_feature_names)
        
        # ç¬¬äºŒæ­¥ï¼šæ•°æ®æ ‡å‡†åŒ–ï¼ˆåœ¨å®Œæ•´ç‰¹å¾å‘é‡ä¸Šï¼‰
        if self.scaler is not None:
            self.logger.info("ğŸ“Š åº”ç”¨æ•°æ®æ ‡å‡†åŒ–...")
            X = self.scaler.transform(X)
        
        # ç¬¬ä¸‰æ­¥ï¼šç‰¹å¾é€‰æ‹©ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„ç‰¹å¾é€‰æ‹©ï¼‰
        if self.feature_names is not None and 'feature_names' in self.feature_names:
            X = self._select_final_features(X)
        
        return X
    
    def _align_and_expand_features(self, X: np.ndarray, data_feature_names: List[str]) -> np.ndarray:
        """ç¬¬ä¸€æ­¥ï¼šç‰¹å¾å¯¹é½å’Œæ‰©å±•ï¼Œç›´æ¥ç”Ÿæˆæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡"""
        self.logger.info("ğŸ”§ ç‰¹å¾å¯¹é½å’Œæ‰©å±•...")
        
        try:
            # è·å–æ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡
            expected_features = self.scaler.n_features_in_ if self.scaler is not None else 36
            final_feature_names = self.feature_names.get('feature_names', [])
            
            self.logger.info(f"é¢„æµ‹æ•°æ®ç‰¹å¾æ•°é‡: {len(data_feature_names)}")
            self.logger.info(f"æœŸæœ›ç‰¹å¾æ•°é‡: {expected_features}")
            self.logger.info(f"æœ€ç»ˆæ¨¡å‹ç‰¹å¾æ•°é‡: {len(final_feature_names)}")
            
            # åˆ›å»ºç‰¹å¾æ˜ å°„å­—å…¸ï¼ˆç§»é™¤step_00_å‰ç¼€è¿›è¡ŒåŒ¹é…ï¼‰
            data_feature_dict = {}
            for i, name in enumerate(data_feature_names):
                clean_name = name.replace('step_00_', '')
                data_feature_dict[clean_name] = i
                data_feature_dict[name] = i  # ä¹Ÿä¿ç•™åŸå
            
            # é€‰æ‹©æœ€ç»ˆæ¨¡å‹éœ€è¦çš„ç‰¹å¾
            selected_features = []
            used_indices = []
            
            for final_name in final_feature_names:
                clean_final = final_name.replace('step_00_', '')
                found_idx = None
                
                # ç²¾ç¡®åŒ¹é…
                if final_name in data_feature_dict:
                    found_idx = data_feature_dict[final_name]
                elif clean_final in data_feature_dict:
                    found_idx = data_feature_dict[clean_final]
                else:
                    # æ¨¡ç³ŠåŒ¹é…
                    for data_name, idx in data_feature_dict.items():
                        if clean_final in data_name or data_name in clean_final:
                            found_idx = idx
                            break
                
                if found_idx is not None and found_idx not in used_indices:
                    selected_features.append(X[:, found_idx])
                    used_indices.append(found_idx)
                    self.logger.debug(f"åŒ¹é…ç‰¹å¾: {final_name} -> {data_feature_names[found_idx]}")
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°ç‰¹å¾ {final_name}ï¼Œä½¿ç”¨é›¶å¡«å……")
                    selected_features.append(np.zeros(X.shape[0]))
            
            if len(selected_features) != len(final_feature_names):
                raise ValueError(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æœŸæœ›{len(final_feature_names)}ï¼Œå®é™…{len(selected_features)}")
            
            # æ„é€ ç‰¹å¾çŸ©é˜µ
            X_selected = np.column_stack(selected_features)
            self.logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: {X.shape} -> {X_selected.shape}")
            
            # å¦‚æœéœ€è¦æ‰©å±•åˆ°æœŸæœ›çš„ç‰¹å¾æ•°é‡ï¼ˆä¾‹å¦‚mean+stdæ‰©å±•ï¼‰
            if expected_features > X_selected.shape[1]:
                expansion_factor = expected_features // X_selected.shape[1]
                if expansion_factor == 2:
                    # mean + std æ‰©å±•
                    self.logger.info("åº”ç”¨mean+stdæ‰©å±•")
                    features_list = []
                    for i in range(X_selected.shape[1]):
                        features_list.append(X_selected[:, i])  # mean
                        features_list.append(np.zeros(X_selected.shape[0]))  # std (é›¶å€¼)
                    X_expanded = np.column_stack(features_list)
                    self.logger.info(f"ç‰¹å¾æ‰©å±•å®Œæˆ: {X_selected.shape} -> {X_expanded.shape}")
                    return X_expanded
                else:
                    # å…¶ä»–æ‰©å±•æ–¹å¼
                    repeat_data = np.tile(X_selected, (1, expansion_factor))
                    remaining = expected_features - repeat_data.shape[1]
                    if remaining > 0:
                        extra_data = X_selected[:, :remaining]
                        X_expanded = np.column_stack([repeat_data, extra_data])
                    else:
                        X_expanded = repeat_data[:, :expected_features]
                    self.logger.info(f"ç‰¹å¾é‡å¤æ‰©å±•å®Œæˆ: {X_selected.shape} -> {X_expanded.shape}")
                    return X_expanded
            
            return X_selected
                
        except Exception as e:
            self.logger.error(f"ç‰¹å¾å¯¹é½å’Œæ‰©å±•å¤±è´¥: {e}")
            self.logger.info("ä½¿ç”¨åŸå§‹ç‰¹å¾...")
            return X
    
    def _expand_features_with_stats(self, X: np.ndarray, stat_features: List[str], base_names: List[str]) -> np.ndarray:
        """ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾æ‰©å±•å•æ—¶é—´æ­¥æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼šåªä½¿ç”¨mean+stdï¼‰"""
        # æ ¹æ®è®­ç»ƒæ—¶çš„é€»è¾‘ï¼Œä¼¼ä¹åªä½¿ç”¨äº†meanå’Œstdï¼Œè€Œä¸”å¯èƒ½æ˜¯å…ˆé€‰æ‹©ç‰¹å¾å†æ‰©å±•
        final_feature_names = self.feature_names.get('feature_names', [])
        
        # å¦‚æœStandardScaleræœŸæœ›36ä¸ªç‰¹å¾ï¼Œè€Œæœ€ç»ˆç‰¹å¾æ˜¯18ä¸ªï¼Œè¯´æ˜æ˜¯18*2=36
        if hasattr(self, 'scaler') and self.scaler is not None:
            expected_features = self.scaler.n_features_in_
            if expected_features == len(final_feature_names) * 2:
                self.logger.info(f"ä½¿ç”¨ç®€åŒ–ç»Ÿè®¡ç‰¹å¾æ‰©å±•: mean+stdï¼Œç›®æ ‡ç‰¹å¾æ•°: {expected_features}")
                
                # å…ˆé€‰æ‹©æœ€ç»ˆéœ€è¦çš„ç‰¹å¾
                target_features = []
                data_feature_dict = {name.replace('step_00_', ''): i for i, name in enumerate(base_names)}
                
                for final_name in final_feature_names:
                    clean_name = final_name.replace('step_00_', '')
                    # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                    found_idx = None
                    if clean_name in data_feature_dict:
                        found_idx = data_feature_dict[clean_name]
                    else:
                        # æ¨¡ç³ŠåŒ¹é…
                        for data_name, idx in data_feature_dict.items():
                            if clean_name in data_name or data_name in clean_name:
                                found_idx = idx
                                break
                    
                    if found_idx is not None and found_idx < X.shape[1]:
                        target_features.append(X[:, found_idx])
                    else:
                        self.logger.warning(f"æœªæ‰¾åˆ°ç‰¹å¾ {final_name}ï¼Œä½¿ç”¨é›¶å¡«å……")
                        target_features.append(np.zeros(X.shape[0]))
                
                if len(target_features) == len(final_feature_names):
                    # æ„é€ mean+stdç‰¹å¾
                    features_list = []
                    for feature_data in target_features:
                        features_list.append(feature_data)  # mean
                        features_list.append(np.zeros_like(feature_data))  # std (é›¶å€¼)
                    
                    return np.column_stack(features_list)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰é€»è¾‘ä½†åªç”¨mean+std
        features_list = []
        for i, base_name in enumerate(base_names):
            if i >= X.shape[1]:
                break
                
            feature_data = X[:, i]
            features_list.append(feature_data)  # mean
            features_list.append(np.zeros_like(feature_data))  # std
        
        return np.column_stack(features_list)
    
    def _select_final_features(self, X: np.ndarray) -> np.ndarray:
        """ç¬¬ä¸‰æ­¥ï¼šç‰¹å¾é€‰æ‹©ï¼Œé€‰æ‹©æœ€ç»ˆç”¨äºæ¨¡å‹çš„ç‰¹å¾"""
        try:
            final_feature_names = self.feature_names.get('feature_names', [])
            final_feature_count = len(final_feature_names)
            
            self.logger.info(f"ğŸ¯ é€‰æ‹©æœ€ç»ˆç‰¹å¾: {X.shape[1]} -> {final_feature_count}")
            
            # ç®€å•é€‰æ‹©å‰Nä¸ªç‰¹å¾ï¼ˆå‡è®¾ç‰¹å¾é€‰æ‹©ä¿ç•™äº†å‰é¢çš„ç‰¹å¾ï¼‰
            if X.shape[1] >= final_feature_count:
                X_final = X[:, :final_feature_count]
                self.logger.info(f"æœ€ç»ˆç‰¹å¾é€‰æ‹©å®Œæˆ: {X.shape} -> {X_final.shape}")
                return X_final
            else:
                self.logger.warning(f"å¯ç”¨ç‰¹å¾æ•°é‡({X.shape[1]})å°‘äºéœ€è¦çš„ç‰¹å¾æ•°é‡({final_feature_count})")
                return X
                
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return X

    def _align_features(self, X: np.ndarray, data_feature_names: List[str]) -> np.ndarray:
        """æ ¹æ®è®­ç»ƒæ—¶çš„ç‰¹å¾æ˜ å°„å¯¹é½ç‰¹å¾"""
        self.logger.info("ğŸ”§ å¯¹é½ç‰¹å¾...")
        
        try:
            # è·å–è®­ç»ƒæ—¶çš„ç‰¹å¾ä¿¡æ¯
            training_features = self.feature_names.get('feature_names', [])
            feature_mapping = self.feature_names.get('feature_mapping', {})
            timeseries_method = self.feature_names.get('timeseries_method', 'flatten')
            
            self.logger.info(f"è®­ç»ƒç‰¹å¾æ•°é‡: {len(training_features)}")
            self.logger.info(f"é¢„æµ‹æ•°æ®ç‰¹å¾æ•°é‡: {len(data_feature_names)}")
            self.logger.info(f"æ—¶åºå¤„ç†æ–¹æ³•: {timeseries_method}")
            
            # åˆ›å»ºç‰¹å¾æ˜ å°„å­—å…¸
            data_feature_dict = {name: i for i, name in enumerate(data_feature_names)}
            
            # é€‰æ‹©è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾
            selected_features = []
            feature_indices = []
            
            for feature_name in training_features:
                if feature_name in data_feature_dict:
                    selected_features.append(feature_name)
                    feature_indices.append(data_feature_dict[feature_name])
                else:
                    self.logger.warning(f"é¢„æµ‹æ•°æ®ä¸­ç¼ºå°‘ç‰¹å¾: {feature_name}")
            
            if not feature_indices:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç‰¹å¾")
            
            # é€‰æ‹©å¯¹åº”çš„ç‰¹å¾åˆ—
            X_selected = X[:, feature_indices]
            self.logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: {X.shape} -> {X_selected.shape}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œç»Ÿè®¡ç‰¹å¾æå–
            # å¯¹äºå•æ—¶é—´æ­¥æ•°æ®ï¼ˆå¦‚é¢„æµ‹æ•°æ®ï¼‰ï¼Œä¸è¿›è¡Œç»Ÿè®¡ç‰¹å¾æå–
            if timeseries_method == 'statistical':
                # æ£€æŸ¥æ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡
                if hasattr(self, 'scaler') and self.scaler is not None:
                    expected_features = self.scaler.n_features_in_
                    self.logger.info(f"æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡: {expected_features}")
                    
                    # å¦‚æœé€‰æ‹©çš„ç‰¹å¾æ•°é‡å·²ç»åŒ¹é…ï¼Œç›´æ¥è¿”å›
                    if X_selected.shape[1] == expected_features:
                        self.logger.info("ç‰¹å¾æ•°é‡å·²åŒ¹é…ï¼Œè·³è¿‡ç»Ÿè®¡ç‰¹å¾æå–")
                        return X_selected
                    
                    # å¦‚æœéœ€è¦æ‰©å±•ç‰¹å¾ï¼Œè¿›è¡Œç»Ÿè®¡ç‰¹å¾æå–
                    elif X_selected.shape[1] < expected_features:
                        X_processed = self._apply_statistical_features_for_single_step(X_selected, expected_features)
                        self.logger.info(f"ç»Ÿè®¡ç‰¹å¾æå–å®Œæˆ: {X_selected.shape} -> {X_processed.shape}")
                        return X_processed
                
                # é»˜è®¤æƒ…å†µï¼Œä¸è¿›è¡Œç»Ÿè®¡ç‰¹å¾æå–
                self.logger.info("å•æ—¶é—´æ­¥æ•°æ®ï¼Œè·³è¿‡ç»Ÿè®¡ç‰¹å¾æå–")
                return X_selected
            
            return X_selected
            
        except Exception as e:
            self.logger.error(f"ç‰¹å¾å¯¹é½å¤±è´¥: {e}")
            self.logger.info("ä½¿ç”¨åŸå§‹ç‰¹å¾...")
            return X
    
    def _apply_statistical_features_for_single_step(self, X: np.ndarray, expected_features: int) -> np.ndarray:
        """ä¸ºå•æ—¶é—´æ­¥æ•°æ®åº”ç”¨ç»Ÿè®¡ç‰¹å¾æ‰©å±•"""
        current_features = X.shape[1]
        
        if expected_features == current_features * 2:
            # æœ€å¸¸è§æƒ…å†µï¼šæ¯ä¸ªç‰¹å¾æ‰©å±•ä¸º2ä¸ªï¼ˆmean + stdï¼‰
            self.logger.info("åº”ç”¨mean+stdæ‰©å±•")
            features_list = []
            
            for i in range(current_features):
                feature_data = X[:, i]
                # Mean: åŸå§‹å€¼
                features_list.append(feature_data)
                # Std: é›¶å€¼ï¼ˆå•æ—¶é—´æ­¥æ— å˜åŒ–ï¼‰
                features_list.append(np.zeros_like(feature_data))
            
            return np.column_stack(features_list)
            
        elif expected_features == current_features:
            # ç‰¹å¾æ•°é‡å·²åŒ¹é…
            return X
            
        else:
            # å…¶ä»–æƒ…å†µï¼šå°è¯•ç®€å•å¤åˆ¶æˆ–æˆªæ–­
            if expected_features > current_features:
                # éœ€è¦æ‰©å±•ç‰¹å¾
                repeat_times = expected_features // current_features
                remaining = expected_features % current_features
                
                features_list = []
                for _ in range(repeat_times):
                    features_list.append(X)
                
                if remaining > 0:
                    features_list.append(X[:, :remaining])
                
                return np.column_stack(features_list)
            else:
                # éœ€è¦æˆªæ–­ç‰¹å¾
                return X[:, :expected_features]

    def _apply_statistical_features(self, X: np.ndarray) -> np.ndarray:
        """åº”ç”¨ç»Ÿè®¡ç‰¹å¾æå–ï¼ˆæ¨¡æ‹Ÿæ—¶åºç»Ÿè®¡å¤„ç†ï¼‰"""
        # è·å–ç»Ÿè®¡ç‰¹å¾é…ç½®
        stat_features = ['mean', 'std']  # é»˜è®¤ç»Ÿè®¡ç‰¹å¾
        if hasattr(self, 'config') and 'data' in self.config:
            stat_config = self.config['data']['preprocessing']['feature_engineering'].get('statistical_features', ['mean', 'std'])
            if stat_config:
                stat_features = stat_config
        
        # ä¸ºæ¯ä¸ªç‰¹å¾è®¡ç®—ç»Ÿè®¡é‡
        features_list = []
        
        for i in range(X.shape[1]):
            feature_data = X[:, i]
            
            for stat in stat_features:
                if stat == 'mean':
                    # å¯¹äºå•æ—¶é—´æ­¥æ•°æ®ï¼Œå‡å€¼å°±æ˜¯è‡ªèº«
                    features_list.append(feature_data)
                elif stat == 'std':
                    # å¯¹äºå•æ—¶é—´æ­¥æ•°æ®ï¼Œæ ‡å‡†å·®è®¾ä¸º0ï¼ˆæˆ–å°å€¼ï¼‰
                    features_list.append(np.zeros_like(feature_data))
                elif stat == 'max':
                    features_list.append(feature_data)
                elif stat == 'min':
                    features_list.append(feature_data)
                elif stat == 'median':
                    features_list.append(feature_data)
        
        if features_list:
            return np.column_stack(features_list)
        else:
            return X
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """è¿›è¡Œé¢„æµ‹"""
        self.logger.info(f"ğŸ¯ å¼€å§‹é¢„æµ‹ï¼Œæ ·æœ¬æ•°é‡: {X.shape[0]}")
        
        # æ£€æŸ¥ç‰¹å¾æ•°é‡
        if hasattr(self.model, 'n_features_'):
            expected_features = self.model.n_features_
            if X.shape[1] != expected_features:
                self.logger.warning(f"âš ï¸  ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_features}, å®é™… {X.shape[1]}")
        
        # è¿›è¡Œé¢„æµ‹
        if hasattr(self.model, 'predict'):
            # sklearné£æ ¼çš„æ¨¡å‹
            predictions = self.model.predict(X)
        else:
            # LightGBM Boosterå¯¹è±¡
            predictions = self.model.predict(X)
        
        self.logger.info("âœ… é¢„æµ‹å®Œæˆ")
        return predictions
    
    def save_predictions(self, predictions: np.ndarray, stock_codes: Optional[List] = None, 
                        model_name: str = "", data_name: str = "", y_true: Optional[np.ndarray] = None) -> str:
        """
        ä¿å­˜é¢„æµ‹ç»“æœï¼ˆåŒ…å«çœŸå®å€¼å¯¹æ¯”ï¼‰
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            model_name: æ¨¡å‹åç§°
            data_name: æ•°æ®åç§°  
            y_true: çœŸå®ç›®æ ‡å€¼
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "results/predictions"
        os.makedirs(output_dir, exist_ok=True)
        
        output_file = os.path.join(output_dir, f"predictions_{model_name}_{data_name}_{timestamp}.csv")
        
        # åˆ›å»ºç»“æœDataFrame
        if stock_codes is not None and len(stock_codes) == len(predictions):
            df = pd.DataFrame({
                'stock_code': stock_codes,
                'prediction': predictions
            })
        else:
            df = pd.DataFrame({
                'sample_id': range(len(predictions)),
                'prediction': predictions
            })
        
        # æ·»åŠ çœŸå®å€¼å’Œå·®å€¼å¯¹æ¯”
        if y_true is not None and len(y_true) == len(predictions):
            df['y_true'] = y_true
            df['difference'] = predictions - y_true
            df['abs_difference'] = np.abs(predictions - y_true)
            df['error_rate'] = np.abs(predictions - y_true) / (np.abs(y_true) + 1e-8) * 100  # ç›¸å¯¹è¯¯å·®ç‡(%)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mae = np.mean(np.abs(predictions - y_true))
            mse = np.mean((predictions - y_true) ** 2)
            rmse = np.sqrt(mse)
            r2 = 1 - (np.sum((y_true - predictions) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
            
            # æ·»åŠ è¯„ä¼°æŒ‡æ ‡åˆ°æ¯è¡Œï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
            df['mae'] = mae
            df['mse'] = mse
            df['rmse'] = rmse
            df['r2_score'] = r2
            
            self.logger.info(f"ğŸ“Š é¢„æµ‹è¯„ä¼°æŒ‡æ ‡:")
            self.logger.info(f"   MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f}")
            self.logger.info(f"   MSE (å‡æ–¹è¯¯å·®): {mse:.4f}")
            self.logger.info(f"   RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f}")
            self.logger.info(f"   RÂ² (å†³å®šç³»æ•°): {r2:.4f}")
        
        # æ·»åŠ å…ƒä¿¡æ¯
        df['model_name'] = model_name
        df['data_name'] = data_name
        df['prediction_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ä¿å­˜ç»“æœ
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        self.logger.info(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def run_interactive_prediction(self):
        """è¿è¡Œäº¤äº’å¼é¢„æµ‹"""
        print("ğŸš€ æ¬¢è¿ä½¿ç”¨ LightGBM äº¤äº’å¼è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ!")
        print("=" * 80)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹å’Œæ•°æ®
        if not self.available_models:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ models æ–‡ä»¶å¤¹")
            return
        
        if not self.available_data:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®ï¼Œè¯·æ£€æŸ¥ data/datas_predict æ–‡ä»¶å¤¹")
            return
        
        while True:
            try:
                # æ˜¾ç¤ºå¹¶é€‰æ‹©æ¨¡å‹
                self.display_models()
                model_idx = self.get_user_choice("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹", len(self.available_models))
                selected_model = self.available_models[model_idx]
                
                # æ˜¾ç¤ºå¹¶é€‰æ‹©æ•°æ®
                self.display_data()
                data_idx = self.get_user_choice("è¯·é€‰æ‹©è¦é¢„æµ‹çš„æ•°æ®", len(self.available_data))
                selected_data = self.available_data[data_idx]
                
                print(f"\nğŸ¯ å¼€å§‹é¢„æµ‹:")
                print(f"   ğŸ“Š æ¨¡å‹: {selected_model['name']}")
                print(f"   ğŸ“ˆ æ•°æ®: {selected_data['name']}")
                print("-" * 60)
                
                # åŠ è½½æ¨¡å‹
                self.load_model_and_components(selected_model)
                
                # åŠ è½½æ•°æ®
                X, stock_codes, data_feature_names, y_true = self.load_prediction_data(selected_data)
                
                # é¢„å¤„ç†æ•°æ®
                X = self.preprocess_data(X, data_feature_names)
                
                # è¿›è¡Œé¢„æµ‹
                predictions = self.predict(X)
                
                # ä¿å­˜ç»“æœ
                output_file = self.save_predictions(
                    predictions, 
                    stock_codes, 
                    selected_model['name'],
                    selected_data['name'],
                    y_true
                )
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœç»Ÿè®¡
                print(f"\nğŸ“ˆ é¢„æµ‹ç»“æœç»Ÿè®¡:")
                print(f"   æ ·æœ¬æ•°é‡: {len(predictions)}")
                print(f"   é¢„æµ‹å‡å€¼: {np.mean(predictions):.4f}")
                print(f"   é¢„æµ‹æ ‡å‡†å·®: {np.std(predictions):.4f}")
                print(f"   æœ€å°å€¼: {np.min(predictions):.4f}")
                print(f"   æœ€å¤§å€¼: {np.max(predictions):.4f}")
                
                # å¦‚æœæœ‰çœŸå®å€¼ï¼Œæ˜¾ç¤ºå¯¹æ¯”ç»Ÿè®¡
                if y_true is not None:
                    mae = np.mean(np.abs(predictions - y_true))
                    mse = np.mean((predictions - y_true) ** 2)
                    rmse = np.sqrt(mse)
                    r2 = 1 - (np.sum((y_true - predictions) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
                    
                    print(f"\nğŸ¯ é¢„æµ‹æ•ˆæœè¯„ä¼°:")
                    print(f"   çœŸå®å€¼å‡å€¼: {np.mean(y_true):.4f}")
                    print(f"   çœŸå®å€¼æ ‡å‡†å·®: {np.std(y_true):.4f}")
                    print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
                    print(f"   å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
                    print(f"   å†³å®šç³»æ•° (RÂ²): {r2:.4f}")
                    print(f"   å¹³å‡ç›¸å¯¹è¯¯å·®: {np.mean(np.abs(predictions - y_true) / (np.abs(y_true) + 1e-8) * 100):.2f}%")
                
                print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                continue_choice = input("\nğŸ”„ æ˜¯å¦ç»§ç»­è¿›è¡Œå…¶ä»–é¢„æµ‹? (y/n): ").strip().lower()
                if continue_choice not in ['y', 'yes', 'æ˜¯']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§!")
                    break
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
                break
            except Exception as e:
                print(f"\nâŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                print("è¯·æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®")
                
                retry_choice = input("ğŸ”„ æ˜¯å¦é‡è¯•? (y/n): ").strip().lower()
                if retry_choice not in ['y', 'yes', 'æ˜¯']:
                    break


def main():
    """ä¸»å‡½æ•°"""
    predictor = InteractiveLightGBMPredictor()
    predictor.run_interactive_prediction()


if __name__ == '__main__':
    main()