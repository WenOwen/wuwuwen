#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUé…ç½®æ¨¡å— - åŒRTX 3090ä¼˜åŒ–é…ç½®
ä¸“é—¨ä¸ºåŒRTX 3090æ˜¾å¡é…ç½®ä¼˜åŒ–
ä¸»è¦é’ˆå¯¹LightGBM GPUåŠ é€Ÿä¼˜åŒ–
"""

import os
import sys
import logging
from typing import Optional, Dict, Any
import subprocess

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# RTX 3090 æ˜¾å¡è§„æ ¼
RTX_3090_SPECS = {
    'memory_gb': 24,        # 24GB æ˜¾å­˜
    'cuda_cores': 10496,    # CUDAæ ¸å¿ƒæ•°
    'tensor_cores': 328,    # ç¬¬ä¸‰ä»£Tensoræ ¸å¿ƒ
    'memory_bandwidth': 936, # GB/s
    'fp16_performance': 35.6, # TFLOPS
    'tensor_performance': 142, # TFLOPS (ç¨€ç–)
    'boost_clock': 1695,    # MHz
    'base_clock': 1395,     # MHz
}

def setup_dual_gpu() -> Optional[Dict[str, Any]]:
    """
    é…ç½®åŒRTX 3090 GPUç¯å¢ƒ - ä¸“é—¨ä¸ºLightGBMä¼˜åŒ–
    
    Returns:
        GPUé…ç½®å­—å…¸ï¼ŒåŒ…å«LightGBM GPUå‚æ•°ï¼Œå¦‚æœé…ç½®å¤±è´¥åˆ™è¿”å›None
    """
    try:
        # æ£€æŸ¥CUDAå’ŒGPUå¯ç”¨æ€§
        gpu_count = _get_gpu_count()
        
        if gpu_count < 1:
            logger.error("æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return None
        
        logger.info(f"æ£€æµ‹åˆ° {gpu_count} å—GPU")
        
        # æ£€æŸ¥GPUå‹å·
        gpu_info = _get_gpu_info()
        
        # é…ç½®LightGBM GPUå‚æ•°
        gpu_config = {
            'device_type': 'gpu',
            'gpu_platform_id': 0,
            'gpu_device_id': 0,  # ä½¿ç”¨ç¬¬ä¸€å—GPUä½œä¸ºä¸»è¦è®¾å¤‡
            'max_bin': 255,      # RTX 3090çš„å¤§æ˜¾å­˜å…è®¸æ›´å¤§çš„binæ•°é‡
            'gpu_use_dp': True,  # å¯ç”¨åŒç²¾åº¦ï¼ˆæ›´å‡†ç¡®ï¼‰
            'num_gpu': min(gpu_count, 2),  # æœ€å¤šä½¿ç”¨2å—GPU
        }
        
        # RTX 3090ç‰¹å®šä¼˜åŒ–
        if any('RTX 3090' in info for info in gpu_info.values()):
            logger.info("æ£€æµ‹åˆ°RTX 3090ï¼Œåº”ç”¨ä¸“ç”¨ä¼˜åŒ–")
            gpu_config.update({
                'max_bin': 511,      # åˆ©ç”¨å¤§æ˜¾å­˜ä½¿ç”¨æ›´å¤§çš„binæ•°é‡
                'feature_fraction': 0.8,  # å¢åŠ ç‰¹å¾é‡‡æ ·ç‡
                'bagging_fraction': 0.8,  # å¢åŠ æ•°æ®é‡‡æ ·ç‡
                'num_leaves': 255,   # å¢åŠ å¶å­èŠ‚ç‚¹æ•°é‡
                'max_depth': 15,     # å¢åŠ æ ‘çš„æ·±åº¦
                'min_data_in_leaf': 10,  # å‡å°‘å¶å­èŠ‚ç‚¹æœ€å°æ•°æ®é‡
            })
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–GPUæ€§èƒ½
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # éé˜»å¡CUDAå¯åŠ¨
        os.environ['CUDA_CACHE_DISABLE'] = '0'    # å¯ç”¨CUDAç¼“å­˜
        os.environ['NVIDIA_TF32_OVERRIDE'] = '1'  # å¯ç”¨TF32åŠ é€Ÿ
        
        # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œé…ç½®æ•°æ®å¹¶è¡Œ
        if gpu_count >= 2:
            logger.info("é…ç½®åŒGPUå¹¶è¡Œè®­ç»ƒ")
            gpu_config['parallel_threads'] = gpu_count * 8  # æ¯ä¸ªGPU 8ä¸ªçº¿ç¨‹
            
        logger.info("LightGBM GPUé…ç½®å®Œæˆ")
        logger.info(f"é…ç½®å‚æ•°: {gpu_config}")
        
        return gpu_config
        
    except Exception as e:
        logger.error(f"GPUé…ç½®å¤±è´¥: {e}")
        return None

def _get_gpu_count() -> int:
    """è·å–å¯ç”¨GPUæ•°é‡"""
    try:
        # å°è¯•ä½¿ç”¨nvidia-ml-py
        try:
            import pynvml
            pynvml.nvmlInit()
            return pynvml.nvmlDeviceGetCount()
        except:
            pass
        
        # å°è¯•ä½¿ç”¨nvidia-smi
        try:
            result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)
            if result.returncode == 0:
                return len([line for line in result.stdout.strip().split('\n') if line.startswith('GPU')])
        except:
            pass
        
        # å°è¯•ä½¿ç”¨PyTorch
        try:
            import torch
            if torch.cuda.is_available():
                return torch.cuda.device_count()
        except:
            pass
        
        return 0
    except Exception as e:
        logger.error(f"è·å–GPUæ•°é‡å¤±è´¥: {e}")
        return 0

def _get_gpu_info() -> Dict[str, str]:
    """è·å–GPUä¿¡æ¯"""
    gpu_info = {}
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.total', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                if line:
                    parts = line.split(', ')
                    if len(parts) >= 3:
                        index = parts[0]
                        name = parts[1]
                        memory = parts[2]
                        gpu_info[f'GPU_{index}'] = f"{name} ({memory})"
    except Exception as e:
        logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    return gpu_info

def get_optimal_batch_size(base_size: int, gpu_count: int = 2) -> int:
    """
    æ ¹æ®RTX 3090çš„è§„æ ¼åŠ¨æ€è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    ä¸“é—¨ä¸ºLightGBMå’Œè‚¡ç¥¨æ•°æ®è®­ç»ƒä¼˜åŒ–
    
    Args:
        base_size: åŸºç¡€æ‰¹æ¬¡å¤§å°
        gpu_count: GPUæ•°é‡
    
    Returns:
        ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
    """
    # RTX 3090çš„å¤§æ˜¾å­˜å…è®¸ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
    memory_factor = RTX_3090_SPECS['memory_gb'] / 11  # ç›¸å¯¹äºRTX 2080 Ti (11GB)çš„å€æ•°
    
    # æ ¹æ®GPUæ•°é‡å’Œå†…å­˜å¤§å°è°ƒæ•´
    optimal_size = int(base_size * memory_factor)
    
    # LightGBMé’ˆå¯¹æ€§ä¼˜åŒ–
    if gpu_count >= 2:
        # åŒGPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ•°æ®é›†
        optimal_size = int(optimal_size * 1.5)
    
    # é’ˆå¯¹ä¸åŒçš„åŸºç¡€å¤§å°è®¾ç½®åˆç†ä¸Šé™
    if base_size <= 32:
        max_size = 512    # å¯¹äºå°æ‰¹æ¬¡ï¼Œåˆ©ç”¨å¤§æ˜¾å­˜
    elif base_size <= 64:
        max_size = 1024   # å¯¹äºä¸­ç­‰æ‰¹æ¬¡
    else:
        max_size = 2048   # å¯¹äºå¤§æ‰¹æ¬¡ï¼ŒRTX 3090å¯ä»¥å¤„ç†
    
    # ç¡®ä¿æ‰¹æ¬¡å¤§å°åˆç†ä¸”æ˜¯32çš„å€æ•°ï¼ˆå†…å­˜å¯¹é½ä¼˜åŒ–ï¼‰
    optimal_size = min(optimal_size, max_size)
    optimal_size = max(optimal_size, base_size)  # ä¸å°äºåŸºç¡€å¤§å°
    optimal_size = ((optimal_size + 31) // 32) * 32
    
    logger.info(f"æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: {base_size} -> {optimal_size} (GPUæ•°é‡: {gpu_count})")
    
    return optimal_size

def get_memory_info() -> Dict[str, Any]:
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    try:
        memory_info = {}
        gpu_info = _get_gpu_info()
        
        for gpu_id, info in gpu_info.items():
            # è§£æå†…å­˜ä¿¡æ¯
            memory_str = info.split('(')[1].split(')')[0] if '(' in info else "Unknown"
            memory_gb = RTX_3090_SPECS['memory_gb']  # é»˜è®¤RTX 3090è§„æ ¼
            
            if 'RTX 3090' in info:
                memory_gb = 24
            elif 'RTX 3080' in info:
                memory_gb = 10
            elif 'RTX 4090' in info:
                memory_gb = 24
            
            memory_info[gpu_id] = {
                'name': info,
                'total_memory_gb': memory_gb,
                'available_memory_gb': memory_gb - 2,  # é¢„ç•™2GB
                'cuda_cores': RTX_3090_SPECS['cuda_cores'] if 'RTX 3090' in info else 'Unknown',
                'tensor_cores': RTX_3090_SPECS['tensor_cores'] if 'RTX 3090' in info else 'Unknown',
                'boost_clock_mhz': RTX_3090_SPECS['boost_clock'] if 'RTX 3090' in info else 'Unknown'
            }
        
        return memory_info
    except Exception as e:
        logger.error(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    
    return {}

def optimize_for_stock_training() -> Dict[str, Any]:
    """
    ä¸“é—¨ä¸ºè‚¡ç¥¨è®­ç»ƒä¼˜åŒ–çš„é…ç½®
    è¿”å›LightGBMä¸“ç”¨å‚æ•°
    """
    # è‚¡ç¥¨æ•°æ®ç‰¹ç‚¹ï¼šæ—¶é—´åºåˆ—æ•°æ®ï¼Œéœ€è¦é˜²æ­¢è¿‡æ‹Ÿåˆ
    stock_config = {
        # åŸºç¡€é…ç½®
        'objective': 'regression',
        'metric': ['rmse', 'mae'],
        'boosting_type': 'gbdt',
        'num_boost_round': 1000,
        
        # å­¦ä¹ ç›¸å…³
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': 1,
        
        # æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        'lambda_l1': 0.1,
        'lambda_l2': 0.1,
        'min_gain_to_split': 0.02,
        'drop_rate': 0.1,
        
        # æ—©åœ
        'early_stopping_rounds': 100,
        
        # éšæœºç§å­
        'random_state': 42,
        'deterministic': True,
    }
    
    # å¦‚æœæœ‰GPUï¼Œæ·»åŠ GPUé…ç½®
    gpu_count = _get_gpu_count()
    if gpu_count > 0:
        gpu_config = setup_dual_gpu()
        if gpu_config:
            stock_config.update(gpu_config)
    
    logger.info("è‚¡ç¥¨è®­ç»ƒä¸“ç”¨ä¼˜åŒ–é…ç½®å®Œæˆ")
    return stock_config

def get_recommended_settings() -> Dict[str, Any]:
    """
    è·å–åŒRTX 3090çš„æ¨èè®¾ç½®
    ä¸“é—¨ä¸ºLightGBMå’Œè‚¡ç¥¨æ•°æ®ä¼˜åŒ–
    """
    gpu_count = _get_gpu_count()
    
    return {
        'batch_sizes': {
            'small_dataset': get_optimal_batch_size(64, gpu_count),   # å°æ•°æ®é›†
            'medium_dataset': get_optimal_batch_size(128, gpu_count), # ä¸­ç­‰æ•°æ®é›†
            'large_dataset': get_optimal_batch_size(256, gpu_count),  # å¤§æ•°æ®é›†
            'cv_training': get_optimal_batch_size(32, gpu_count),     # äº¤å‰éªŒè¯
        },
        'lightgbm_gpu_settings': {
            'device_type': 'gpu',
            'max_bin': 511 if gpu_count > 0 else 255,
            'gpu_use_dp': True,
            'num_gpu': min(gpu_count, 2),
        },
        'memory_settings': {
            'memory_limit_per_gpu_gb': 22,
            'total_gpu_memory_gb': RTX_3090_SPECS['memory_gb'] * gpu_count,
            'recommended_data_size_gb': RTX_3090_SPECS['memory_gb'] * gpu_count * 0.8,
        },
        'performance_settings': {
            'cuda_cores_total': RTX_3090_SPECS['cuda_cores'] * gpu_count,
            'tensor_cores_total': RTX_3090_SPECS['tensor_cores'] * gpu_count,
            'memory_bandwidth_total': RTX_3090_SPECS['memory_bandwidth'] * gpu_count,
        },
        'stock_training_optimized': True,
        'gpu_count': gpu_count,
    }

def get_lightgbm_gpu_params() -> Dict[str, Any]:
    """
    è·å–LightGBM GPUè®­ç»ƒçš„å®Œæ•´å‚æ•°é…ç½®
    """
    base_params = optimize_for_stock_training()
    gpu_params = setup_dual_gpu()
    
    if gpu_params:
        base_params.update(gpu_params)
        logger.info("LightGBM GPUå‚æ•°é…ç½®å®Œæˆ")
    else:
        logger.warning("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå‚æ•°")
        base_params['device_type'] = 'cpu'
    
    return base_params

def auto_detect_and_setup():
    """è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®GPUç¯å¢ƒ"""
    logger.info("å¼€å§‹è‡ªåŠ¨æ£€æµ‹GPUç¯å¢ƒ...")
    
    # æ£€æµ‹NVIDIA-SMI
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            logger.info("NVIDIAé©±åŠ¨æ£€æµ‹æˆåŠŸ")
            if "RTX 3090" in result.stdout:
                logger.info("âœ… æ£€æµ‹åˆ°RTX 3090æ˜¾å¡")
                gpu_count = result.stdout.count("RTX 3090")
                logger.info(f"RTX 3090æ•°é‡: {gpu_count}")
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°RTX 3090ï¼Œé…ç½®å¯èƒ½éœ€è¦è°ƒæ•´")
        else:
            logger.error("âŒ NVIDIAé©±åŠ¨æ£€æµ‹å¤±è´¥")
    except FileNotFoundError:
        logger.warning("âŒ æœªæ‰¾åˆ°nvidia-smiå‘½ä»¤")
    
    return setup_dual_gpu()

def check_lightgbm_gpu_support() -> bool:
    """
    æ£€æŸ¥LightGBMæ˜¯å¦æ”¯æŒGPU
    """
    try:
        import lightgbm as lgb
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„GPUè®­ç»ƒé›†æ¥æµ‹è¯•
        train_data = lgb.Dataset([[1, 2], [3, 4]], label=[0, 1])
        params = {'device_type': 'gpu', 'objective': 'binary', 'verbose': -1}
        
        # æµ‹è¯•æ˜¯å¦èƒ½æˆåŠŸä½¿ç”¨GPU
        lgb.train(params, train_data, num_boost_round=1, valid_sets=[train_data], 
                 callbacks=[lgb.early_stopping(1)])
        logger.info("âœ… LightGBM GPUæ”¯æŒæ£€æµ‹æˆåŠŸ")
        return True
    except Exception as e:
        logger.warning(f"âŒ LightGBM GPUæ”¯æŒæ£€æµ‹å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    print("=== åŒRTX 3090 GPUé…ç½®æµ‹è¯• ===")
    print("ä¸“é—¨ä¸ºLightGBMè‚¡ç¥¨è®­ç»ƒä¼˜åŒ–\n")
    
    # è‡ªåŠ¨æ£€æµ‹
    gpu_config = auto_detect_and_setup()
    
    print("ğŸ” GPUæ£€æµ‹ç»“æœ:")
    gpu_count = _get_gpu_count()
    print(f"  æ£€æµ‹åˆ°GPUæ•°é‡: {gpu_count}")
    
    if gpu_config:
        print("âœ… GPUé…ç½®æˆåŠŸ")
        print(f"  é…ç½®ç±»å‹: LightGBM GPUä¼˜åŒ–")
        
        # æ˜¾ç¤ºGPUè¯¦ç»†ä¿¡æ¯
        gpu_info = _get_gpu_info()
        if gpu_info:
            print("\nğŸ’¾ GPUè¯¦ç»†ä¿¡æ¯:")
            for gpu_id, info in gpu_info.items():
                print(f"  {gpu_id}: {info}")
        
        # LightGBM GPUæ”¯æŒæµ‹è¯•
        print("\nğŸ§ª LightGBM GPUæ”¯æŒæµ‹è¯•:")
        if check_lightgbm_gpu_support():
            print("  âœ… LightGBM GPUåŠ é€Ÿå¯ç”¨")
        else:
            print("  âŒ LightGBM GPUåŠ é€Ÿä¸å¯ç”¨")
        
        # æ˜¾ç¤ºæ¨èè®¾ç½®
        settings = get_recommended_settings()
        print("\nğŸ“Š æ¨èè®¾ç½®:")
        for key, value in settings.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for sub_key, sub_value in value.items():
                    print(f"    {sub_key}: {sub_value}")
            else:
                print(f"  {key}: {value}")
        
        # æµ‹è¯•æ‰¹æ¬¡å¤§å°ä¼˜åŒ–
        print("\nğŸ”§ æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•:")
        test_sizes = [16, 32, 48, 64, 128]
        for base in test_sizes:
            optimal = get_optimal_batch_size(base, gpu_count)
            print(f"  åŸºç¡€å¤§å° {base:3d} -> ä¼˜åŒ–å {optimal:4d}")
        
        # æ˜¾ç¤ºå®Œæ•´çš„LightGBMå‚æ•°
        print("\nâš™ï¸ LightGBM GPUå®Œæ•´å‚æ•°:")
        lgb_params = get_lightgbm_gpu_params()
        for key, value in lgb_params.items():
            print(f"  {key}: {value}")
            
    else:
        print("âŒ GPUé…ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        
    print("\n=== é…ç½®æµ‹è¯•å®Œæˆ ===")
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    try:
        import json
        config_summary = {
            'gpu_count': _get_gpu_count(),
            'gpu_info': _get_gpu_info(),
            'recommended_settings': get_recommended_settings(),
            'lightgbm_params': get_lightgbm_gpu_params() if gpu_config else None,
            'timestamp': str(subprocess.run(['date'], capture_output=True, text=True).stdout.strip())
        }
        
        with open('gpu_config_summary.json', 'w', encoding='utf-8') as f:
            json.dump(config_summary, f, indent=2, ensure_ascii=False)
        print("ğŸ“ é…ç½®æ‘˜è¦å·²ä¿å­˜åˆ° gpu_config_summary.json")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é…ç½®æ‘˜è¦å¤±è´¥: {e}")