#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM è‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹parquetæ ¼å¼è‚¡ç¥¨æ•°æ®çš„è®­ç»ƒè„šæœ¬
é›†æˆæ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒåŠŸèƒ½
"""

import os
import sys
import yaml
import numpy as np
import pandas as pd
import json
import joblib
import logging
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple, List, Any, Optional
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from stock_data_processor import StockDataProcessor

# æŠ‘åˆ¶å¸¸è§çš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore', category=UserWarning, message='.*X does not have valid feature names.*')
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

import lightgbm as lgb
from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
from sklearn.feature_selection import SelectFromModel, RFE
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import copy
import random
from collections import defaultdict

# å¯¼å…¥å­—ä½“é…ç½®æ¨¡å—
try:
    from font_config import setup_chinese_plot
    setup_chinese_plot()  # è®¾ç½®ä¸­æ–‡å­—ä½“
    print("âœ… ä¸­æ–‡å­—ä½“é…ç½®å·²åŠ è½½")
except ImportError as e:
    print(f"âš ï¸ å­—ä½“é…ç½®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    # ä½¿ç”¨å¤‡ç”¨å­—ä½“é…ç½®
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… ä½¿ç”¨å¤‡ç”¨ä¸­æ–‡å­—ä½“é…ç½®")
except Exception as e:
    print(f"âš ï¸ å­—ä½“é…ç½®å¤±è´¥: {e}")

try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("è­¦å‘Š: Optuna æœªå®‰è£…ï¼Œè¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("è­¦å‘Š: tqdm æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º")


class SuppressOutput:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå®Œå…¨æŠ‘åˆ¶stdoutå’Œstderrè¾“å‡º"""
    def __enter__(self):
        self.stdout = sys.stdout
        self.stderr = sys.stderr
        sys.stdout = StringIO()
        sys.stderr = StringIO()
        return self
    
    def __exit__(self, *args):
        sys.stdout = self.stdout
        sys.stderr = self.stderr


class BuiltinVisualizer:
    """å†…ç½®å¯è§†åŒ–å™¨ï¼Œæ›¿ä»£å¤–éƒ¨å¯è§†åŒ–æ¨¡å—"""
    
    def __init__(self, output_dir, logger=None):
        self.output_dir = Path(output_dir)
        self.logger = logger
        self.training_history = {
            'iteration': [],
            'train_loss': [],
            'val_loss': []
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def record_callback(self):
        """è¿”å›è®­ç»ƒè®°å½•å›è°ƒå‡½æ•°"""
        def callback(env):
            if env.evaluation_result_list:
                iteration = env.iteration
                self.training_history['iteration'].append(iteration)
                
                for eval_result in env.evaluation_result_list:
                    dataset_name, eval_name, result, is_higher_better = eval_result
                    if dataset_name == 'train':
                        self.training_history['train_loss'].append(result)
                    elif dataset_name == 'val':
                        self.training_history['val_loss'].append(result)
        
        return callback
    
    def plot_learning_curves(self, model=None):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib.font_manager as fm
            
            # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®è®¾ç½®
            if 'Microsoft YaHei' not in plt.rcParams['font.sans-serif']:
                plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei'] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
            
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if self.training_history['iteration']:
                ax.plot(self.training_history['iteration'], 
                       self.training_history['train_loss'], 
                       label='è®­ç»ƒæŸå¤±', linewidth=2, color='blue')
                ax.plot(self.training_history['iteration'], 
                       self.training_history['val_loss'], 
                       label='éªŒè¯æŸå¤±', linewidth=2, color='red')
                
                ax.set_xlabel('è®­ç»ƒè½®æ•°')
                ax.set_ylabel('æŸå¤±å€¼')
                ax.set_title('LightGBM å­¦ä¹ æ›²çº¿')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # ä¿å­˜å›¾è¡¨
                save_path = self.output_dir / 'learning_curves.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                if self.logger:
                    self.logger.info(f"   ğŸ“ˆ å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {save_path}")
                
                return str(save_path)
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"   âš ï¸ å­¦ä¹ æ›²çº¿ç»˜åˆ¶å¤±è´¥: {e}")
            return None
    
    def plot_feature_importance(self, model, feature_names, top_n=20):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
        try:
            import matplotlib.pyplot as plt
            
            # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®è®¾ç½®
            if 'Microsoft YaHei' not in plt.rcParams['font.sans-serif']:
                plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei'] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = model.feature_importance(importance_type='gain')
            feature_importance = list(zip(feature_names, importance))
            feature_importance.sort(key=lambda x: x[1], reverse=True)
            
            # å–å‰Nä¸ªé‡è¦ç‰¹å¾
            top_features = feature_importance[:top_n]
            features, scores = zip(*top_features)
            
            # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
            fig, ax = plt.subplots(figsize=(10, max(6, len(features) * 0.3)))
            
            y_pos = range(len(features))
            bars = ax.barh(y_pos, scores, color='skyblue', alpha=0.8)
            
            ax.set_yticks(y_pos)
            ax.set_yticklabels(features)
            ax.invert_yaxis()  # é‡è¦æ€§é«˜çš„åœ¨ä¸Šé¢
            ax.set_xlabel('é‡è¦æ€§å¾—åˆ†')
            ax.set_title(f'å‰ {top_n} ä¸ªæœ€é‡è¦ç‰¹å¾')
            ax.grid(True, alpha=0.3, axis='x')
            
            # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                width = bar.get_width()
                ax.text(width, bar.get_y() + bar.get_height()/2, 
                       f'{width:.0f}', ha='left', va='center', fontsize=8)
            
            # ä¿å­˜å›¾è¡¨
            save_path = self.output_dir / 'feature_importance.png'
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            if self.logger:
                self.logger.info(f"   ğŸ“Š ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {save_path}")
            
            return str(save_path)
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"   âš ï¸ ç‰¹å¾é‡è¦æ€§å›¾ç»˜åˆ¶å¤±è´¥: {e}")
            return None
    
    def plot_predictions_scatter(self, y_true, y_pred, split_name='æµ‹è¯•é›†'):
        """ç»˜åˆ¶é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®è®¾ç½®
            if 'Microsoft YaHei' not in plt.rcParams['font.sans-serif']:
                plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei'] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
            
            fig, ax = plt.subplots(figsize=(8, 8))
            
            # ç»˜åˆ¶æ•£ç‚¹å›¾
            ax.scatter(y_true, y_pred, alpha=0.6, s=10, color='blue')
            
            # ç»˜åˆ¶ç†æƒ³é¢„æµ‹çº¿ (y=x)
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='ç†æƒ³é¢„æµ‹')
            
            ax.set_xlabel('çœŸå®å€¼')
            ax.set_ylabel('é¢„æµ‹å€¼')
            ax.set_title(f'{split_name} - é¢„æµ‹å€¼ vs çœŸå®å€¼')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ ç›¸å…³ç³»æ•°
            correlation = np.corrcoef(y_true, y_pred)[0, 1]
            ax.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.4f}', 
                   transform=ax.transAxes, fontsize=12, 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
            
            # ä¿å­˜å›¾è¡¨
            save_path = self.output_dir / f'predictions_scatter_{split_name}.png'
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            if self.logger:
                self.logger.info(f"   ğŸ“Š é¢„æµ‹æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
            
            return str(save_path)
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"   âš ï¸ é¢„æµ‹æ•£ç‚¹å›¾ç»˜åˆ¶å¤±è´¥: {e}")
            return None
    
    def plot_residuals(self, y_true, y_pred, split_name='æµ‹è¯•é›†'):
        """ç»˜åˆ¶æ®‹å·®å›¾"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # ç¡®ä¿ä¸­æ–‡å­—ä½“æ­£ç¡®è®¾ç½®
            if 'Microsoft YaHei' not in plt.rcParams['font.sans-serif']:
                plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei'] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
            
            residuals = y_true - y_pred
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # æ®‹å·®vsé¢„æµ‹å€¼å›¾
            ax1.scatter(y_pred, residuals, alpha=0.6, s=10, color='green')
            ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)
            ax1.set_xlabel('é¢„æµ‹å€¼')
            ax1.set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)')
            ax1.set_title(f'{split_name} - æ®‹å·®åˆ†æ')
            ax1.grid(True, alpha=0.3)
            
            # æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾
            ax2.hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')
            ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)
            ax2.set_xlabel('æ®‹å·®å€¼')
            ax2.set_ylabel('é¢‘æ¬¡')
            ax2.set_title(f'{split_name} - æ®‹å·®åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            mean_residual = np.mean(residuals)
            std_residual = np.std(residuals)
            ax2.text(0.05, 0.95, f'å‡å€¼: {mean_residual:.4f}\næ ‡å‡†å·®: {std_residual:.4f}', 
                    transform=ax2.transAxes, fontsize=10,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
            
            # ä¿å­˜å›¾è¡¨
            save_path = self.output_dir / f'residuals_analysis_{split_name}.png'
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            if self.logger:
                self.logger.info(f"   ğŸ“Š æ®‹å·®åˆ†æå›¾å·²ä¿å­˜: {save_path}")
            
            return str(save_path)
            
        except Exception as e:
            if self.logger:
                self.logger.warning(f"   âš ï¸ æ®‹å·®åˆ†æå›¾ç»˜åˆ¶å¤±è´¥: {e}")
            return None
    
    def generate_all_visualizations(self, model, y_train, y_val, 
                                   y_train_pred, y_val_pred):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ - åªä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
        results = {}
        
        # å­¦ä¹ æ›²çº¿
        learning_curve_path = self.plot_learning_curves(model)
        if learning_curve_path:
            results['learning_curves'] = learning_curve_path
        
        # ç‰¹å¾é‡è¦æ€§
        if hasattr(model, 'feature_importance'):
            # å°è¯•è·å–ç‰¹å¾åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆé»˜è®¤åç§°
            if hasattr(self, 'feature_names') and self.feature_names:
                feature_names = self.feature_names
            else:
                feature_names = [f'feature_{i}' for i in range(model.num_feature())]
            
            feature_importance_path = self.plot_feature_importance(model, feature_names)
            if feature_importance_path:
                results['feature_importance'] = feature_importance_path
        
        # é¢„æµ‹æ•£ç‚¹å›¾ - åªç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
        for split_name, y_true, y_pred in [
            ('è®­ç»ƒé›†', y_train, y_train_pred),
            ('éªŒè¯é›†', y_val, y_val_pred)
        ]:
            scatter_path = self.plot_predictions_scatter(y_true, y_pred, split_name)
            if scatter_path:
                results[f'predictions_scatter_{split_name}'] = scatter_path
            
            residuals_path = self.plot_residuals(y_true, y_pred, split_name)
            if residuals_path:
                results[f'residuals_{split_name}'] = residuals_path
        
        return results


class StockLightGBMTrainer:
    """è‚¡ç¥¨æ•°æ®ä¸“ç”¨çš„LightGBMè®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹
        self.training_folder = self._create_training_folder()
        
        self.setup_logging()
        
        # åˆå§‹åŒ–å˜é‡
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.feature_names = None
        self.scaler = None
        self.model = None
        self.stock_data_processor = None
        self.visualizer = None  # å¯è§†åŒ–å™¨
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_history = {}
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def _create_training_folder(self) -> str:
        """åˆ›å»ºå½“å‰è®­ç»ƒçš„å”¯ä¸€æ–‡ä»¶å¤¹"""
        file_naming = self.config.get('output', {}).get('file_naming', {})
        identifier_type = file_naming.get('identifier_type', 'unique_id')
        folder_prefix = file_naming.get('folder_name_prefix', 'stock_training')
        
        if identifier_type == 'timestamp':
            identifier = datetime.now().strftime("%Y%m%d_%H%M%S")
        else:  # unique_id
            digits = file_naming.get('unique_id_digits', 3)
            # æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹ï¼Œç¡®å®šä¸‹ä¸€ä¸ªID
            model_dir = Path(self.config.get('output', {}).get('model_save', {}).get('save_dir', './models/lightgbm_stock'))
            existing_folders = []
            if model_dir.exists():
                existing_folders = [d.name for d in model_dir.iterdir() 
                                  if d.is_dir() and d.name.startswith(folder_prefix)]
            
            # æå–å·²å­˜åœ¨çš„IDå·
            max_id = 0
            for folder in existing_folders:
                try:
                    id_part = folder.replace(folder_prefix + '_', '')
                    if id_part.isdigit():
                        max_id = max(max_id, int(id_part))
                except:
                    continue
            
            identifier = str(max_id + 1).zfill(digits)
        
        training_folder = f"{folder_prefix}_{identifier}"
        return training_folder
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('output', {}).get('logging', {})
        log_level = log_config.get('log_level', 'INFO')
        log_file = log_config.get('log_file', './logs/lightgbm_stock_training.log')
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler() if log_config.get('console_output', True) else logging.NullHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸš€ å¼€å§‹è‚¡ç¥¨LightGBMè®­ç»ƒ: {self.training_folder}")
    
    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # æ¨¡å‹ä¿å­˜ç›®å½•
        model_save_config = self.config.get('output', {}).get('model_save', {})
        self.model_save_dir = Path(model_save_config.get('save_dir', './models/lightgbm_stock')) / self.training_folder
        self.model_save_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»“æœä¿å­˜ç›®å½•
        results_save_config = self.config.get('output', {}).get('results_save', {})
        self.results_save_dir = Path(results_save_config.get('save_dir', './results/lightgbm_stock')) / self.training_folder
        self.results_save_dir.mkdir(parents=True, exist_ok=True)
    
    def _create_builtin_visualizer(self):
        """åˆ›å»ºå†…ç½®å¯è§†åŒ–å™¨"""
        return BuiltinVisualizer(self.results_save_dir, self.logger)
    
    def preprocess_stock_data(self) -> bool:
        """é¢„å¤„ç†è‚¡ç¥¨æ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            if direct_training.get('enabled', False):
                self.logger.info("ğŸ¯ ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼ï¼Œè·³è¿‡æ•°æ®é¢„å¤„ç†")
                return True
            
            self.logger.info("ğŸ“Š å¼€å§‹è‚¡ç¥¨æ•°æ®é¢„å¤„ç†...")
            
            # è·å–æ•°æ®é…ç½®
            source_data_config = data_config.get('source_data', {})
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨å¤„ç†æ•°æ®
            if source_data_config.get('auto_process', True):
                parquet_dir = source_data_config.get('parquet_dir', './data/professional_parquet')
                output_dir = data_config.get('data_dir', './data/processed_stock_data')
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†å¥½çš„æ•°æ®
                processed_dir = Path(output_dir)
                if processed_dir.exists() and any(processed_dir.glob('processed_*')):
                    self.logger.info("âœ… å‘ç°å·²å¤„ç†çš„æ•°æ®ï¼Œè·³è¿‡é¢„å¤„ç†æ­¥éª¤")
                    return True
                
                # åˆ›å»ºè‚¡ç¥¨æ•°æ®å¤„ç†å™¨
                self.stock_data_processor = StockDataProcessor(
                    data_dir=parquet_dir,
                    output_dir=output_dir
                )
                
                # è·å–è‚¡ç¥¨ç‰¹å®šé…ç½®
                stock_config = data_config.get('stock_specific', {})
                time_series_config = stock_config.get('time_series', {})
                
                # è¿è¡Œæ•°æ®å¤„ç†æµç¨‹
                processed_path = self.stock_data_processor.run_full_pipeline(
                    lookback_days=time_series_config.get('lookback_days', 5),
                    target_days=time_series_config.get('target_days', 1)
                )
                
                if processed_path:
                    # æ›´æ–°é…ç½®ä¸­çš„æ•°æ®è·¯å¾„
                    self.config['data']['data_dir'] = processed_path
                    self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {processed_path}")
                    return True
                else:
                    self.logger.error("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
                    return False
            else:
                self.logger.info("â­ï¸ è·³è¿‡è‡ªåŠ¨æ•°æ®é¢„å¤„ç†")
                return True
                
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def load_data(self) -> bool:
        """åŠ è½½æ•°æ®"""
        try:
            self.logger.info("ğŸ“‚ åŠ è½½è‚¡ç¥¨è®­ç»ƒæ•°æ®...")
            
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼
            if direct_training.get('enabled', False):
                return self._load_direct_data()
            
            # åŸæœ‰çš„åŠ è½½é€»è¾‘
            data_dir = Path(data_config.get('data_dir', './data/processed_stock_data'))
            
            # å¦‚æœdata_diræ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„processed_*æ–‡ä»¶å¤¹
            if data_dir.is_dir() and not (data_dir / data_config.get('X_features_file', 'X_features.csv')).exists():
                processed_folders = list(data_dir.glob('processed_*'))
                if processed_folders:
                    # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶å¤¹
                    data_dir = max(processed_folders, key=lambda x: x.stat().st_mtime)
                    self.logger.info(f"   ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶å¤¹: {data_dir}")
            
            # æ–‡ä»¶è·¯å¾„
            X_features_file = data_dir / data_config.get('X_features_file', 'X_features.csv')
            y_targets_file = data_dir / data_config.get('y_targets_file', 'y_targets.csv')
            full_data_file = data_dir / data_config.get('full_data_file', 'full_data.csv')
            
            loading_options = data_config.get('loading_options', {})
            prefer_full_data = loading_options.get('prefer_full_data', True)
            encoding = loading_options.get('encoding', 'utf-8')
            
            # åŠ è½½æ•°æ®
            if prefer_full_data and full_data_file.exists():
                self.logger.info("   åŠ è½½å®Œæ•´æ•°æ®æ–‡ä»¶...")
                full_data = pd.read_csv(full_data_file, encoding=encoding)
                
                # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
                exclude_cols = ['stock_code', 'date', 'target']
                feature_cols = [col for col in full_data.columns if col not in exclude_cols]
                
                self.X = full_data[feature_cols]
                self.y = full_data['target']
                
                # ä¿å­˜è‚¡ç¥¨ç›¸å…³ä¿¡æ¯ç”¨äºé¢„æµ‹ç»“æœ
                info_cols = []
                if 'stock_code' in full_data.columns:
                    info_cols.append('stock_code')
                if 'date' in full_data.columns:
                    info_cols.append('date')
                
                # å°è¯•æŸ¥æ‰¾æ›´å¤šå¯èƒ½çš„åˆ—
                for col in full_data.columns:
                    if 'è‚¡ç¥¨åç§°' in col or 'stock_name' in col or 'åç§°' in col:
                        info_cols.append(col)
                    elif 'æ¬¡æ—¥æ¶¨è·Œå¹…' in col or 'next_day_return' in col:
                        info_cols.append(col)
                
                self.stock_info = full_data[info_cols].copy() if info_cols else None
                if self.stock_info is not None:
                    self.logger.info(f"   âœ… ä¿å­˜è‚¡ç¥¨ä¿¡æ¯åˆ—: {list(self.stock_info.columns)}")
                
            else:
                self.logger.info("   åˆ†åˆ«åŠ è½½ç‰¹å¾å’Œç›®æ ‡æ–‡ä»¶...")
                self.X = pd.read_csv(X_features_file, encoding=encoding)
                self.y = pd.read_csv(y_targets_file, encoding=encoding)
                if hasattr(self.y, 'iloc'):
                    self.y = self.y.iloc[:, 0]  # å–ç¬¬ä¸€åˆ—ä½œä¸ºç›®æ ‡
                self.stock_info = None
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = list(self.X.columns)
            
            self.logger.info(f"   âœ… æ•°æ®åŠ è½½å®Œæˆ:")
            self.logger.info(f"     - ç‰¹å¾ç»´åº¦: {self.X.shape}")
            self.logger.info(f"     - ç›®æ ‡ç»´åº¦: {self.y.shape}")
            self.logger.info(f"     - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _load_direct_data(self) -> bool:
        """ç›´æ¥åŠ è½½parquetæ ¼å¼çš„è‚¡ç¥¨æ•°æ® - ç®€åŒ–ç‰ˆ"""
        try:
            self.logger.info("ğŸ“Š ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼åŠ è½½æ•°æ®...")
            
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            data_dir = Path(data_config.get('data_dir', './data/professional_parquet'))
            data_format = direct_training.get('data_format', 'parquet')
            target_column = direct_training.get('target_column', 'æ¶¨è·Œå¹…')
            exclude_columns = direct_training.get('exclude_columns', ['name', 'æ¶¨è·Œå¹…'])
            
            # åŠ è½½æ•°æ®æ–‡ä»¶
            if data_format == 'parquet':
                # æŸ¥æ‰¾parquetæ–‡ä»¶
                parquet_files = list(data_dir.glob("*.parquet"))
                if not parquet_files:
                    self.logger.error(f"âŒ åœ¨{data_dir}ä¸­æœªæ‰¾åˆ°parquetæ–‡ä»¶")
                    return False
                
                self.logger.info(f"   å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
                
                # ä½¿ç”¨ç®€å•é«˜æ•ˆçš„æ–‡ä»¶é…å¯¹æ–¹æ¡ˆ
                parquet_files = sorted(parquet_files)  # æŒ‰æ—¥æœŸæ’åº
                self.logger.info("   ğŸ“… ä½¿ç”¨æ–‡ä»¶é…å¯¹æ–¹æ¡ˆï¼šä»Šå¤©æ–‡ä»¶ â†’ æ˜å¤©ç›®æ ‡")
                
                features_list = []
                targets_list = []
                file_dates_list = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ—¥æœŸä¿¡æ¯
                processed_pairs = 0
                
                # ç›¸é‚»æ–‡ä»¶é…å¯¹
                for i in range(len(parquet_files) - 1):
                    today_file = parquet_files[i]      # ä»Šå¤©çš„ç‰¹å¾
                    tomorrow_file = parquet_files[i+1]  # æ˜å¤©çš„ç›®æ ‡
                    
                    try:
                        # ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸä¿¡æ¯
                        today_date = today_file.stem  # ç§»é™¤.parquetæ‰©å±•å
                        tomorrow_date = tomorrow_file.stem
                        
                        # è¯»å–ä»Šå¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
                        today_data = pd.read_parquet(today_file)
                        # è¯»å–æ˜å¤©çš„æ•°æ®æå–ç›®æ ‡
                        tomorrow_data = pd.read_parquet(tomorrow_file)
                        
                        # æŒ‰è‚¡ç¥¨ä»£ç åŒ¹é…ï¼ˆå–äº¤é›†ï¼‰
                        common_stocks = today_data.index.intersection(tomorrow_data.index)
                        
                        if len(common_stocks) > 0:
                            # ä»Šå¤©çš„æ‰€æœ‰ä¿¡æ¯ä½œä¸ºç‰¹å¾
                            features_list.append(today_data.loc[common_stocks])
                            # æ˜å¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡
                            targets_list.append(tomorrow_data.loc[common_stocks, target_column])
                            
                            # è®°å½•æ¯ä¸ªè‚¡ç¥¨æ ·æœ¬çš„æ—¥æœŸä¿¡æ¯
                            date_info = pd.DataFrame({
                                'feature_date': [today_date] * len(common_stocks),
                                'target_date': [tomorrow_date] * len(common_stocks),
                                'stock_code': common_stocks.tolist()
                            }, index=common_stocks)  # ä½¿ç”¨è‚¡ç¥¨ä»£ç ä½œä¸ºç´¢å¼•
                            file_dates_list.append(date_info)
                            
                            processed_pairs += 1
                            
                        self.logger.info(f"   âœ… é…å¯¹: {today_file.name} â†’ {tomorrow_file.name}, è‚¡ç¥¨: {len(common_stocks)}")
                        
                    except Exception as e:
                        self.logger.warning(f"   è·³è¿‡é…å¯¹ {today_file.name} â†’ {tomorrow_file.name}: {e}")
                        continue
                
                if not features_list:
                    self.logger.error("âŒ æ²¡æœ‰æˆåŠŸé…å¯¹ä»»ä½•æ–‡ä»¶")
                    return False
                
                # åˆå¹¶æ‰€æœ‰é…å¯¹çš„æ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
                self.logger.info(f"   ğŸ”„ åˆå¹¶ {processed_pairs} ä¸ªæ–‡ä»¶é…å¯¹çš„æ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...")
                
                # å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹åˆå¹¶ä»¥å‡å°‘å³°å€¼å†…å­˜ä½¿ç”¨
                import gc
                self.logger.info("   ğŸ§¹ æ¸…ç†å†…å­˜...")
                gc.collect()
                
                # åˆå¹¶ç‰¹å¾æ•°æ®
                self.logger.info("   ğŸ“Š åˆå¹¶ç‰¹å¾æ•°æ®...")
                full_data = pd.concat(features_list, ignore_index=False, copy=False)
                del features_list  # ç«‹å³é‡Šæ”¾å†…å­˜
                gc.collect()
                
                # åˆå¹¶ç›®æ ‡æ•°æ®
                self.logger.info("   ğŸ¯ åˆå¹¶ç›®æ ‡æ•°æ®...")
                targets_data = pd.concat(targets_list, ignore_index=False, copy=False)
                del targets_list  # ç«‹å³é‡Šæ”¾å†…å­˜
                gc.collect()
                
                # åˆå¹¶æ—¥æœŸä¿¡æ¯
                if file_dates_list:
                    self.logger.info("   ğŸ“… åˆå¹¶æ—¥æœŸä¿¡æ¯...")
                    all_dates_info = pd.concat(file_dates_list, ignore_index=False, copy=False)
                    del file_dates_list  # ç«‹å³é‡Šæ”¾å†…å­˜
                    gc.collect()
                    self.logger.info(f"   âœ… åˆå¹¶æ—¥æœŸä¿¡æ¯: {len(all_dates_info)} æ¡è®°å½•")
                    self.logger.info(f"   ğŸ“‹ æ—¥æœŸä¿¡æ¯ç´¢å¼•ç¤ºä¾‹: {list(all_dates_info.index[:5])}")
                else:
                    all_dates_info = None
                
                # æ·»åŠ ç›®æ ‡åˆ—
                self.logger.info("   ğŸ¯ æ·»åŠ ç›®æ ‡åˆ—...")
                full_data['next_day_target'] = targets_data
                del targets_data  # ç«‹å³é‡Šæ”¾å†…å­˜
                gc.collect()
                
                self.logger.info(f"   âœ… æ–‡ä»¶é…å¯¹å®Œæˆ:")
                self.logger.info(f"   - å¤„ç†æ–‡ä»¶å¯¹: {processed_pairs}")
                self.logger.info(f"   - æœ€ç»ˆæ ·æœ¬æ•°: {len(full_data):,}")
                self.logger.info(f"   - ç‰¹å¾åˆ—æ•°: {len(full_data.columns)}")
                self.logger.info(f"   ğŸ“‹ full_dataç´¢å¼•ç¤ºä¾‹: {list(full_data.index[:5])}")
                
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_format}")
                return False
            
            # æ£€æŸ¥æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
            if 'next_day_target' not in full_data.columns:
                self.logger.error(f"âŒ æœªæ‰¾åˆ°æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ— 'next_day_target'")
                return False
            
            # æ£€æŸ¥é¢„æµ‹æ¨¡å¼
            prediction_mode = direct_training.get('prediction_mode', 'regression')
            self.prediction_mode = prediction_mode  # ä¿å­˜é¢„æµ‹æ¨¡å¼
            
            # è®¾ç½®ç›®æ ‡å˜é‡
            raw_targets = full_data['next_day_target']
            
            if prediction_mode == 'direction':
                # ğŸ¯ æ–¹å‘é¢„æµ‹æ¨¡å¼ï¼šæ¶¨è·Œå¹… > 0 ä¸ºçœ‹å¤š(1)ï¼Œ<= 0 ä¸ºçœ‹ç©º(0)
                self.y = (raw_targets > 0).astype(int)
                actual_target_column = 'next_day_direction'
                self.logger.info(f"   ğŸ¯ é¢„æµ‹æ¨¡å¼: æ¶¨è·Œæ–¹å‘é¢„æµ‹ï¼ˆäºŒåˆ†ç±»ï¼‰")
                self.logger.info(f"   ğŸ“Š çœ‹å¤šæ ·æœ¬: {(self.y == 1).sum():,} ({(self.y == 1).mean()*100:.1f}%)")
                self.logger.info(f"   ğŸ“Š çœ‹ç©ºæ ·æœ¬: {(self.y == 0).sum():,} ({(self.y == 0).mean()*100:.1f}%)")
            else:
                # ğŸ“ˆ å›å½’é¢„æµ‹æ¨¡å¼ï¼šé¢„æµ‹å…·ä½“æ¶¨è·Œå¹…
                self.y = raw_targets
                actual_target_column = 'next_day_target'
                self.logger.info(f"   ğŸ“ˆ é¢„æµ‹æ¨¡å¼: æ¶¨è·Œå¹…é¢„æµ‹ï¼ˆå›å½’ï¼‰")
                self.logger.info(f"   ğŸ“Š ç›®æ ‡å€¼èŒƒå›´: [{self.y.min():.4f}, {self.y.max():.4f}]")            
            # æ’é™¤ç›®æ ‡åˆ—å’Œè¾…åŠ©åˆ—ï¼Œä¿ç•™ä»Šå¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç‰¹å¾
            exclude_columns = exclude_columns + ['next_day_target']
            self.logger.info(f"   ğŸ’¡ ä»Šå¤©çš„'{target_column}'ç”¨ä½œé¢„æµ‹æ˜å¤©æ¶¨è·Œå¹…çš„ç‰¹å¾")
            
            # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤æŒ‡å®šçš„åˆ—ï¼‰
            feature_columns = [col for col in full_data.columns if col not in exclude_columns]
            self.X = full_data[feature_columns]
            
            # åªä¿ç•™æ•°å€¼åˆ—ä½œä¸ºç‰¹å¾ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            self.logger.info("   ğŸ”§ ç­›é€‰æ•°å€¼åˆ—ï¼ˆå†…å­˜ä¼˜åŒ–å¤„ç†ï¼‰...")
            numeric_columns = []
            for col in self.X.columns:
                if pd.api.types.is_numeric_dtype(self.X[col]):
                    numeric_columns.append(col)
            
            # å¦‚æœéœ€è¦ç­›é€‰åˆ—ï¼Œä½¿ç”¨å†…å­˜å‹å¥½çš„æ–¹å¼
            if len(numeric_columns) < len(self.X.columns):
                # é€åˆ—ç­›é€‰ï¼Œé¿å…å¤§å†…å­˜åˆ†é…
                self.X = self.X[numeric_columns].copy()
            
            numeric_columns = list(self.X.columns)  # æ›´æ–°åˆ—åˆ—è¡¨
            
            self.logger.info(f"   ğŸ“‹ æ’é™¤çš„åˆ—: {exclude_columns}")
            self.logger.info(f"   ğŸ“Š æ•°å€¼ç‰¹å¾åˆ—æ•°: {len(numeric_columns)}")
            
            # å¤„ç†ç¼ºå¤±å€¼ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
            self.logger.info("   ğŸ”§ å¤„ç†ç¼ºå¤±å€¼ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...")
            self.X.fillna(0, inplace=True)  # ä½¿ç”¨inplaceé¿å…åˆ›å»ºå‰¯æœ¬
            self.y.fillna(0, inplace=True)
            
            # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢ä¸ºfloat32é™ä½å†…å­˜ä½¿ç”¨
            self.logger.info("   ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹ï¼ˆfloat64 â†’ float32ï¼‰...")
            for col in self.X.columns:
                if self.X[col].dtype == 'float64':
                    self.X[col] = self.X[col].astype('float32')
            
            if self.y.dtype == 'float64':
                self.y = self.y.astype('float32')
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = list(self.X.columns)
            
            # ä¸ºç›´æ¥è®­ç»ƒæ¨¡å¼ä¿å­˜è‚¡ç¥¨ä¿¡æ¯
            # æ„å»ºåŒ…å«è‚¡ç¥¨ä»£ç ã€æ—¥æœŸå’Œæ”¶ç›Šç‡çš„å®Œæ•´ä¿¡æ¯
            stock_info_data = {
                'stock_code': full_data.index.tolist(),
                'next_day_return': raw_targets.tolist()  # ä¿å­˜æ¬¡æ—¥æ¶¨è·Œå¹…
            }
            
            # æ·»åŠ æ—¥æœŸä¿¡æ¯ï¼ˆä»æ–‡ä»¶åä¸­æå–ï¼‰
            if all_dates_info is not None:
                # ç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•åŒ¹é…ï¼Œå› ä¸ºall_dates_infoå’Œfull_dataæŒ‰ç›¸åŒé¡ºåºåˆå¹¶
                try:
                    self.logger.info(f"   ğŸ” full_dataè¡Œæ•°: {len(full_data)}")
                    self.logger.info(f"   ğŸ” all_dates_infoè¡Œæ•°: {len(all_dates_info)}")
                    
                    if len(full_data) == len(all_dates_info):
                        # è¡Œæ•°åŒ¹é…ï¼Œç›´æ¥æŒ‰ä½ç½®å¯¹åº”
                        all_dates_reset = all_dates_info.reset_index(drop=True)
                        stock_info_data['feature_date'] = all_dates_reset['feature_date'].astype(str).tolist()
                        stock_info_data['target_date'] = all_dates_reset['target_date'].astype(str).tolist()
                        
                        # ç»Ÿè®¡æ—¥æœŸåˆ†å¸ƒ
                        unique_feature_dates = set(stock_info_data['feature_date'])
                        unique_target_dates = set(stock_info_data['target_date'])
                        
                        self.logger.info(f"   âœ… æ·»åŠ æ—¥æœŸä¿¡æ¯: feature_date, target_date")
                        self.logger.info(f"   ğŸ“… å”¯ä¸€ç‰¹å¾æ—¥æœŸæ•°: {len(unique_feature_dates)}")
                        self.logger.info(f"   ğŸ“… å”¯ä¸€ç›®æ ‡æ—¥æœŸæ•°: {len(unique_target_dates)}")
                        self.logger.info(f"   ğŸ“… ç‰¹å¾æ—¥æœŸç¤ºä¾‹: {list(unique_feature_dates)[:5]}")
                        self.logger.info(f"   ğŸ“… ç›®æ ‡æ—¥æœŸç¤ºä¾‹: {list(unique_target_dates)[:5]}")
                        
                    else:
                        self.logger.warning(f"   âš ï¸  æ•°æ®è¡Œæ•°ä¸åŒ¹é…: full_data={len(full_data)}, all_dates_info={len(all_dates_info)}")
                        # æ·»åŠ é»˜è®¤å€¼
                        stock_info_data['feature_date'] = ['unknown'] * len(full_data)
                        stock_info_data['target_date'] = ['unknown'] * len(full_data)
                    
                except Exception as e:
                    self.logger.warning(f"   âš ï¸  æ—¥æœŸä¿¡æ¯åˆå¹¶å¤±è´¥: {e}")
                    import traceback
                    self.logger.warning(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    # æ·»åŠ é»˜è®¤å€¼
                    stock_info_data['feature_date'] = ['unknown'] * len(full_data)
                    stock_info_data['target_date'] = ['unknown'] * len(full_data)
            else:
                self.logger.warning("   âš ï¸  æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸä¿¡æ¯")
                # æ·»åŠ é»˜è®¤å€¼
                stock_info_data['feature_date'] = ['unknown'] * len(full_data)
                stock_info_data['target_date'] = ['unknown'] * len(full_data)
            
            # å¦‚æœåŸå§‹æ•°æ®ä¸­æœ‰å…¶ä»–ä¿¡æ¯åˆ—ï¼ˆå¦‚è‚¡ç¥¨åç§°ï¼‰
            if 'name' in full_data.columns:
                stock_info_data['stock_name'] = full_data['name'].tolist()
                self.logger.info(f"   âœ… æ·»åŠ è‚¡ç¥¨åç§°ä¿¡æ¯")
            
            # æ„å»ºæœ€ç»ˆçš„stock_info
            self.stock_info = pd.DataFrame(stock_info_data)
            self.logger.info(f"   âœ… å®Œæ•´è‚¡ç¥¨ä¿¡æ¯å·²ä¿å­˜: {list(self.stock_info.columns)}")
            self.logger.info(f"   ğŸ“Š è‚¡ç¥¨ä¿¡æ¯ç»´åº¦: {self.stock_info.shape}")
            
            # æœ€ç»ˆå†…å­˜æ¸…ç†
            import gc
            gc.collect()
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨æƒ…å†µ
            try:
                import psutil
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                self.logger.info(f"   ğŸ’¾ å½“å‰å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
            except ImportError:
                self.logger.info("   ğŸ’¾ å†…å­˜ä¼˜åŒ–å·²å®Œæˆ")
            
            self.logger.info(f"   âœ… æ¬¡æ—¥é¢„æµ‹æ•°æ®åŠ è½½å®Œæˆ:")
            self.logger.info(f"     - ç‰¹å¾ç»´åº¦: {self.X.shape}")
            self.logger.info(f"     - ç›®æ ‡ç»´åº¦: {self.y.shape}")
            self.logger.info(f"     - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            self.logger.info(f"     - ç›®æ ‡åˆ—: {actual_target_column}")
            self.logger.info(f"     - é¢„æµ‹ä»»åŠ¡: ä»Šå¤©ç‰¹å¾ â†’ æ˜å¤©æ¶¨è·Œå¹…")
            self.logger.info(f"     - ç›®æ ‡å€¼èŒƒå›´: [{self.y.min():.4f}, {self.y.max():.4f}]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False


    def split_data(self) -> bool:
        """åˆ†å‰²æ•°æ®"""
        try:
            self.logger.info("âœ‚ï¸ åˆ†å‰²è®­ç»ƒ/éªŒè¯æ•°æ®...")
            
            split_config = self.config.get('training', {}).get('data_split', {})
            validation_size = split_config.get('validation_size', 0.2)  # å¢åŠ éªŒè¯é›†æ¯”ä¾‹
            random_state = split_config.get('random_state', 42)
            time_series_split = split_config.get('time_series_split', True)
            
            if time_series_split:
                # æ—¶åºåˆ†å‰²ï¼ˆè‚¡ç¥¨æ•°æ®çš„æ¨èæ–¹å¼ï¼‰
                n_samples = len(self.X)
                val_start = int(n_samples * (1 - validation_size))
                
                # åˆ†å‰²æ•°æ® - åªåˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
                self.X_train = self.X.iloc[:val_start]
                self.X_val = self.X.iloc[val_start:]
                
                self.y_train = self.y.iloc[:val_start]
                self.y_val = self.y.iloc[val_start:]
                
                # ä¿å­˜å¯¹åº”çš„è‚¡ç¥¨ä¿¡æ¯ç´¢å¼•
                if self.stock_info is not None:
                    self.stock_info_train = self.stock_info.iloc[:val_start]
                    self.stock_info_val = self.stock_info.iloc[val_start:]
                else:
                    self.stock_info_train = self.stock_info_val = None
                
                self.logger.info("   ä½¿ç”¨æ—¶åºåˆ†å‰²æ–¹å¼")
                
            else:
                # éšæœºåˆ†å‰²
                if self.stock_info is not None:
                    self.X_train, self.X_val, self.y_train, self.y_val, self.stock_info_train, self.stock_info_val = train_test_split(
                        self.X, self.y, self.stock_info, test_size=validation_size, random_state=random_state
                    )
                else:
                    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
                        self.X, self.y, test_size=validation_size, random_state=random_state
                    )
                    
                    self.stock_info_train = self.stock_info_val = None
                
                self.logger.info("   ä½¿ç”¨éšæœºåˆ†å‰²æ–¹å¼")
            
            # è®¾ç½®æµ‹è¯•é›†ä¸ºNoneï¼ˆä¸ä½¿ç”¨ï¼‰
            self.X_test = None
            self.y_test = None
            self.stock_info_test = None
            
            self.logger.info(f"   âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
            self.logger.info(f"     - è®­ç»ƒé›†: {self.X_train.shape[0]} æ ·æœ¬")
            self.logger.info(f"     - éªŒè¯é›†: {self.X_val.shape[0]} æ ·æœ¬")
            self.logger.info(f"     - æµ‹è¯•é›†: å·²ç¦ç”¨")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åˆ†å‰²å¤±è´¥: {e}")
            return False
    
    def preprocess_features(self) -> bool:
        """ç‰¹å¾é¢„å¤„ç†"""
        try:
            self.logger.info("ğŸ”§ ç‰¹å¾é¢„å¤„ç†...")
            
            preprocessing_config = self.config.get('data', {}).get('preprocessing', {})
            normalization_config = preprocessing_config.get('normalization', {})
            outlier_config = preprocessing_config.get('outlier_handling', {})
            
            # æ•°æ®æ ‡å‡†åŒ–
            method = normalization_config.get('method', 'robust')
            if method:
                if method == 'standard':
                    self.scaler = StandardScaler()
                elif method == 'minmax':
                    self.scaler = MinMaxScaler()
                elif method == 'robust':
                    self.scaler = RobustScaler()
                else:
                    self.logger.warning(f"æœªçŸ¥çš„æ ‡å‡†åŒ–æ–¹æ³•: {method}ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                    self.scaler = None
                
                if self.scaler:
                    self.X_train = pd.DataFrame(
                        self.scaler.fit_transform(self.X_train),
                        columns=self.X_train.columns,
                        index=self.X_train.index
                    )
                    self.X_val = pd.DataFrame(
                        self.scaler.transform(self.X_val),
                        columns=self.X_val.columns,
                        index=self.X_val.index
                    )
                    
                    self.logger.info(f"   âœ… ä½¿ç”¨ {method} æ ‡å‡†åŒ–")
            
            # å¼‚å¸¸å€¼å¤„ç†
            if outlier_config.get('enabled', False):
                method = outlier_config.get('method', 'winsorize')
                if method == 'winsorize':
                    limits = outlier_config.get('winsorize_limits', [0.01, 0.01])
                    from scipy.stats.mstats import winsorize
                    
                    # åªå¯¹è®­ç»ƒé›†è¿›è¡Œwinsorizeï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„é™åˆ¶
                    for col in self.X_train.columns:
                        self.X_train[col] = winsorize(self.X_train[col], limits=limits)
                    
                    self.logger.info(f"   âœ… ä½¿ç”¨ winsorize å¤„ç†å¼‚å¸¸å€¼ï¼Œé™åˆ¶: {limits}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾é¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def hyperparameter_tuning(self) -> bool:
        """è¶…å‚æ•°è°ƒä¼˜"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¶…å‚æ•°è°ƒä¼˜
            tuning_config = self.config.get('hyperparameter_tuning', {})
            if not tuning_config.get('enabled', False):
                self.logger.info("âš ï¸ è¶…å‚æ•°è°ƒä¼˜æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
                return True
            
            self.logger.info("ğŸ” å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
            
            # è·å–è°ƒä¼˜é…ç½®
            strategy = tuning_config.get('strategy', 'grid_search')
            max_trials = tuning_config.get('max_trials', 20)
            optimization_metric = tuning_config.get('optimization_metric', 'auc')
            optimization_direction = tuning_config.get('optimization_direction', 'maximize')
            param_space = tuning_config.get('param_space', {})
            
            # æ—©åœè®¾ç½®
            early_stopping_config = tuning_config.get('early_stopping', {})
            patience = early_stopping_config.get('patience', 50)
            min_improvement = early_stopping_config.get('min_improvement', 0.001)
            
            # ç”Ÿæˆå‚æ•°ç»„åˆ
            if strategy == 'grid_search':
                param_combinations = self._generate_grid_search_params(param_space)
                self.logger.info(f"   ğŸ“Š ç½‘æ ¼æœç´¢ï¼šæ€»å…± {len(param_combinations)} ç§å‚æ•°ç»„åˆ")
            elif strategy == 'random_search':
                param_combinations = self._generate_random_search_params(param_space, max_trials)
                self.logger.info(f"   ğŸ² éšæœºæœç´¢ï¼šæ€»å…± {len(param_combinations)} ç§å‚æ•°ç»„åˆ")
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„æœç´¢ç­–ç•¥: {strategy}")
                return False
            
            # å­˜å‚¨è°ƒä¼˜ç»“æœ
            tuning_results = []
            best_score = float('-inf') if optimization_direction == 'maximize' else float('inf')
            best_params = None
            best_model = None
            
            # è·å–åŸºç¡€æ¨¡å‹å‚æ•°
            lgb_config = self.config.get('lightgbm', {})
            base_params = {**lgb_config.get('basic_params', {}), **lgb_config.get('advanced_params', {})}
            fit_params = lgb_config.get('fit_params', {})
            
            # è®­ç»ƒå‚æ•°
            training_config = self.config.get('training', {}).get('training_params', {})
            verbose_eval = training_config.get('verbose', 100)
            
            # åˆ›å»ºæ•°æ®é›†
            train_data = lgb.Dataset(self.X_train, label=self.y_train)
            val_data = lgb.Dataset(self.X_val, label=self.y_val, reference=train_data)
            
            # å¼€å§‹è°ƒå‚å¾ªç¯
            self.logger.info(f"   ğŸš€ å¼€å§‹è°ƒå‚ï¼Œç›®æ ‡æŒ‡æ ‡: {optimization_metric} ({optimization_direction})")
            
            for trial_idx, trial_params in enumerate(param_combinations):
                try:
                    self.logger.info(f"   ğŸ”„ ç¬¬ {trial_idx + 1}/{len(param_combinations)} æ¬¡å°è¯•")
                    self.logger.info(f"     å‚æ•°: {trial_params}")
                    
                    # åˆå¹¶å‚æ•°
                    model_params = base_params.copy()
                    model_params.update(trial_params)
                    
                    # è®­ç»ƒæ¨¡å‹
                    with SuppressOutput():  # æŠ‘åˆ¶è®­ç»ƒè¾“å‡º
                        # ç¡®ä¿æ¨¡å‹å‚æ•°ä¸­æœ‰è¯„ä¼°æŒ‡æ ‡
                        if 'metric' not in model_params:
                            if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                                model_params['metric'] = 'binary_logloss'
                            else:
                                model_params['metric'] = 'rmse'
                        
                        trial_model = lgb.train(
                            model_params,
                            train_data,
                            valid_sets=[val_data],  # åªä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœ
                            valid_names=['val'],
                            num_boost_round=fit_params.get('num_boost_round', 1000),
                            callbacks=[
                                lgb.early_stopping(patience),
                                lgb.log_evaluation(0)  # ä¸è¾“å‡ºè®­ç»ƒæ—¥å¿—
                            ]
                        )
                    
                    # è¯„ä¼°æ¨¡å‹
                    trial_score = self._evaluate_single_trial(trial_model, optimization_metric)
                    
                    # è®°å½•ç»“æœ
                    trial_result = {
                        'trial': trial_idx + 1,
                        'params': trial_params.copy(),
                        'score': trial_score,
                        'metric': optimization_metric
                    }
                    tuning_results.append(trial_result)
                    
                    self.logger.info(f"     ğŸ“Š {optimization_metric}: {trial_score:.6f}")
                    
                    # æ›´æ–°æœ€ä½³ç»“æœ
                    if self._is_better_score(trial_score, best_score, optimization_direction):
                        best_score = trial_score
                        best_params = trial_params.copy()
                        best_model = trial_model
                        self.logger.info(f"     ğŸ† å‘ç°æ›´å¥½çš„å‚æ•°ï¼{optimization_metric}: {best_score:.6f}")
                    
                except Exception as e:
                    self.logger.warning(f"     âŒ ç¬¬ {trial_idx + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜è°ƒä¼˜ç»“æœ
            self._save_tuning_results(tuning_results, best_params, best_score, tuning_config)
            
            # ä½¿ç”¨æœ€ä½³å‚æ•°æ›´æ–°æ¨¡å‹é…ç½®
            if best_params is not None:
                self.logger.info(f"   ğŸ¯ è°ƒä¼˜å®Œæˆï¼æœ€ä½³ {optimization_metric}: {best_score:.6f}")
                self.logger.info(f"   ğŸ† æœ€ä½³å‚æ•°: {best_params}")
                
                # æ›´æ–°é…ç½®ä¸­çš„å‚æ•°
                self.config['lightgbm']['basic_params'].update(best_params)
                self.model = best_model  # ä¿å­˜æœ€ä½³æ¨¡å‹
                
                return True
            else:
                self.logger.error("âŒ è°ƒä¼˜å¤±è´¥ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆçš„å‚æ•°ç»„åˆ")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False
    
    def _generate_grid_search_params(self, param_space: Dict) -> List[Dict]:
        """ç”Ÿæˆç½‘æ ¼æœç´¢çš„å‚æ•°ç»„åˆ"""
        if not param_space:
            return [{}]
        
        param_names = list(param_space.keys())
        param_values = list(param_space.values())
        
        combinations = []
        for combination in itertools.product(*param_values):
            param_dict = dict(zip(param_names, combination))
            combinations.append(param_dict)
        
        return combinations
    
    def _generate_random_search_params(self, param_space: Dict, max_trials: int) -> List[Dict]:
        """ç”Ÿæˆéšæœºæœç´¢çš„å‚æ•°ç»„åˆ"""
        if not param_space:
            return [{}]
        
        combinations = []
        for _ in range(max_trials):
            param_dict = {}
            for param_name, param_values in param_space.items():
                param_dict[param_name] = random.choice(param_values)
            combinations.append(param_dict)
        
        return combinations
    
    def _evaluate_single_trial(self, model, metric: str) -> float:
        """è¯„ä¼°å•æ¬¡è¯•éªŒçš„æ¨¡å‹"""
        # è·å–éªŒè¯é›†é¢„æµ‹
        if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
            # äºŒåˆ†ç±»
            y_pred_proba = model.predict(self.X_val)
            y_pred = (y_pred_proba > 0.5).astype(int)
        else:
            # å›å½’
            y_pred = model.predict(self.X_val)
            y_pred_proba = None
        
        # è®¡ç®—æŒ‡æ ‡
        try:
            if metric == 'auc' and y_pred_proba is not None:
                from sklearn.metrics import roc_auc_score
                return roc_auc_score(self.y_val, y_pred_proba)
            elif metric == 'accuracy':
                return np.mean(self.y_val == y_pred)
            elif metric == 'precision':
                from sklearn.metrics import precision_score
                return precision_score(self.y_val, y_pred, zero_division=0)
            elif metric == 'recall':
                from sklearn.metrics import recall_score
                return recall_score(self.y_val, y_pred, zero_division=0)
            elif metric == 'f1_score':
                from sklearn.metrics import f1_score
                return f1_score(self.y_val, y_pred, zero_division=0)
            elif metric == 'rmse':
                return np.sqrt(mean_squared_error(self.y_val, y_pred))
            elif metric == 'mae':
                return mean_absolute_error(self.y_val, y_pred)
            elif metric == 'r2_score':
                return r2_score(self.y_val, y_pred)
            else:
                self.logger.warning(f"æœªçŸ¥çš„è¯„ä¼°æŒ‡æ ‡: {metric}ï¼Œä½¿ç”¨é»˜è®¤AUC")
                if y_pred_proba is not None:
                    from sklearn.metrics import roc_auc_score
                    return roc_auc_score(self.y_val, y_pred_proba)
                else:
                    return r2_score(self.y_val, y_pred)
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æŒ‡æ ‡ {metric} å¤±è´¥: {e}")
            return 0.0
    
    def _is_better_score(self, current_score: float, best_score: float, direction: str) -> bool:
        """åˆ¤æ–­å½“å‰åˆ†æ•°æ˜¯å¦æ›´å¥½"""
        if direction == 'maximize':
            return current_score > best_score
        else:
            return current_score < best_score
    
    def _save_tuning_results(self, results: List[Dict], best_params: Dict, best_score: float, config: Dict):
        """ä¿å­˜è°ƒä¼˜ç»“æœ"""
        try:
            results_save_config = config.get('results_save', {})
            
            if results_save_config.get('save_all_trials', True):
                # ä¿å­˜æ‰€æœ‰è¯•éªŒç»“æœ
                results_df = pd.DataFrame(results)
                results_path = self.results_save_dir / "hyperparameter_tuning_results.csv"
                results_df.to_csv(results_path, index=False, encoding='utf-8')
                self.logger.info(f"   âœ… è°ƒä¼˜ç»“æœå·²ä¿å­˜: {results_path}")
            
            if results_save_config.get('save_best_model', True):
                # ä¿å­˜æœ€ä½³å‚æ•°
                best_result = {
                    'best_score': best_score,
                    'best_params': best_params,
                    'total_trials': len(results),
                    'optimization_metric': config.get('optimization_metric', 'auc'),
                    'strategy': config.get('strategy', 'grid_search')
                }
                
                best_params_path = self.results_save_dir / "best_hyperparameters.json"
                with open(best_params_path, 'w', encoding='utf-8') as f:
                    json.dump(best_result, f, ensure_ascii=False, indent=2)
                self.logger.info(f"   âœ… æœ€ä½³å‚æ•°å·²ä¿å­˜: {best_params_path}")
            
            if results_save_config.get('detailed_log', True):
                # ä¿å­˜è¯¦ç»†æ—¥å¿—
                log_content = []
                log_content.append("è¶…å‚æ•°è°ƒä¼˜è¯¦ç»†ç»“æœ")
                log_content.append("=" * 50)
                log_content.append(f"ç­–ç•¥: {config.get('strategy', 'grid_search')}")
                log_content.append(f"ä¼˜åŒ–æŒ‡æ ‡: {config.get('optimization_metric', 'auc')}")
                log_content.append(f"ä¼˜åŒ–æ–¹å‘: {config.get('optimization_direction', 'maximize')}")
                log_content.append(f"æ€»è¯•éªŒæ¬¡æ•°: {len(results)}")
                log_content.append(f"æœ€ä½³åˆ†æ•°: {best_score:.6f}")
                log_content.append(f"æœ€ä½³å‚æ•°: {best_params}")
                log_content.append("")
                log_content.append("æ‰€æœ‰è¯•éªŒç»“æœ:")
                log_content.append("-" * 30)
                
                for result in results:
                    log_content.append(f"è¯•éªŒ {result['trial']}: {result['metric']}={result['score']:.6f}, å‚æ•°={result['params']}")
                
                log_path = self.results_save_dir / "hyperparameter_tuning_log.txt"
                with open(log_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(log_content))
                self.logger.info(f"   âœ… è¯¦ç»†æ—¥å¿—å·²ä¿å­˜: {log_path}")
                
        except Exception as e:
            self.logger.warning(f"ä¿å­˜è°ƒä¼˜ç»“æœå¤±è´¥: {e}")

    def train_model(self) -> bool:
        """è®­ç»ƒæ¨¡å‹"""
        try:
            self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡è°ƒä¼˜è®­ç»ƒäº†æ¨¡å‹
            if hasattr(self, 'model') and self.model is not None:
                self.logger.info("   âœ… ä½¿ç”¨è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹")
                
                # åˆ›å»ºå†…ç½®å¯è§†åŒ–å™¨
                self.visualizer = self._create_builtin_visualizer()
                self.logger.info("   ğŸ¨ å†…ç½®å¯è§†åŒ–å™¨å·²å¯ç”¨")
                
                return True
            
            # è·å–æ¨¡å‹å‚æ•°
            lgb_config = self.config.get('lightgbm', {})
            basic_params = lgb_config.get('basic_params', {})
            advanced_params = lgb_config.get('advanced_params', {})
            fit_params = lgb_config.get('fit_params', {})
            
            # åˆå¹¶å‚æ•°
            model_params = {**basic_params, **advanced_params}
            
            # è®­ç»ƒå‚æ•°
            training_config = self.config.get('training', {}).get('training_params', {})
            early_stopping_rounds = training_config.get('early_stopping_rounds', 100)
            verbose = training_config.get('verbose', 100)
            eval_metric = training_config.get('eval_metric', ['rmse'])
            
            # åˆ›å»ºæ•°æ®é›†
            train_data = lgb.Dataset(self.X_train, label=self.y_train)
            val_data = lgb.Dataset(self.X_val, label=self.y_val, reference=train_data)
            
            # è®­ç»ƒæ¨¡å‹
            self.logger.info("   å¼€å§‹è®­ç»ƒ...")
            
            # åˆ›å»ºå†…ç½®å¯è§†åŒ–å™¨ï¼ˆç”¨äºè®°å½•è®­ç»ƒå†å²ï¼‰
            self.visualizer = self._create_builtin_visualizer()
            self.logger.info("   ğŸ¨ å†…ç½®å¯è§†åŒ–å™¨å·²å¯ç”¨")
            
            callbacks = [
                lgb.early_stopping(early_stopping_rounds),
                lgb.log_evaluation(verbose)
            ]
            
            # æ·»åŠ å¯è§†åŒ–è®°å½•å›è°ƒ
            if self.visualizer and hasattr(self.visualizer, 'record_callback'):
                callbacks.append(self.visualizer.record_callback())
            
            self.model = lgb.train(
                model_params,
                train_data,
                valid_sets=[train_data, val_data],
                valid_names=['train', 'val'],
                num_boost_round=fit_params.get('num_boost_round', 1000),
                callbacks=callbacks
            )
            
            self.logger.info("   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def evaluate_model(self) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            self.logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è·å–é¢„æµ‹ç»“æœ
            if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                # äºŒåˆ†ç±»ï¼šè·å–æ¦‚ç‡é¢„æµ‹
                y_train_pred_proba = self.model.predict(self.X_train)
                y_val_pred_proba = self.model.predict(self.X_val)
                
                # è½¬æ¢ä¸ºç±»åˆ«é¢„æµ‹ï¼ˆæ¦‚ç‡ > 0.5 ä¸ºçœ‹å¤šï¼‰
                y_train_pred = (y_train_pred_proba > 0.5).astype(int)
                y_val_pred = (y_val_pred_proba > 0.5).astype(int)
            else:
                # å›å½’ï¼šç›´æ¥é¢„æµ‹æ•°å€¼
                y_train_pred = self.model.predict(self.X_train)
                y_val_pred = self.model.predict(self.X_val)
                y_train_pred_proba = y_val_pred_proba = None
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            eval_config = self.config.get('evaluation', {})
            metrics_list = eval_config.get('metrics', ['rmse', 'mae', 'r2_score'])
            
            results = {}
            
            for split, y_true, y_pred, y_pred_proba in [
                ('train', self.y_train, y_train_pred, y_train_pred_proba),
                ('val', self.y_val, y_val_pred, y_val_pred_proba)
            ]:
                split_metrics = {}
                
                for metric in metrics_list:
                    try:
                        if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                            # ğŸ¯ åˆ†ç±»æŒ‡æ ‡
                            if metric == 'accuracy':
                                value = np.mean(y_true == y_pred) * 100
                            elif metric == 'auc' and y_pred_proba is not None:
                                from sklearn.metrics import roc_auc_score
                                value = roc_auc_score(y_true, y_pred_proba)
                            elif metric == 'precision':
                                from sklearn.metrics import precision_score
                                value = precision_score(y_true, y_pred, zero_division=0)
                            elif metric == 'recall':
                                from sklearn.metrics import recall_score
                                value = recall_score(y_true, y_pred, zero_division=0)
                            elif metric == 'f1_score':
                                from sklearn.metrics import f1_score
                                value = f1_score(y_true, y_pred, zero_division=0)
                            elif metric == 'log_loss' and y_pred_proba is not None:
                                from sklearn.metrics import log_loss
                                # å¤„ç†æ¦‚ç‡è¾¹ç•Œé—®é¢˜
                                y_pred_proba_clipped = np.clip(y_pred_proba, 1e-15, 1-1e-15)
                                value = log_loss(y_true, y_pred_proba_clipped)
                            else:
                                continue
                        else:
                            # ğŸ“ˆ å›å½’æŒ‡æ ‡
                            if metric == 'rmse':
                                value = np.sqrt(mean_squared_error(y_true, y_pred))
                            elif metric == 'mae':
                                value = mean_absolute_error(y_true, y_pred)
                            elif metric == 'mape':
                                value = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                            elif metric == 'r2_score':
                                value = r2_score(y_true, y_pred)
                            elif metric == 'explained_variance':
                                value = explained_variance_score(y_true, y_pred)
                            elif metric == 'directional_accuracy':
                                # æ–¹å‘å‡†ç¡®ç‡ï¼ˆè‚¡ç¥¨é¢„æµ‹ç‰¹æœ‰æŒ‡æ ‡ï¼‰
                                direction_true = np.sign(y_true)
                                direction_pred = np.sign(y_pred)
                                value = np.mean(direction_true == direction_pred) * 100
                            else:
                                continue
                        
                        split_metrics[metric] = float(value)
                        
                    except Exception as e:
                        self.logger.warning(f"   è®¡ç®—æŒ‡æ ‡ {metric} å¤±è´¥: {e}")
                        continue
                
                results[split] = split_metrics
            
            # è¾“å‡ºç»“æœ
            prediction_type = "æ–¹å‘é¢„æµ‹" if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction' else "å›å½’é¢„æµ‹"
            self.logger.info(f"   ğŸ“ˆ è¯„ä¼°ç»“æœ ({prediction_type}):")
            for split, metrics in results.items():
                self.logger.info(f"     {split.upper()}:")
                for metric, value in metrics.items():
                    if metric in ['mape', 'directional_accuracy', 'accuracy']:
                        self.logger.info(f"       {metric}: {value:.2f}%")
                    else:
                        self.logger.info(f"       {metric}: {value:.6f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}

    def save_model(self) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            self.logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶...")
            
            model_config = self.config.get('output', {}).get('model_save', {})
            model_name = model_config.get('model_name', 'lightgbm_stock_model')
            save_formats = model_config.get('save_format', ['pkl'])
            
            # ä¿å­˜æ¨¡å‹
            for fmt in save_formats:
                if fmt == 'pkl':
                    model_path = self.model_save_dir / f"{model_name}.pkl"
                    joblib.dump(self.model, model_path)
                    self.logger.info(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
                elif fmt == 'txt':
                    model_path = self.model_save_dir / f"{model_name}.txt"
                    self.model.save_model(str(model_path))
                    self.logger.info(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            if self.scaler:
                scaler_path = self.model_save_dir / "scaler.pkl"
                joblib.dump(self.scaler, scaler_path)
                self.logger.info(f"   âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
            
            # ä¿å­˜ç‰¹å¾åç§°
            feature_info = {
                'feature_names': self.feature_names,
                'feature_count': len(self.feature_names),
                'training_config': self.config_path
            }
            
            with open(self.model_save_dir / "feature_names.json", 'w', encoding='utf-8') as f:
                json.dump(feature_info, f, ensure_ascii=False, indent=2)
            
            with open(self.model_save_dir / "feature_names_simple.txt", 'w', encoding='utf-8') as f:
                f.write('\n'.join(self.feature_names))
            
            self.logger.info(f"   âœ… ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def save_results(self, evaluation_results: Dict) -> bool:
        """ä¿å­˜ç»“æœ"""
        try:
            self.logger.info("ğŸ“Š ä¿å­˜è®­ç»ƒç»“æœ...")
            
            results_config = self.config.get('output', {}).get('results_save', {})
            
            # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
            if results_config.get('save_metrics', True):
                metrics_path = self.results_save_dir / "metrics.json"
                with open(metrics_path, 'w', encoding='utf-8') as f:
                    json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
                self.logger.info(f"   âœ… è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
            
            # ä¿å­˜é¢„æµ‹ç»“æœ - åªä¿å­˜éªŒè¯é›†
            if results_config.get('save_predictions', True):
                # åªè·å–éªŒè¯é›†é¢„æµ‹
                y_val_pred = self.model.predict(self.X_val)
                
                # åŸºç¡€é¢„æµ‹æ•°æ®æ¡† - åªåŒ…å«éªŒè¯é›†
                pred_data = {
                    'split': ['val'] * len(self.y_val),
                    'y_true': self.y_val.tolist(),
                    'y_pred': y_val_pred.tolist()
                }
                
                # å¦‚æœæœ‰è‚¡ç¥¨ä¿¡æ¯ï¼Œæ·»åŠ éªŒè¯é›†çš„è‚¡ç¥¨ä¿¡æ¯
                if self.stock_info_val is not None:
                    # æ·»åŠ éªŒè¯é›†è‚¡ç¥¨ä¿¡æ¯åˆ°é¢„æµ‹æ•°æ®ä¸­
                    for col in self.stock_info_val.columns:
                        pred_data[col] = self.stock_info_val[col].tolist()
                    
                    self.logger.info(f"   âœ… é¢„æµ‹ç»“æœåŒ…å«è‚¡ç¥¨ä¿¡æ¯: {list(self.stock_info_val.columns)}")
                
                pred_df = pd.DataFrame(pred_data)
                
                pred_path = self.results_save_dir / "predictions.csv"
                pred_df.to_csv(pred_path, index=False, encoding='utf-8')
                self.logger.info(f"   âœ… éªŒè¯é›†é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_path}")
                self.logger.info(f"   ğŸ“Š é¢„æµ‹ç»“æœåŒ…å« {len(pred_df)} æ¡è®°å½•ï¼Œ{len(pred_df.columns)} åˆ—ä¿¡æ¯")
            
            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            if results_config.get('save_feature_importance', True):
                importance = self.model.feature_importance(importance_type='gain')
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                importance_path = self.results_save_dir / "feature_importance.csv"
                importance_df.to_csv(importance_path, index=False, encoding='utf-8')
                
                # ä¿å­˜å‰20ä¸ªé‡è¦ç‰¹å¾
                top_features_path = self.results_save_dir / "top_features.txt"
                with open(top_features_path, 'w', encoding='utf-8') as f:
                    f.write("å‰20ä¸ªæœ€é‡è¦ç‰¹å¾:\n")
                    f.write("=" * 50 + "\n")
                    for i, (_, row) in enumerate(importance_df.head(20).iterrows()):
                        f.write(f"{i+1:2d}. {row['feature']}: {row['importance']:.6f}\n")
                
                self.logger.info(f"   âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_path}")
            
            # ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            if hasattr(self, 'visualizer') and self.visualizer:
                try:
                    self.logger.info("   ğŸ¨ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    y_train_pred = self.model.predict(self.X_train)
                    y_val_pred = self.model.predict(self.X_val)
                    y_test_pred = self.model.predict(self.X_test)
                    
                    # è®¾ç½®ç‰¹å¾åç§°ç»™å¯è§†åŒ–å™¨
                    if hasattr(self.visualizer, '__dict__'):
                        self.visualizer.feature_names = self.feature_names
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
                    if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                        # å¯¹äºæ–¹å‘é¢„æµ‹ï¼Œåªç”Ÿæˆéƒ¨åˆ†å›¾è¡¨
                        viz_results = {}
                        
                        # å­¦ä¹ æ›²çº¿
                        learning_curve_path = self.visualizer.plot_learning_curves(self.model)
                        if learning_curve_path:
                            viz_results['learning_curves'] = learning_curve_path
                        
                        # ç‰¹å¾é‡è¦æ€§
                        feature_importance_path = self.visualizer.plot_feature_importance(
                            self.model, self.feature_names, top_n=20)
                        if feature_importance_path:
                            viz_results['feature_importance'] = feature_importance_path
                        
                        self.logger.info(f"   ğŸ¯ æ–¹å‘é¢„æµ‹æ¨¡å¼ï¼Œç”Ÿæˆäº† {len(viz_results)} ä¸ªå›¾è¡¨")
                        
                    else:
                        # å›å½’ä»»åŠ¡ç”Ÿæˆæ‰€æœ‰å›¾è¡¨ - åªä½¿ç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†
                        viz_results = {}
                        
                        # å­¦ä¹ æ›²çº¿
                        learning_curve_path = self.visualizer.plot_learning_curves(self.model)
                        if learning_curve_path:
                            viz_results['learning_curves'] = learning_curve_path
                        
                        # ç‰¹å¾é‡è¦æ€§
                        feature_importance_path = self.visualizer.plot_feature_importance(
                            self.model, self.feature_names, top_n=20)
                        if feature_importance_path:
                            viz_results['feature_importance'] = feature_importance_path
                        
                        # é¢„æµ‹æ•£ç‚¹å›¾å’Œæ®‹å·®å›¾ - åªç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
                        for split_name, y_true, y_pred in [
                            ('è®­ç»ƒé›†', self.y_train, y_train_pred),
                            ('éªŒè¯é›†', self.y_val, y_val_pred)
                        ]:
                            scatter_path = self.visualizer.plot_predictions_scatter(y_true, y_pred, split_name)
                            if scatter_path:
                                viz_results[f'predictions_scatter_{split_name}'] = scatter_path
                            
                            residuals_path = self.visualizer.plot_residuals(y_true, y_pred, split_name)
                            if residuals_path:
                                viz_results[f'residuals_{split_name}'] = residuals_path
                        
                        self.logger.info(f"   ğŸ“ˆ å›å½’æ¨¡å¼ï¼Œç”Ÿæˆäº† {len(viz_results)} ä¸ªå›¾è¡¨")
                    
                    # æ˜¾ç¤ºå›¾è¡¨ä¿å­˜ä½ç½®
                    for chart_type, chart_path in viz_results.items():
                        self.logger.info(f"     ğŸ“Š {chart_type}: {chart_path}")
                        
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
                    import traceback
                    self.logger.warning(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            else:
                self.logger.info("   â„¹ï¸ æœªå¯ç”¨å¯è§†åŒ–åŠŸèƒ½")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def run_training_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹è‚¡ç¥¨LightGBMè®­ç»ƒæµç¨‹...")
            
            # 1. æ•°æ®é¢„å¤„ç†
            if not self.preprocess_stock_data():
                return False
            
            # 2. åŠ è½½æ•°æ®
            if not self.load_data():
                return False
            
            # 3. åˆ†å‰²æ•°æ®
            if not self.split_data():
                return False
            
            # 4. ç‰¹å¾é¢„å¤„ç†
            if not self.preprocess_features():
                return False
            
            # 5. è¶…å‚æ•°è°ƒä¼˜ï¼ˆå¯é€‰ï¼‰
            if not self.hyperparameter_tuning():
                return False
            
            # 6. è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæ²¡æœ‰é€šè¿‡è°ƒä¼˜å¾—åˆ°æ¨¡å‹ï¼‰
            if not self.train_model():
                return False
            
            # 7. è¯„ä¼°æ¨¡å‹
            evaluation_results = self.evaluate_model()
            if not evaluation_results:
                return False
            
            # 8. ä¿å­˜æ¨¡å‹
            if not self.save_model():
                return False
            
            # 9. ä¿å­˜ç»“æœ
            if not self.save_results(evaluation_results):
                return False
            
            self.logger.info("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
            self.logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {self.model_save_dir}")
            self.logger.info(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {self.results_save_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LightGBMè‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--config', type=str, 
                       default='config/train/lightGBM_stock_train.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ LightGBMè‚¡ç¥¨é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = StockLightGBMTrainer(args.config)
    except Exception as e:
        print(f"âŒ åˆ›å»ºè®­ç»ƒå™¨å¤±è´¥: {e}")
        sys.exit(1)
    
    # è¿è¡Œè®­ç»ƒæµç¨‹
    success = trainer.run_training_pipeline()
    
    if success:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹æ–‡ä»¶: {trainer.model_save_dir}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {trainer.results_save_dir}")
        sys.exit(0)
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        sys.exit(1)


if __name__ == "__main__":
    main()