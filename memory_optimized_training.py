# -*- coding: utf-8 -*-
"""
å†…å­˜ä¼˜åŒ–çš„è‚¡ç¥¨è®­ç»ƒè„šæœ¬ - è§£å†³å¤§é‡è‚¡ç¥¨è®­ç»ƒæ—¶çš„å†…å­˜æº¢å‡ºé—®é¢˜
"""

import os
import sys
import logging
import gc
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Tuple

# å¯¼å…¥LightGBMï¼ˆéœ€è¦åœ¨å¯¼å…¥æ¨¡å‹ä¹‹å‰ï¼‰
try:
    import lightgbm as lgb
except ImportError:
    print("âš ï¸ è­¦å‘Š: LightGBMæœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install lightgbm")
    sys.exit(1)

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.training_pipeline import ModelTrainingPipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'memory_optimized_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MemoryOptimizedPipeline(ModelTrainingPipeline):
    """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒç®¡é“"""
    
    def __init__(self, *args, **kwargs):
        # å¼ºåˆ¶ç¦ç”¨æ‰€æœ‰ç¼“å­˜
        kwargs['enable_batch_cache'] = False
        kwargs['cache_workers'] = 1
        super().__init__(*args, **kwargs)
        
        # å†…å­˜ä¼˜åŒ–é…ç½®
        self.batch_size = 50  # æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        self.max_samples_per_stock = 500  # æ¯åªè‚¡ç¥¨æœ€å¤§æ ·æœ¬æ•°
        self.feature_reduction_ratio = 0.7  # ç‰¹å¾é™ç»´æ¯”ä¾‹
    
    def memory_optimized_prepare_data(self, stock_codes: List[str], 
                                    prediction_days: int = 1) -> Tuple[np.ndarray, np.ndarray, List[str], Dict]:
        """
        å†…å­˜ä¼˜åŒ–çš„æ•°æ®å‡†å¤‡æ–¹æ³• - åˆ†æ‰¹å¤„ç† + æ•°æ®å‹ç¼©
        """
        logger.info(f"ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–æ•°æ®å‡†å¤‡ï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        logger.info(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        logger.info(f"   æœ€å¤§æ ·æœ¬æ•°/è‚¡ç¥¨: {self.max_samples_per_stock}")
        
        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
        all_batches = []
        feature_names = None
        feature_info = None
        total_samples = 0
        processed_stocks = 0
        
        for i in range(0, len(stock_codes), self.batch_size):
            batch_stocks = stock_codes[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(stock_codes) + self.batch_size - 1) // self.batch_size
            
            logger.info(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_stocks)} åªè‚¡ç¥¨)")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            try:
                batch_X, batch_y, batch_feature_names, batch_feature_info = self._process_stock_batch(
                    batch_stocks, prediction_days
                )
                
                if batch_X is not None and len(batch_X) > 0:
                    # é™åˆ¶æ¯æ‰¹çš„æ ·æœ¬æ•°é‡ä»¥æ§åˆ¶å†…å­˜
                    if len(batch_X) > self.max_samples_per_stock * len(batch_stocks):
                        max_samples = self.max_samples_per_stock * len(batch_stocks)
                        indices = np.random.choice(len(batch_X), max_samples, replace=False)
                        batch_X = batch_X[indices]
                        batch_y = batch_y[indices]
                    
                    all_batches.append((batch_X, batch_y))
                    total_samples += len(batch_X)
                    processed_stocks += len(batch_stocks)
                    
                    if feature_names is None:
                        feature_names = batch_feature_names
                        feature_info = batch_feature_info
                    
                    logger.info(f"   âœ… æ‰¹æ¬¡å®Œæˆ: {len(batch_X)} ä¸ªæ ·æœ¬")
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    del batch_X, batch_y
                    gc.collect()
                else:
                    logger.warning(f"   âŒ æ‰¹æ¬¡æ— æœ‰æ•ˆæ•°æ®")
                    
            except Exception as e:
                logger.error(f"   âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}")
                continue
        
        if not all_batches:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ‰¹æ¬¡æ•°æ®")
        
        logger.info(f"\nğŸ”„ åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®...")
        logger.info(f"   æ€»æ‰¹æ¬¡æ•°: {len(all_batches)}")
        logger.info(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        logger.info(f"   å¤„ç†è‚¡ç¥¨æ•°: {processed_stocks}/{len(stock_codes)}")
        
        # é€æ­¥åˆå¹¶æ‰¹æ¬¡æ•°æ®ä»¥èŠ‚çœå†…å­˜
        X_combined = None
        y_combined = None
        
        for i, (batch_X, batch_y) in enumerate(all_batches):
            if X_combined is None:
                X_combined = batch_X.copy()
                y_combined = batch_y.copy()
            else:
                X_combined = np.vstack([X_combined, batch_X])
                y_combined = np.hstack([y_combined, batch_y])
            
            # åˆ é™¤å·²å¤„ç†çš„æ‰¹æ¬¡æ•°æ®
            del batch_X, batch_y
            
            if (i + 1) % 10 == 0:  # æ¯10ä¸ªæ‰¹æ¬¡å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                logger.info(f"   å·²åˆå¹¶ {i+1}/{len(all_batches)} ä¸ªæ‰¹æ¬¡")
        
        # æ¸…ç†æ‰¹æ¬¡åˆ—è¡¨
        del all_batches
        gc.collect()
        
        # æœ€ç»ˆæ•°æ®å‹ç¼©å’Œé‡‡æ ·
        if len(X_combined) > 50000:  # å¦‚æœæ ·æœ¬æ•°è¿‡å¤šï¼Œè¿›è¡Œé‡‡æ ·
            logger.info(f"ğŸ”½ æ ·æœ¬æ•°è¿‡å¤š({len(X_combined)})ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...")
            indices = np.random.choice(len(X_combined), 50000, replace=False)
            X_combined = X_combined[indices]
            y_combined = y_combined[indices]
            logger.info(f"   é‡‡æ ·åæ ·æœ¬æ•°: {len(X_combined)}")
        
        logger.info(f"\nâœ… å†…å­˜ä¼˜åŒ–æ•°æ®å‡†å¤‡å®Œæˆ:")
        logger.info(f"   æœ€ç»ˆæ ·æœ¬æ•°: {len(X_combined)}")
        logger.info(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        logger.info(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {y_combined.mean():.3f}")
        logger.info(f"   å†…å­˜å ç”¨ä¼°è®¡: {X_combined.nbytes / 1024 / 1024:.1f} MB")
        
        return X_combined, y_combined, feature_names, feature_info
    
    def _process_stock_batch(self, stock_codes: List[str], 
                           prediction_days: int) -> Tuple[np.ndarray, np.ndarray, List[str], Dict]:
        """å¤„ç†å•ä¸ªè‚¡ç¥¨æ‰¹æ¬¡"""
        batch_X, batch_y = [], []
        feature_names = None
        feature_info = None
        
        for stock_code in stock_codes:
            try:
                # åŠ è½½è‚¡ç¥¨æ•°æ®
                df = self.load_stock_data(stock_code)
                
                if len(df) < self.config['min_samples']:
                    continue
                
                # æ•°æ®é¢„å¤„ç† - é™åˆ¶æ•°æ®é‡
                if len(df) > 2000:  # å¦‚æœæ•°æ®è¿‡å¤šï¼Œåªå–æœ€è¿‘çš„æ•°æ®
                    df = df.tail(2000)
                
                # ç‰¹å¾å·¥ç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
                df_features = self.feature_engineer.create_all_features(df, stock_code)
                
                # å‡†å¤‡æ¨¡å‹æ•°æ®
                X, y, feature_names_temp, feature_info_temp = self.feature_engineer.prepare_model_data(
                    df_features, 
                    prediction_days=prediction_days,
                    lookback_window=min(30, self.config['sequence_length'])  # å‡å°‘å›æœ›çª—å£
                )
                
                if len(X) > 0:
                    # é™åˆ¶æ¯åªè‚¡ç¥¨çš„æ ·æœ¬æ•°
                    if len(X) > self.max_samples_per_stock:
                        indices = np.random.choice(len(X), self.max_samples_per_stock, replace=False)
                        X = X[indices]
                        y = y[indices]
                    
                    batch_X.append(X)
                    batch_y.append(y)
                    
                    if feature_names is None:
                        feature_names = feature_names_temp
                        feature_info = feature_info_temp
                
                # åŠæ—¶æ¸…ç†å†…å­˜
                del df, df_features, X, y
                gc.collect()
                
            except Exception as e:
                logger.warning(f"å¤„ç†è‚¡ç¥¨ {stock_code} å¤±è´¥: {str(e)}")
                continue
        
        if not batch_X:
            return None, None, None, None
        
        # åˆå¹¶æ‰¹æ¬¡å†…çš„æ•°æ®
        X_batch = np.vstack(batch_X)
        y_batch = np.hstack(batch_y)
        
        # æ¸…ç†ä¸´æ—¶æ•°æ®
        del batch_X, batch_y
        gc.collect()
        
        return X_batch, y_batch, feature_names, feature_info
    
    def memory_optimized_train_model(self, stock_codes: List[str], 
                                   prediction_days: int = 1) -> object:
        """å†…å­˜ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒ"""
        logger.info(f"ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–æ¨¡å‹è®­ç»ƒï¼Œé¢„æµ‹ {prediction_days} å¤©")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®å‡†å¤‡æ–¹æ³•
        X, y, feature_names, feature_info = self.memory_optimized_prepare_data(stock_codes, prediction_days)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = self.split_data(X, y)
        
        # åˆ›å»ºè½»é‡çº§æ¨¡å‹ï¼ˆä¸»è¦ä½¿ç”¨LightGBMï¼‰
        from core.ai_models import LightGBMModel
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        model = LightGBMModel()
        
        # è®­ç»ƒæ¨¡å‹
        logger.info("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        model.fit(X_train.reshape(X_train.shape[0], -1), y_train, 
                 X_test.reshape(X_test.shape[0], -1), y_test)
        
        # æ‰‹åŠ¨è¯„ä¼°æ¨¡å‹ï¼ˆå› ä¸ºLightGBMModelæ²¡æœ‰evaluateæ–¹æ³•ï¼‰
        logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        X_test_2d = X_test.reshape(X_test.shape[0], -1)
        y_pred = model.predict(X_test_2d)
        y_proba = model.predict_proba(X_test_2d)[:, 1]
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)
        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'predictions': y_pred,
            'probabilities': y_proba
        }
        
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"   ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"   å¬å›ç‡: {recall:.4f}")
        logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
        
        return model

def main():
    """å†…å­˜ä¼˜åŒ–çš„ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–è‚¡ç¥¨è®­ç»ƒæµç¨‹")
    logger.info("=" * 80)
    
    # åˆå§‹åŒ–ä¼˜åŒ–çš„è®­ç»ƒç®¡é“
    pipeline = MemoryOptimizedPipeline(
        data_dir="data/datas_em",
        enable_batch_cache=False,
        cache_workers=1
    )
    
    # è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
    logger.info("ğŸ“Š æ‰«ææ‰€æœ‰å¯ç”¨è‚¡ç¥¨...")
    all_stocks = pipeline.get_available_stocks()
    
    if not all_stocks:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
        return False
    
    logger.info(f"âœ… å‘ç° {len(all_stocks)} åªæœ‰æ•ˆè‚¡ç¥¨")
    
    # ä¸ºäº†é¿å…å†…å­˜é—®é¢˜ï¼Œå¯ä»¥é€‰æ‹©å¤„ç†éƒ¨åˆ†è‚¡ç¥¨
    if len(all_stocks) > 1000:
        logger.info(f"âš ï¸ è‚¡ç¥¨æ•°é‡è¿‡å¤š({len(all_stocks)})ï¼Œéšæœºé€‰æ‹©1000åªè¿›è¡Œè®­ç»ƒ")
        import random
        all_stocks = random.sample(all_stocks, 1000)
    
    logger.info(f"ğŸ¯ å®é™…è®­ç»ƒè‚¡ç¥¨æ•°: {len(all_stocks)}")
    
    try:
        # è®­ç»ƒæ¨¡å‹ï¼ˆåªè®­ç»ƒä¸€ä¸ªé¢„æµ‹å¤©æ•°ä»¥èŠ‚çœå†…å­˜ï¼‰
        prediction_days = 1
        logger.info(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹...")
        
        model = pipeline.memory_optimized_train_model(all_stocks, prediction_days)
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"models/memory_optimized_model_{prediction_days}d_{timestamp}"
        os.makedirs(model_path, exist_ok=True)
        
        if hasattr(model, 'save_model'):
            model.save_model(os.path.join(model_path, 'model.pkl'))
        else:
            import joblib
            joblib.dump(model, os.path.join(model_path, 'model.pkl'))
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ å†…å­˜ä¼˜åŒ–è®­ç»ƒæµç¨‹å®Œæˆï¼")
        logger.info("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ å†…å­˜ä¼˜åŒ–è®­ç»ƒæˆåŠŸå®Œæˆï¼")
    else:
        print("\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        sys.exit(1)