#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦ç½‘ç›˜ä¸Šä¼ åŠ©æ‰‹ - åˆ›å»ºé€‚åˆç™¾åº¦ç½‘ç›˜çš„éƒ¨ç½²åŒ…
"""

import os
import tarfile
import zipfile
import shutil
from pathlib import Path
import json
from datetime import datetime
import argparse

class BaiduUploadHelper:
    """ç™¾åº¦ç½‘ç›˜æ•°æ®åŒ…ä¸Šä¼ åŠ©æ‰‹ - åªæ‰“åŒ…æ•°æ®ï¼Œä¸æ‰“åŒ…ä»£ç """
    
    def __init__(self, data_dir: str = "datas_em"):
        self.data_dir = Path(data_dir)
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
    def create_data_package(self, format_type: str = "tar.gz") -> str:
        """åˆ›å»ºçº¯æ•°æ®åŒ…ï¼ˆä¸åŒ…å«ä»£ç ï¼‰"""
        print("ğŸ“¦ åˆ›å»ºè‚¡ç¥¨æ•°æ®åŒ…...")
        
        # åˆ›å»ºæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        package_name = f"stock_data_{timestamp}.{format_type}"
        
        # ç›´æ¥æ‰“åŒ…æ•°æ®æ–‡ä»¶
        csv_count = self._create_data_archive(package_name, format_type)
        
        # æ˜¾ç¤ºç»“æœ
        self._show_data_package_info(package_name, csv_count)
        
        return package_name
    
    def _create_data_archive(self, package_name: str, format_type: str) -> int:
        """åˆ›å»ºæ•°æ®å‹ç¼©åŒ…"""
        print("ğŸ“Š å‹ç¼©è‚¡ç¥¨æ•°æ®æ–‡ä»¶...")
        
        csv_files = list(self.data_dir.glob("*.csv"))
        if not csv_files:
            raise FileNotFoundError(f"åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        
        csv_count = 0
        
        if format_type == "tar.gz":
            with tarfile.open(package_name, "w:gz") as tar:
                for csv_file in csv_files:
                    # ä¿æŒç›®å½•ç»“æ„ datas_em/æ–‡ä»¶å
                    tar.add(csv_file, arcname=f"datas_em/{csv_file.name}")
                    csv_count += 1
                    if csv_count % 500 == 0:
                        print(f"å·²å‹ç¼© {csv_count} ä¸ªæ–‡ä»¶...")
        
        elif format_type == "zip":
            with zipfile.ZipFile(package_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for csv_file in csv_files:
                    zipf.write(csv_file, f"datas_em/{csv_file.name}")
                    csv_count += 1
                    if csv_count % 500 == 0:
                        print(f"å·²å‹ç¼© {csv_count} ä¸ªæ–‡ä»¶...")
        
        # è®¡ç®—å‹ç¼©æ¯”
        original_size = sum(f.stat().st_size for f in csv_files)
        compressed_size = Path(package_name).stat().st_size
        compression_ratio = (1 - compressed_size / original_size) * 100
        
        print(f"âœ… æ•°æ®å‹ç¼©å®Œæˆ:")
        print(f"   - æ–‡ä»¶æ•°é‡: {csv_count}")
        print(f"   - åŸå§‹å¤§å°: {original_size // 1024 // 1024} MB")
        print(f"   - å‹ç¼©å: {compressed_size // 1024 // 1024} MB")
        print(f"   - å‹ç¼©ç‡: {compression_ratio:.1f}%")
        
        return csv_count
    
    def _show_data_package_info(self, package_path: str, csv_count: int):
        """æ˜¾ç¤ºæ•°æ®åŒ…ä¿¡æ¯"""
        package_file = Path(package_path)
        file_size_mb = package_file.stat().st_size / 1024 / 1024
        
        print("\n" + "="*50)
        print("ğŸ‰ è‚¡ç¥¨æ•°æ®åŒ…åˆ›å»ºå®Œæˆï¼")
        print("="*50)
        print(f"ğŸ“¦ æ–‡ä»¶å: {package_path}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        print(f"ğŸ“ˆ æ•°æ®æ–‡ä»¶: {csv_count} ä¸ªCSVæ–‡ä»¶")
        print()
        print("ğŸ“ Dockeréƒ¨ç½²ä½¿ç”¨æ–¹æ³•ï¼š")
        print("1. ğŸ“¤ ä¸Šä¼ åˆ°äº‘å­˜å‚¨")
        print("   - ç™¾åº¦ç½‘ç›˜ï¼šä¸Šä¼ å¹¶åˆ›å»ºåˆ†äº«é“¾æ¥")
        print("   - é˜¿é‡Œäº‘OSSï¼šossutil cp package.tar.gz oss://bucket/")
        print()
        print("2. ğŸ³ æœåŠ¡å™¨Dockeréƒ¨ç½²")
        print("   - git clone your-repo && cd wuwuquant")
        print("   - echo 'DATA_PACKAGE_URL=åˆ†äº«é“¾æ¥' > .env")
        print("   - python å¿«é€Ÿéƒ¨ç½².py")
        print()
        print("3. âœ… è‡ªåŠ¨å®Œæˆ")
        print("   - Dockerå®¹å™¨è‡ªåŠ¨ä¸‹è½½æ•°æ®åŒ…")
        print("   - è‡ªåŠ¨è§£å‹åˆ° datas_em/ ç›®å½•")
        print("   - å¯åŠ¨Webç•Œé¢: http://æœåŠ¡å™¨IP:8501")
        print()
        print("ğŸ’¡ çº¯æ•°æ®åŒ…ï¼Œä»£ç é€šè¿‡gitè·å–ï¼Œå®Œç¾åˆ†ç¦»ï¼")

def main():
    parser = argparse.ArgumentParser(description="è‚¡ç¥¨æ•°æ®åŒ…ä¸Šä¼ åŠ©æ‰‹")
    parser.add_argument("--data-dir", default="datas_em", help="æ•°æ®ç›®å½•")
    parser.add_argument("--format", choices=["tar.gz", "zip"], default="tar.gz", help="å‹ç¼©æ ¼å¼")
    
    args = parser.parse_args()
    
    print("ğŸ“Š AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - æ•°æ®åŒ…åˆ›å»ºå·¥å…·")
    print("=" * 50)
    print("ğŸ¯ åŠŸèƒ½ï¼šåˆ›å»ºçº¯æ•°æ®åŒ…ï¼ˆä¸åŒ…å«ä»£ç ï¼‰")
    print("ğŸ³ ç”¨é€”ï¼šDockeråˆ†ç¦»å¼éƒ¨ç½²")
    print()
    
    try:
        helper = BaiduUploadHelper(args.data_dir)
        package_path = helper.create_data_package(args.format)
        
        print(f"\nğŸŠ æ•°æ®åŒ…åˆ›å»ºæˆåŠŸ: {package_path}")
        print("ğŸ“¤ è¯·ä¸Šä¼ åˆ°ç™¾åº¦ç½‘ç›˜æˆ–äº‘å­˜å‚¨ï¼")
        
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œä¸”datas_emç›®å½•å­˜åœ¨")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®åŒ…å¤±è´¥: {e}")

if __name__ == "__main__":
    main()