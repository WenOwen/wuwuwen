    def evaluate_model(self) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            self.logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è·å–é¢„æµ‹ç»“æœ
            if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                # äºŒåˆ†ç±»ï¼šè·å–æ¦‚ç‡é¢„æµ‹
                y_train_pred_proba = self.model.predict(self.X_train)
                y_val_pred_proba = self.model.predict(self.X_val)
                y_test_pred_proba = self.model.predict(self.X_test)
                
                # è½¬æ¢ä¸ºç±»åˆ«é¢„æµ‹ï¼ˆæ¦‚ç‡ > 0.5 ä¸ºçœ‹å¤šï¼‰
                y_train_pred = (y_train_pred_proba > 0.5).astype(int)
                y_val_pred = (y_val_pred_proba > 0.5).astype(int)
                y_test_pred = (y_test_pred_proba > 0.5).astype(int)
            else:
                # å›å½’ï¼šç›´æ¥é¢„æµ‹æ•°å€¼
                y_train_pred = self.model.predict(self.X_train)
                y_val_pred = self.model.predict(self.X_val)
                y_test_pred = self.model.predict(self.X_test)
                y_train_pred_proba = y_val_pred_proba = y_test_pred_proba = None
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            eval_config = self.config.get('evaluation', {})
            metrics_list = eval_config.get('metrics', ['rmse', 'mae', 'r2_score'])
            
            results = {}
            
            for split, y_true, y_pred, y_pred_proba in [
                ('train', self.y_train, y_train_pred, y_train_pred_proba),
                ('val', self.y_val, y_val_pred, y_val_pred_proba),
                ('test', self.y_test, y_test_pred, y_test_pred_proba)
            ]:
                split_metrics = {}
                
                for metric in metrics_list:
                    try:
                        if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                            # ğŸ¯ åˆ†ç±»æŒ‡æ ‡
                            if metric == 'accuracy':
                                value = np.mean(y_true == y_pred) * 100
                            elif metric == 'auc' and y_pred_proba is not None:
                                from sklearn.metrics import roc_auc_score
                                value = roc_auc_score(y_true, y_pred_proba)
                            elif metric == 'precision':
                                from sklearn.metrics import precision_score
                                value = precision_score(y_true, y_pred, zero_division=0)
                            elif metric == 'recall':
                                from sklearn.metrics import recall_score
                                value = recall_score(y_true, y_pred, zero_division=0)
                            elif metric == 'f1_score':
                                from sklearn.metrics import f1_score
                                value = f1_score(y_true, y_pred, zero_division=0)
                            elif metric == 'log_loss' and y_pred_proba is not None:
                                from sklearn.metrics import log_loss
                                # å¤„ç†æ¦‚ç‡è¾¹ç•Œé—®é¢˜
                                y_pred_proba_clipped = np.clip(y_pred_proba, 1e-15, 1-1e-15)
                                value = log_loss(y_true, y_pred_proba_clipped)
                            else:
                                continue
                        else:
                            # ğŸ“ˆ å›å½’æŒ‡æ ‡
                            if metric == 'rmse':
                                value = np.sqrt(mean_squared_error(y_true, y_pred))
                            elif metric == 'mae':
                                value = mean_absolute_error(y_true, y_pred)
                            elif metric == 'mape':
                                value = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                            elif metric == 'r2_score':
                                value = r2_score(y_true, y_pred)
                            elif metric == 'explained_variance':
                                value = explained_variance_score(y_true, y_pred)
                            elif metric == 'directional_accuracy':
                                # æ–¹å‘å‡†ç¡®ç‡ï¼ˆè‚¡ç¥¨é¢„æµ‹ç‰¹æœ‰æŒ‡æ ‡ï¼‰
                                direction_true = np.sign(y_true)
                                direction_pred = np.sign(y_pred)
                                value = np.mean(direction_true == direction_pred) * 100
                            else:
                                continue
                        
                        split_metrics[metric] = float(value)
                        
                    except Exception as e:
                        self.logger.warning(f"   è®¡ç®—æŒ‡æ ‡ {metric} å¤±è´¥: {e}")
                        continue
                
                results[split] = split_metrics
            
            # è¾“å‡ºç»“æœ
            prediction_type = "æ–¹å‘é¢„æµ‹" if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction' else "å›å½’é¢„æµ‹"
            self.logger.info(f"   ğŸ“ˆ è¯„ä¼°ç»“æœ ({prediction_type}):")
            for split, metrics in results.items():
                self.logger.info(f"     {split.upper()}:")
                for metric, value in metrics.items():
                    if metric in ['mape', 'directional_accuracy', 'accuracy']:
                        self.logger.info(f"       {metric}: {value:.2f}%")
                    else:
                        self.logger.info(f"       {metric}: {value:.6f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}