# -*- coding: utf-8 -*-
"""
å¢å¼ºAIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - æ”¯æŒè‚¡ç¥¨æ ‡è¯†å’Œæ¿å—æ•ˆåº”çš„æ¨¡å‹æ¶æ„
åŠŸèƒ½ï¼šå®ç°åŒ…å«Embeddingå±‚çš„å¤šæ¨¡å‹èåˆé¢„æµ‹ç³»ç»Ÿ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import joblib
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ æ¡†æ¶
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, 
    Embedding, Concatenate, MultiHeadAttention, LayerNormalization,
    Add, GlobalAveragePooling1D, Lambda, Reshape
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, RobustScaler

# æœºå™¨å­¦ä¹ æ¨¡å‹
import lightgbm as lgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# å¯¼å…¥åŸæœ‰åŸºç±»
from ai_models import StockPredictionModel, EnsembleModel


class EnhancedLSTMModel(StockPredictionModel):
    """
    å¢å¼ºLSTMæ¨¡å‹ - æ”¯æŒè‚¡ç¥¨å’Œæ¿å—Embedding
    """
    
    def __init__(self, sequence_length=60, n_features=50, n_stocks=100, n_sectors=20,
                 lstm_units=128, dropout_rate=0.3, embedding_dim=32):
        super().__init__("Enhanced_LSTM")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.n_stocks = n_stocks
        self.n_sectors = n_sectors
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.embedding_dim = embedding_dim
        self.scaler = StandardScaler()
        
        # è®°å½•åˆ†ç±»ç‰¹å¾çš„ç´¢å¼•
        self.stock_id_idx = None
        self.sector_id_idx = None
        self.categorical_indices = []
        self.numerical_indices = []
        
    def _build_model(self, feature_info: Dict):
        """æ„å»ºå¢å¼ºLSTMæ¨¡å‹"""
        
        # æ›´æ–°ç‰¹å¾æ˜ å°„
        self.n_stocks = feature_info.get('n_stocks', self.n_stocks)
        self.n_sectors = feature_info.get('n_sectors', self.n_sectors)
        
        # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„ç´¢å¼•
        all_cols = feature_info['all_cols']
        categorical_cols = feature_info['categorical_cols']
        numerical_cols = feature_info['numerical_cols']
        
        self.categorical_indices = [all_cols.index(col) for col in categorical_cols if col in all_cols]
        self.numerical_indices = [all_cols.index(col) for col in numerical_cols if col in all_cols]
        
        if 'stock_id' in all_cols:
            self.stock_id_idx = all_cols.index('stock_id')
        if 'sector_id' in all_cols:
            self.sector_id_idx = all_cols.index('sector_id')
        
        # è¾“å…¥å±‚
        main_input = Input(shape=(self.sequence_length, self.n_features), name='main_input')
        
        # åˆ†ç¦»æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾ - ä½¿ç”¨Keraså±‚è€ŒéTFå‡½æ•°
        if self.categorical_indices:
            # æå–åˆ†ç±»ç‰¹å¾ï¼ˆå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å€¼ï¼‰
            # ä½¿ç”¨Lambdaå±‚æ¥å®‰å…¨åœ°æå–ç‰¹å¾
            def extract_categorical_features(x):
                # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                last_step = x[:, -1, :]
                # æå–åˆ†ç±»ç‰¹å¾
                cat_features = tf.gather(last_step, self.categorical_indices, axis=1)
                return cat_features
            
            categorical_features = Lambda(extract_categorical_features)(main_input)
            
            # è‚¡ç¥¨ID Embedding
            if self.stock_id_idx is not None and 'stock_id' in categorical_cols:
                stock_id_idx = categorical_cols.index('stock_id')
                
                def extract_stock_id(x):
                    return tf.cast(x[:, stock_id_idx:stock_id_idx+1], tf.int32)
                
                stock_id = Lambda(extract_stock_id)(categorical_features)
                stock_id_flat = Reshape((1,))(stock_id)
                stock_embedding = Embedding(self.n_stocks, self.embedding_dim, name='stock_embedding')(stock_id_flat)
                stock_embedding = Reshape((self.embedding_dim,))(stock_embedding)
            else:
                # åˆ›å»ºé›¶å‘é‡
                def create_zero_embedding(x):
                    batch_size = tf.shape(x)[0]
                    return tf.zeros((batch_size, self.embedding_dim))
                
                stock_embedding = Lambda(create_zero_embedding)(main_input)
            
            # æ¿å—ID Embedding
            if self.sector_id_idx is not None and 'sector_id' in categorical_cols:
                sector_id_idx = categorical_cols.index('sector_id')
                
                def extract_sector_id(x):
                    return tf.cast(x[:, sector_id_idx:sector_id_idx+1], tf.int32)
                
                sector_id = Lambda(extract_sector_id)(categorical_features)
                sector_id_flat = Reshape((1,))(sector_id)
                sector_embedding = Embedding(self.n_sectors, self.embedding_dim, name='sector_embedding')(sector_id_flat)
                sector_embedding = Reshape((self.embedding_dim,))(sector_embedding)
            else:
                # åˆ›å»ºé›¶å‘é‡
                def create_zero_embedding(x):
                    batch_size = tf.shape(x)[0]
                    return tf.zeros((batch_size, self.embedding_dim))
                
                sector_embedding = Lambda(create_zero_embedding)(main_input)
            
            # å…¶ä»–åˆ†ç±»ç‰¹å¾
            other_categorical = []
            for i, col in enumerate(categorical_cols):
                if col not in ['stock_id', 'sector_id']:
                    def extract_feature(x, idx=i):
                        return x[:, idx:idx+1]
                    
                    feat = Lambda(lambda x: extract_feature(x, i))(categorical_features)
                    other_categorical.append(feat)
            
            if other_categorical:
                other_cat_concat = Concatenate()(other_categorical)
            else:
                # åˆ›å»ºå•ç»´é›¶å‘é‡
                def create_zero_feature(x):
                    batch_size = tf.shape(x)[0]
                    return tf.zeros((batch_size, 1))
                
                other_cat_concat = Lambda(create_zero_feature)(main_input)
        else:
            # å¦‚æœæ²¡æœ‰åˆ†ç±»ç‰¹å¾ï¼Œåˆ›å»ºé›¶å‘é‡
            def create_zero_embedding(x):
                batch_size = tf.shape(x)[0]
                return tf.zeros((batch_size, self.embedding_dim))
            
            def create_zero_feature(x):
                batch_size = tf.shape(x)[0]
                return tf.zeros((batch_size, 1))
            
            stock_embedding = Lambda(create_zero_embedding)(main_input)
            sector_embedding = Lambda(create_zero_embedding)(main_input)
            other_cat_concat = Lambda(create_zero_feature)(main_input)
        
        # æ•°å€¼ç‰¹å¾çš„LSTMå¤„ç† - ä½¿ç”¨Lambdaå±‚
        if self.numerical_indices:
            def extract_numerical_features(x):
                return tf.gather(x, self.numerical_indices, axis=2)
            
            numerical_features = Lambda(extract_numerical_features)(main_input)
        else:
            numerical_features = main_input
        
        # LSTMå±‚
        lstm_out = LSTM(self.lstm_units, return_sequences=True, name='lstm1')(numerical_features)
        lstm_out = Dropout(self.dropout_rate)(lstm_out)
        
        lstm_out = LSTM(self.lstm_units // 2, return_sequences=True, name='lstm2')(lstm_out)
        lstm_out = Dropout(self.dropout_rate)(lstm_out)
        
        lstm_out = LSTM(self.lstm_units // 4, return_sequences=False, name='lstm3')(lstm_out)
        lstm_out = Dropout(self.dropout_rate)(lstm_out)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        combined_features = Concatenate(name='feature_concat')([
            lstm_out, 
            stock_embedding, 
            sector_embedding, 
            other_cat_concat
        ])
        
        # å…¨è¿æ¥å±‚
        dense_out = Dense(64, activation='relu', name='dense1')(combined_features)
        dense_out = Dropout(self.dropout_rate)(dense_out)
        
        dense_out = Dense(32, activation='relu', name='dense2')(dense_out)
        dense_out = Dropout(self.dropout_rate)(dense_out)
        
        # è¾“å‡ºå±‚
        output = Dense(1, activation='sigmoid', name='output')(dense_out)
        
        model = Model(inputs=main_input, outputs=output)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_info=None, epochs=100, batch_size=32):
        """è®­ç»ƒå¢å¼ºLSTMæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        if feature_info is None:
            raise ValueError("éœ€è¦feature_infoæ¥æ„å»ºæ¨¡å‹")
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model(feature_info)
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=validation_data,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†ï¼ˆåªå¤„ç†æ•°å€¼ç‰¹å¾ï¼‰"""
        n_samples, n_timesteps, n_features = X.shape
        
        if self.numerical_indices:
            # åªæ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
            X_numerical = X[:, :, self.numerical_indices]
            X_categorical = X[:, :, self.categorical_indices] if self.categorical_indices else None
            
            # é‡å¡‘ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–
            X_num_2d = X_numerical.reshape(-1, len(self.numerical_indices))
            
            if fit_scaler:
                X_num_scaled_2d = self.scaler.fit_transform(X_num_2d)
            else:
                X_num_scaled_2d = self.scaler.transform(X_num_2d)
            
            # é‡å¡‘å›3D
            X_num_scaled = X_num_scaled_2d.reshape(n_samples, n_timesteps, len(self.numerical_indices))
            
            # é‡æ–°ç»„åˆç‰¹å¾
            if X_categorical is not None:
                # ä¿æŒåŸå§‹é¡ºåº
                X_scaled = np.zeros_like(X)
                X_scaled[:, :, self.numerical_indices] = X_num_scaled
                X_scaled[:, :, self.categorical_indices] = X_categorical
            else:
                X_scaled = X_num_scaled
        else:
            # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾ç´¢å¼•ï¼ŒæŒ‰åŸæ–¹å¼å¤„ç†
            X_2d = X.reshape(-1, n_features)
            if fit_scaler:
                X_scaled_2d = self.scaler.fit_transform(X_2d)
            else:
                X_scaled_2d = self.scaler.transform(X_2d)
            X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        
        return X_scaled


class EnhancedTransformerModel(StockPredictionModel):
    """
    å¢å¼ºTransformeræ¨¡å‹ - æ”¯æŒè‚¡ç¥¨å’Œæ¿å—Embedding
    """
    
    def __init__(self, sequence_length=60, n_features=50, n_stocks=100, n_sectors=20,
                 d_model=128, num_heads=8, num_layers=4, embedding_dim=32):
        super().__init__("Enhanced_Transformer")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.n_stocks = n_stocks
        self.n_sectors = n_sectors
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.embedding_dim = embedding_dim
        self.scaler = StandardScaler()
        
        # è®°å½•åˆ†ç±»ç‰¹å¾çš„ç´¢å¼•
        self.categorical_indices = []
        self.numerical_indices = []
        
    def _build_model(self, feature_info: Dict):
        """æ„å»ºå¢å¼ºTransformeræ¨¡å‹"""
        
        # æ›´æ–°ç‰¹å¾æ˜ å°„
        self.n_stocks = feature_info.get('n_stocks', self.n_stocks)
        self.n_sectors = feature_info.get('n_sectors', self.n_sectors)
        
        # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„ç´¢å¼•
        all_cols = feature_info['all_cols']
        categorical_cols = feature_info['categorical_cols']
        numerical_cols = feature_info['numerical_cols']
        
        self.categorical_indices = [all_cols.index(col) for col in categorical_cols if col in all_cols]
        self.numerical_indices = [all_cols.index(col) for col in numerical_cols if col in all_cols]
        
        # è¾“å…¥å±‚
        main_input = Input(shape=(self.sequence_length, self.n_features), name='main_input')
        
        # åˆ†ç¦»æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾ - ä½¿ç”¨Lambdaå±‚
        if self.numerical_indices:
            def extract_numerical_features(x):
                return tf.gather(x, self.numerical_indices, axis=2)
            
            numerical_features = Lambda(extract_numerical_features)(main_input)
        else:
            numerical_features = main_input
            
        # æ•°å€¼ç‰¹å¾çš„Transformerå¤„ç†
        # è¾“å…¥æŠ•å½±å±‚
        x = Dense(self.d_model)(numerical_features)
        
        # å¤šå±‚Transformerå—
        for _ in range(self.num_layers):
            # å¤šå¤´æ³¨æ„åŠ›
            attn_output = MultiHeadAttention(
                num_heads=self.num_heads, 
                key_dim=self.d_model // self.num_heads
            )(x, x)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = Add()([x, attn_output])
            x = LayerNormalization()(x)
            
            # å‰é¦ˆç½‘ç»œ
            ff_output = Dense(self.d_model * 2, activation='relu')(x)
            ff_output = Dense(self.d_model)(ff_output)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = Add()([x, ff_output])
            x = LayerNormalization()(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        transformer_out = GlobalAveragePooling1D()(x)
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆEmbeddingï¼‰- ä½¿ç”¨Lambdaå±‚
        if self.categorical_indices:
            # æå–åˆ†ç±»ç‰¹å¾
            def extract_categorical_features(x):
                last_step = x[:, -1, :]
                return tf.gather(last_step, self.categorical_indices, axis=1)
            
            categorical_features = Lambda(extract_categorical_features)(main_input)
            
            embeddings = []
            for i, col in enumerate(categorical_cols):
                if col == 'stock_id':
                    def extract_stock_id(x, idx=i):
                        return tf.cast(x[:, idx:idx+1], tf.int32)
                    
                    stock_id = Lambda(lambda x: extract_stock_id(x, i))(categorical_features)
                    stock_id_flat = Reshape((1,))(stock_id)
                    stock_emb = Embedding(self.n_stocks, self.embedding_dim)(stock_id_flat)
                    stock_emb = Reshape((self.embedding_dim,))(stock_emb)
                    embeddings.append(stock_emb)
                elif col == 'sector_id':
                    def extract_sector_id(x, idx=i):
                        return tf.cast(x[:, idx:idx+1], tf.int32)
                    
                    sector_id = Lambda(lambda x: extract_sector_id(x, i))(categorical_features)
                    sector_id_flat = Reshape((1,))(sector_id)
                    sector_emb = Embedding(self.n_sectors, self.embedding_dim)(sector_id_flat)
                    sector_emb = Reshape((self.embedding_dim,))(sector_emb)
                    embeddings.append(sector_emb)
                else:
                    # å…¶ä»–åˆ†ç±»ç‰¹å¾ç›´æ¥ä½¿ç”¨
                    def extract_feature(x, idx=i):
                        return x[:, idx:idx+1]
                    
                    feat = Lambda(lambda x: extract_feature(x, i))(categorical_features)
                    embeddings.append(feat)
            
            if embeddings:
                categorical_concat = Concatenate()(embeddings)
            else:
                def create_zero_feature(x):
                    batch_size = tf.shape(x)[0]
                    return tf.zeros((batch_size, 1))
                
                categorical_concat = Lambda(create_zero_feature)(main_input)
            
            # åˆå¹¶ç‰¹å¾
            combined_features = Concatenate()([transformer_out, categorical_concat])
        else:
            combined_features = transformer_out
        
        # åˆ†ç±»å¤´
        x = Dense(64, activation='relu')(combined_features)
        x = Dropout(0.3)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(main_input, outputs)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_info=None, epochs=100, batch_size=32):
        """è®­ç»ƒå¢å¼ºTransformeræ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        if feature_info is None:
            raise ValueError("éœ€è¦feature_infoæ¥æ„å»ºæ¨¡å‹")
        
        # æ•°æ®é¢„å¤„ç†ï¼ˆä¸EnhancedLSTMModelç›¸åŒï¼‰
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model(feature_info)
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=validation_data,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†ï¼ˆä¸EnhancedLSTMModelç›¸åŒï¼‰"""
        n_samples, n_timesteps, n_features = X.shape
        
        if self.numerical_indices:
            # åªæ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
            X_numerical = X[:, :, self.numerical_indices]
            X_categorical = X[:, :, self.categorical_indices] if self.categorical_indices else None
            
            # é‡å¡‘ä¸º2Dè¿›è¡Œæ ‡å‡†åŒ–
            X_num_2d = X_numerical.reshape(-1, len(self.numerical_indices))
            
            if fit_scaler:
                X_num_scaled_2d = self.scaler.fit_transform(X_num_2d)
            else:
                X_num_scaled_2d = self.scaler.transform(X_num_2d)
            
            # é‡å¡‘å›3D
            X_num_scaled = X_num_scaled_2d.reshape(n_samples, n_timesteps, len(self.numerical_indices))
            
            # é‡æ–°ç»„åˆç‰¹å¾
            if X_categorical is not None:
                X_scaled = np.zeros_like(X)
                X_scaled[:, :, self.numerical_indices] = X_num_scaled
                X_scaled[:, :, self.categorical_indices] = X_categorical
            else:
                X_scaled = X_num_scaled
        else:
            # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾ç´¢å¼•ï¼ŒæŒ‰åŸæ–¹å¼å¤„ç†
            X_2d = X.reshape(-1, n_features)
            if fit_scaler:
                X_scaled_2d = self.scaler.fit_transform(X_2d)
            else:
                X_scaled_2d = self.scaler.transform(X_2d)
            X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        
        return X_scaled


class EnhancedLightGBMModel(StockPredictionModel):
    """
    å¢å¼ºLightGBMæ¨¡å‹ - ç›´æ¥ä½¿ç”¨åˆ†ç±»ç‰¹å¾
    """
    
    def __init__(self, **params):
        super().__init__("Enhanced_LightGBM")
        self.params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 1000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'verbose': -1,
            **params
        }
        self.scaler = RobustScaler()
        self.categorical_indices = []
        self.numerical_indices = []
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_info=None, **kwargs):
        """è®­ç»ƒå¢å¼ºLightGBMæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        if feature_info is not None:
            all_cols = feature_info['all_cols']
            categorical_cols = feature_info['categorical_cols']
            numerical_cols = feature_info['numerical_cols']
            
            self.categorical_indices = [all_cols.index(col) for col in categorical_cols if col in all_cols]
            self.numerical_indices = [all_cols.index(col) for col in numerical_cols if col in all_cols]
        
        # Flatten 3D to 2D
        if len(X_train.shape) == 3:
            X_train_2d = X_train.reshape(X_train.shape[0], -1)
        else:
            X_train_2d = X_train
        
        # åˆ†ç¦»æ•°å€¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
        if self.numerical_indices:
            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
            last_step_features = X_train[:, -1, :] if len(X_train.shape) == 3 else X_train
            
            X_numerical = last_step_features[:, self.numerical_indices]
            X_categorical = last_step_features[:, self.categorical_indices] if self.categorical_indices else np.array([]).reshape(len(X_train), 0)
            
            # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
            X_num_scaled = self.scaler.fit_transform(X_numerical)
            
            # åˆå¹¶ç‰¹å¾
            if X_categorical.shape[1] > 0:
                X_train_processed = np.hstack([X_num_scaled, X_categorical])
            else:
                X_train_processed = X_num_scaled
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾ä¿¡æ¯ï¼ŒæŒ‰åŸæ–¹å¼å¤„ç†
            X_train_processed = self.scaler.fit_transform(X_train_2d)
        
        # éªŒè¯é›†å¤„ç†
        eval_set = None
        if X_val is not None and y_val is not None:
            if len(X_val.shape) == 3:
                last_step_val = X_val[:, -1, :]
            else:
                last_step_val = X_val
                
            if self.numerical_indices:
                X_val_numerical = last_step_val[:, self.numerical_indices]
                X_val_categorical = last_step_val[:, self.categorical_indices] if self.categorical_indices else np.array([]).reshape(len(X_val), 0)
                
                X_val_num_scaled = self.scaler.transform(X_val_numerical)
                
                if X_val_categorical.shape[1] > 0:
                    X_val_processed = np.hstack([X_val_num_scaled, X_val_categorical])
                else:
                    X_val_processed = X_val_num_scaled
            else:
                X_val_2d = X_val.reshape(X_val.shape[0], -1) if len(X_val.shape) == 3 else X_val
                X_val_processed = self.scaler.transform(X_val_2d)
                
            eval_set = [(X_val_processed, y_val)]
        
        # è®­ç»ƒæ¨¡å‹
        self.model = lgb.LGBMClassifier(**self.params)
        
        # LightGBMè®­ç»ƒ
        try:
            self.model.fit(
                X_train_processed, y_train,
                eval_set=eval_set,
                callbacks=[lgb.early_stopping(50, verbose=False)] if eval_set else None
            )
        except Exception as e:
            print(f"  âš ï¸ æ—©åœè®­ç»ƒå¤±è´¥: {e}, å°è¯•åŸºæœ¬è®­ç»ƒ")
            # ç®€å•è®­ç»ƒæ–¹å¼ï¼ˆæ— æ—©åœï¼‰
            try:
                self.model = lgb.LGBMClassifier(**self.params)
                self.model.fit(X_train_processed, y_train)
            except Exception as e2:
                print(f"  âŒ åŸºæœ¬è®­ç»ƒä¹Ÿå¤±è´¥: {e2}")
                raise e2
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")


class EnhancedEnsembleModel(EnsembleModel):
    """
    å¢å¼ºé›†æˆæ¨¡å‹ - æ”¯æŒç‰¹å¾ä¿¡æ¯ä¼ é€’
    """
    
    def __init__(self, model_weights: Dict[str, float] = None):
        super().__init__(model_weights)
        self.feature_info = None
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_info=None, **kwargs):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆä¼ é€’feature_infoï¼‰"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºé›†æˆæ¨¡å‹...")
        
        self.feature_info = feature_info
        
        # è®­ç»ƒå„ä¸ªå­æ¨¡å‹
        for model_name, model in self.models.items():
            print(f"\n--- è®­ç»ƒ {model_name} ---")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒfeature_info
            if hasattr(model, 'fit') and 'feature_info' in model.fit.__code__.co_varnames:
                model.fit(X_train, y_train, X_val, y_val, feature_info=feature_info, **kwargs)
            else:
                model.fit(X_train, y_train, X_val, y_val, **kwargs)
        
        # åŠ¨æ€è°ƒæ•´æƒé‡ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½ï¼‰
        if X_val is not None and y_val is not None:
            self._adjust_weights(X_val, y_val)
        
        print("âœ… å¢å¼ºé›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")


def create_enhanced_ensemble_model(sequence_length=60, n_features=50, 
                                 n_stocks=100, n_sectors=20) -> EnhancedEnsembleModel:
    """
    åˆ›å»ºå¢å¼ºçš„é›†æˆæ¨¡å‹
    
    Args:
        sequence_length: æ—¶åºé•¿åº¦
        n_features: ç‰¹å¾æ•°é‡
        n_stocks: è‚¡ç¥¨æ•°é‡
        n_sectors: æ¿å—æ•°é‡
        
    Returns:
        EnhancedEnsembleModelå®ä¾‹
    """
    
    # åˆ›å»ºé›†æˆæ¨¡å‹
    ensemble = EnhancedEnsembleModel()
    
    # æ·»åŠ å¢å¼ºçš„å­æ¨¡å‹
    enhanced_lstm = EnhancedLSTMModel(sequence_length, n_features, n_stocks, n_sectors)
    enhanced_transformer = EnhancedTransformerModel(sequence_length, n_features, n_stocks, n_sectors)
    enhanced_lgb = EnhancedLightGBMModel()
    
    # è¿˜å¯ä»¥ä½¿ç”¨åŸæœ‰çš„CNN-LSTMæ¨¡å‹
    from ai_models import CNNLSTMModel
    cnn_lstm = CNNLSTMModel(sequence_length, n_features)
    
    ensemble.add_model(enhanced_lstm)
    ensemble.add_model(enhanced_transformer)
    ensemble.add_model(enhanced_lgb)
    ensemble.add_model(cnn_lstm)
    
    return ensemble


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºæ¨¡å‹
    print("ğŸ§ª æµ‹è¯•å¢å¼ºæ¨¡å‹...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    n_samples, sequence_length, n_features = 1000, 60, 45
    n_stocks, n_sectors = 50, 10
    
    X_train = np.random.randn(n_samples, sequence_length, n_features)
    y_train = np.random.randint(0, 2, n_samples)
    
    # æ¨¡æ‹Ÿfeature_info
    feature_info = {
        'numerical_cols': [f'feature_{i}' for i in range(40)],
        'categorical_cols': ['stock_id', 'sector_id', 'sector_type', 'is_market_sensitive', 'is_growth_stock'],
        'all_cols': [f'feature_{i}' for i in range(40)] + ['stock_id', 'sector_id', 'sector_type', 'is_market_sensitive', 'is_growth_stock'],
        'n_stocks': n_stocks,
        'n_sectors': n_sectors
    }
    
    # æµ‹è¯•å¢å¼ºLSTMæ¨¡å‹
    lstm_model = EnhancedLSTMModel(sequence_length, n_features, n_stocks, n_sectors)
    print("âœ… å¢å¼ºæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("ğŸ‰ å¢å¼ºæ¨¡å‹æµ‹è¯•å®Œæˆï¼")