# -*- coding: utf-8 -*-
"""
AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - è®­ç»ƒç®¡é“æ¨¡å—
åŠŸèƒ½ï¼šè‡ªåŠ¨åŒ–æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–å’Œéƒ¨ç½²ç®¡é“
"""

import os
# è®¾ç½®TensorFlowç¯å¢ƒå˜é‡ï¼Œå±è”½è°ƒè¯•ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªæ˜¾ç¤ºé”™è¯¯
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # å…³é—­oneDNNæç¤º
os.environ['TF_DISABLE_MKL'] = '1'  # ç¦ç”¨MKLä¼˜åŒ–æç¤º

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import joblib
from datetime import datetime, timedelta
import logging
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

# å¯¼å…¥å¤„ç† - æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥
try:
    from .feature_engineering import FeatureEngineering
    from .ai_models import EnsembleModel, create_ensemble_model, LightGBMModel
    from .enhanced_ai_models import create_enhanced_ensemble_model
    from .feature_cache import BatchFeatureProcessor
    from .training_report_generator import TrainingReportGenerator
    from ..utils.gpu_config import setup_dual_gpu, get_optimal_batch_size
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
    sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'utils'))
    
    from feature_engineering import FeatureEngineering
    from ai_models import EnsembleModel, create_ensemble_model, LightGBMModel
    from enhanced_ai_models import create_enhanced_ensemble_model
    from feature_cache import BatchFeatureProcessor
    from training_report_generator import TrainingReportGenerator
    try:
        from gpu_config import setup_dual_gpu, get_optimal_batch_size
    except ImportError:
        # å¦‚æœGPUé…ç½®ä¸å¯ç”¨ï¼Œæä¾›é»˜è®¤å®ç°
        def setup_dual_gpu():
            return None
        def get_optimal_batch_size(base_size, gpu_count):
            return base_size

# LightGBMå¯¼å…¥
import lightgbm as lgb

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainingPipeline:
    """
    æ¨¡å‹è®­ç»ƒç®¡é“ç±»
    è´Ÿè´£æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–ç­‰
    """
    
    def __init__(self, data_dir: str = None, model_dir: str = "models", 
                 enable_batch_cache: bool = True, cache_workers: int = 1):
        # è‡ªåŠ¨å¯»æ‰¾æ•°æ®ç›®å½•
        if data_dir is None:
            possible_dirs = ["data/datas_em", "datas_em", "data_em", "data/datas_QMT", "datas_QMT", "financial_csv"]
            for dir_path in possible_dirs:
                if os.path.exists(dir_path):
                    csv_files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]
                    if csv_files:
                        data_dir = dir_path
                        logger.info(f"è‡ªåŠ¨å‘ç°æ•°æ®ç›®å½•: {data_dir}ï¼ŒåŒ…å« {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
                        break
            
            if data_dir is None:
                data_dir = "data/datas_em"  # é»˜è®¤å€¼
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®ç›®å½•ï¼Œä½¿ç”¨é»˜è®¤: {data_dir}")
        self.data_dir = data_dir
        self.model_dir = model_dir
        self.feature_engineer = FeatureEngineering(enable_cache=False)
        self.models = {}
        self.performance_history = []
        self.report_generator = TrainingReportGenerator()
        
        # ğŸš€ é…ç½®åŒGPUç¯å¢ƒ
        self.gpu_strategy = setup_dual_gpu()
        
        # ğŸš€ æ‰¹é‡ç¼“å­˜å¤„ç†å™¨
        self.enable_batch_cache = enable_batch_cache
        # å½»åº•ç¦ç”¨æ‰¹é‡ç¼“å­˜
        self.batch_processor = None
        if enable_batch_cache:
            logger.warning("âš ï¸ æ‰¹é‡ç¼“å­˜å·²è¢«å¼ºåˆ¶ç¦ç”¨")
        logger.info("ğŸ“Š æ‰¹é‡ç¼“å­˜å¤„ç†å™¨: å·²ç¦ç”¨")
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        os.makedirs(model_dir, exist_ok=True)
        
        # è®­ç»ƒé…ç½® - å®Œæ•´è®­ç»ƒä¼˜åŒ–å‚æ•°
        self.config = {
            'sequence_length': 60,
            'prediction_days': [1, 3, 5],
            'train_test_split': 0.8,
            'validation_split': 0.2,
            'min_samples': 300,  # é™ä½æœ€å°‘æ ·æœ¬æ•°ï¼ŒåŒ…å«æ›´å¤šè‚¡ç¥¨
            'performance_threshold': 0.55,  # æœ€ä½å‡†ç¡®ç‡è¦æ±‚
            
            # ğŸš€ å®Œæ•´è®­ç»ƒå‚æ•°é…ç½®
            'training_params': {
                'epochs': 150,  # å®Œæ•´è®­ç»ƒè½®æ¬¡
                'batch_size': 64,  # ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
                'early_stopping_patience': 15,  # æ—©åœè€å¿ƒå€¼
                'cv_epochs': 50,  # äº¤å‰éªŒè¯è½®æ¬¡
                'cv_folds': 3,  # äº¤å‰éªŒè¯æŠ˜æ•°
            },
            
            # LightGBMä¸“ç”¨é…ç½® - å®Œæ•´è®­ç»ƒä¼˜åŒ–
            'lightgbm_config': {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_threads': 8,  # å¢åŠ çº¿ç¨‹æ•°
                'n_estimators': 500,  # å¢åŠ æ ‘çš„æ•°é‡
                'learning_rate': 0.05,  # é™ä½å­¦ä¹ ç‡ï¼Œæé«˜ç¨³å®šæ€§
                'max_depth': 8,  # å¢åŠ æ ‘æ·±åº¦
                'num_leaves': 127,  # å¢åŠ å¶å­æ•°
                'feature_fraction': 0.8,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'min_child_samples': 20,
                'early_stopping_rounds': 100,  # å¢åŠ æ—©åœè½®æ¬¡
                'verbose': -1,
                'random_state': 42
            }
        }
    
    def load_stock_data(self, stock_code: str) -> pd.DataFrame:
        """åŠ è½½å•åªè‚¡ç¥¨æ•°æ®"""
        file_path = os.path.join(self.data_dir, f"{stock_code}.csv")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è‚¡ç¥¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        df = pd.read_csv(file_path)
        df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
        df = df.sort_values('äº¤æ˜“æ—¥æœŸ').reset_index(drop=True)
        
        # é™é»˜åŠ è½½ï¼Œä¸æ‰“å°æ¯åªè‚¡ç¥¨çš„ä¿¡æ¯
        return df
    
    def prepare_training_data(self, stock_codes: List[str], 
                            prediction_days: int = 1) -> Tuple[np.ndarray, np.ndarray, List[str], Dict]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ® - æ”¯æŒæ‰¹é‡ç¼“å­˜ä¼˜åŒ–
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            prediction_days: é¢„æµ‹å¤©æ•°
            
        Returns:
            X, y, feature_names, feature_info
        """
        logger.info(f"ğŸš€ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        
        # ç¼“å­˜å·²ç¦ç”¨ï¼Œåªä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼ˆç¼“å­˜å·²ç¦ç”¨ï¼‰")
        return self._prepare_training_data_traditional(stock_codes, prediction_days)
    
    def _prepare_training_data_with_batch_cache(self, stock_codes: List[str], 
                                              prediction_days: int = 1) -> Tuple[np.ndarray, np.ndarray, List[str], Dict]:
        """
        ä½¿ç”¨æ‰¹é‡ç¼“å­˜å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        logger.info("ğŸ“ˆ ä½¿ç”¨æ‰¹é‡ç¼“å­˜å¤„ç†å™¨è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
        
        # å®šä¹‰æ•°æ®åŠ è½½å‡½æ•°
        def data_loader(stock_code: str) -> pd.DataFrame:
            try:
                df = self.load_stock_data(stock_code)
                if len(df) < self.config['min_samples']:
                    logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡")
                    return None
                return df
            except Exception as e:
                logger.error(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ•°æ®å¤±è´¥: {e}")
                return None
        
        # æ‰¹é‡å¤„ç†ç‰¹å¾å·¥ç¨‹
        features_results = self.batch_processor.process_stocks_with_cache(
            stock_codes=stock_codes,
            data_loader_func=data_loader,
            show_progress=True
        )
        
        if not features_results:
            raise ValueError("æ‰¹é‡ç‰¹å¾å¤„ç†æœªè¿”å›ä»»ä½•æœ‰æ•ˆæ•°æ®")
        
        # å‡†å¤‡æ¨¡å‹æ•°æ®
        logger.info("ğŸ”§ å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®...")
        all_X, all_y = [], []
        feature_names = None
        feature_info = None
        
        for stock_code, df_features in features_results.items():
            try:
                # å‡†å¤‡æ¨¡å‹æ•°æ®
                X, y, feature_names_temp, feature_info_temp = self.feature_engineer.prepare_model_data(
                    df_features, 
                    prediction_days=prediction_days,
                    lookback_window=self.config['sequence_length']
                )
                
                if len(X) > 0:
                    all_X.append(X)
                    all_y.append(y)
                    if feature_names is None:
                        feature_names = feature_names_temp
                    if feature_info is None:
                        feature_info = feature_info_temp
                    logger.info(f"è‚¡ç¥¨ {stock_code} ç”Ÿæˆæ ·æœ¬ {len(X)} ä¸ª")
                
            except Exception as e:
                logger.error(f"å‡†å¤‡è‚¡ç¥¨ {stock_code} æ¨¡å‹æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                continue
        
        if not all_X:
            raise ValueError("æ²¡æœ‰æˆåŠŸå‡†å¤‡çš„æ¨¡å‹æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        X_combined = np.vstack(all_X)
        y_combined = np.hstack(all_y)
        
        logger.info(f"âœ… æ‰¹é‡è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
        logger.info(f"   æ€»è®¡æ ·æœ¬æ•°: {len(X_combined)}")
        logger.info(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        logger.info(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {y_combined.mean():.3f}")
        
        # æ˜¾ç¤ºç¼“å­˜æ•ˆæœç»Ÿè®¡
        if hasattr(self.batch_processor.cache, 'cache_hits'):
            total_access = self.batch_processor.cache.cache_hits + self.batch_processor.cache.cache_misses
            if total_access > 0:
                hit_rate = self.batch_processor.cache.cache_hits / total_access
                logger.info(f"   ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1%}")
                logger.info(f"   ç¼“å­˜åŠ é€Ÿ: ç¬¬äºŒæ¬¡è¿è¡Œé¢„è®¡æé€Ÿ {hit_rate*8:.1f}x")
        
        return X_combined, y_combined, feature_names, feature_info
    
    def _prepare_training_data_traditional(self, stock_codes: List[str], 
                                         prediction_days: int = 1) -> Tuple[np.ndarray, np.ndarray, List[str], Dict]:
        """
        ä¼ ç»Ÿæ–¹å¼å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆçœŸæ­£çš„åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
        """
        print(f"ğŸ“Š å¤„ç† {len(stock_codes)} åªè‚¡ç¥¨çš„ç‰¹å¾å·¥ç¨‹...")
        
        # åˆ†æ‰¹å¤„ç†é…ç½®
        batch_size = 50  # æ¯æ‰¹å¤„ç†50åªè‚¡ç¥¨
        max_samples_per_stock = 500  # æ¯åªè‚¡ç¥¨æœ€å¤§æ ·æœ¬æ•°
        max_total_samples = 50000  # æ€»æœ€å¤§æ ·æœ¬æ•°
        # å†…å­˜ä¼˜åŒ–ï¼šå‡å°‘å›æœ›çª—å£
        optimized_lookback = min(30, self.config['sequence_length'])
        
        feature_names = None
        feature_info = None
        processed_count = 0
        total_samples = 0
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨ï¼ˆä½¿ç”¨åˆ—è¡¨é€æ­¥æ„å»ºï¼Œé¿å…å¤§æ•°ç»„é¢„åˆ†é…ï¼‰
        batch_results = []
        
        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
        for batch_start in range(0, len(stock_codes), batch_size):
            batch_end = min(batch_start + batch_size, len(stock_codes))
            batch_stocks = stock_codes[batch_start:batch_end]
            batch_num = batch_start // batch_size + 1
            total_batches = (len(stock_codes) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}: å¤„ç†è‚¡ç¥¨ {batch_start+1}-{batch_end}")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_X, batch_y = [], []
            
            for i, stock_code in enumerate(batch_stocks):
                try:
                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯åªè‚¡ç¥¨éƒ½æ˜¾ç¤ºï¼Œä¸ä½¿ç”¨\rè¦†ç›–ï¼‰
                    progress = f"[{batch_start + i + 1:4d}/{len(stock_codes):4d}] {stock_code}"
                    print(f"{progress}", end='', flush=True)
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ ·æœ¬æ•°é™åˆ¶
                    if total_samples >= max_total_samples:
                        print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶({max_total_samples})ï¼Œåœæ­¢å¤„ç†")
                        break
                    
                    # åŠ è½½è‚¡ç¥¨æ•°æ®
                    df = self.load_stock_data(stock_code)
                    
                    if len(df) < self.config['min_samples']:
                        print(f" âŒæ•°æ®ä¸è¶³")
                        continue
                    
                    # é™åˆ¶å•åªè‚¡ç¥¨çš„æ•°æ®é‡ä»¥æ§åˆ¶å†…å­˜
                    if len(df) > 2000:
                        df = df.tail(2000)  # åªä¿ç•™æœ€è¿‘2000æ¡è®°å½•
                    
                    # ç‰¹å¾å·¥ç¨‹
                    df_features = self.feature_engineer.create_all_features(df, stock_code)
                    
                    # å‡†å¤‡æ¨¡å‹æ•°æ®
                    X, y, feature_names_temp, feature_info_temp = self.feature_engineer.prepare_model_data(
                        df_features, 
                        prediction_days=prediction_days,
                        lookback_window=optimized_lookback
                    )
                    
                    if len(X) > 0:
                        # é™åˆ¶æ¯åªè‚¡ç¥¨çš„æ ·æœ¬æ•°
                        if len(X) > max_samples_per_stock:
                            indices = np.random.choice(len(X), max_samples_per_stock, replace=False)
                            X = X[indices]
                            y = y[indices]
                        
                        batch_X.append(X)
                        batch_y.append(y)
                        total_samples += len(X)
                        
                        if feature_names is None:
                            feature_names = feature_names_temp
                        if feature_info is None:
                            feature_info = feature_info_temp
                        processed_count += 1
                        
                        print(f" âœ…{len(X)}æ ·æœ¬")
                    else:
                        print(f" âŒæ— æœ‰æ•ˆæ ·æœ¬")
                    
                    # æ¸…ç†å†…å­˜
                    del df, df_features, X, y
                    
                except Exception as e:
                    print(f" âŒå¤„ç†å¤±è´¥: {str(e)[:20]}")
                    continue
            
            # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
            if batch_X:
                batch_X_combined = np.vstack(batch_X)
                batch_y_combined = np.hstack(batch_y)
                batch_results.append((batch_X_combined, batch_y_combined))
                
                print(f"\n   âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {len(batch_X_combined)} ä¸ªæ ·æœ¬")
                
                # æ¸…ç†æ‰¹æ¬¡ä¸´æ—¶æ•°æ®
                del batch_X, batch_y, batch_X_combined, batch_y_combined
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ ·æœ¬é™åˆ¶
            if total_samples >= max_total_samples:
                break
        
        print()  # æ¢è¡Œ
        
        if not batch_results:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†çš„è‚¡ç¥¨æ•°æ®")
        
        # é€æ­¥åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœï¼ˆé¿å…å†…å­˜å³°å€¼ï¼‰
        print("ğŸ”„ åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®...")
        X_combined = None
        y_combined = None
        
        for i, (batch_X, batch_y) in enumerate(batch_results):
            if X_combined is None:
                X_combined = batch_X.copy()
                y_combined = batch_y.copy()
            else:
                X_combined = np.vstack([X_combined, batch_X])
                y_combined = np.hstack([y_combined, batch_y])
            
            # æ¸…ç†å·²å¤„ç†çš„æ‰¹æ¬¡
            del batch_X, batch_y
            
            # æ¯å¤„ç†5ä¸ªæ‰¹æ¬¡è¿›è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
            if (i + 1) % 5 == 0:
                gc.collect()
                print(f"   å·²åˆå¹¶ {i+1}/{len(batch_results)} ä¸ªæ‰¹æ¬¡")
        
        # æ¸…ç†æ‰¹æ¬¡ç»“æœåˆ—è¡¨
        del batch_results
        gc.collect()
        
        # æœ€ç»ˆæ•°æ®é‡‡æ ·ï¼ˆå¦‚æœä»ç„¶å¤ªå¤§ï¼‰
        if len(X_combined) > max_total_samples:
            print(f"ğŸ”½ æœ€ç»ˆé‡‡æ ·: {len(X_combined)} -> {max_total_samples}")
            indices = np.random.choice(len(X_combined), max_total_samples, replace=False)
            X_combined = X_combined[indices]
            y_combined = y_combined[indices]
        
        print(f"âœ… åˆ†æ‰¹ç‰¹å¾å·¥ç¨‹å®Œæˆ: {processed_count}/{len(stock_codes)} åªè‚¡ç¥¨, {len(X_combined)} ä¸ªæ ·æœ¬, {len(feature_names)} ä¸ªç‰¹å¾")
        print(f"   åºåˆ—é•¿åº¦: {optimized_lookback} (åŸå§‹: {self.config['sequence_length']})")
        print(f"   å†…å­˜å ç”¨ä¼°è®¡: {X_combined.nbytes / 1024 / 1024:.1f} MB")
        
        # æ›´æ–°feature_infoä¸­çš„åºåˆ—é•¿åº¦ä¿¡æ¯
        if feature_info is None:
            feature_info = {}
        feature_info['actual_sequence_length'] = optimized_lookback
        feature_info['original_sequence_length'] = self.config['sequence_length']
        
        return X_combined, y_combined, feature_names, feature_info
    
    def get_available_stocks(self, limit: int = None) -> List[str]:
        """
        è·å–å¯ç”¨çš„è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            limit: é™åˆ¶è‚¡ç¥¨æ•°é‡
            
        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        if not os.path.exists(self.data_dir):
            logger.warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return []
        
        csv_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]
        stock_codes = [f.replace('.csv', '') for f in csv_files]
        
        if limit:
            stock_codes = stock_codes[:limit]
        
        # éªŒè¯æ•°æ®è´¨é‡
        valid_stocks = []
        for stock_code in stock_codes:
            try:
                df = self.load_stock_data(stock_code)
                if len(df) >= self.config['min_samples']:
                    valid_stocks.append(stock_code)
                if limit and len(valid_stocks) >= limit:
                    break
            except Exception:
                continue
        
        logger.info(f"å‘ç° {len(valid_stocks)} åªæœ‰æ•ˆè‚¡ç¥¨ï¼ˆæ€»å…± {len(csv_files)} ä¸ªæ–‡ä»¶ï¼‰")
        return valid_stocks
    
    def warm_up_cache(self, stock_codes: List[str], show_progress: bool = True):
        """
        é¢„çƒ­ç¼“å­˜ - å·²ç¦ç”¨ç¼“å­˜åŠŸèƒ½
        """
        logger.warning("âš ï¸ ç¼“å­˜åŠŸèƒ½å·²è¢«ç¦ç”¨ï¼Œè·³è¿‡é¢„çƒ­æ­¥éª¤")
        return
    
    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        æ—¶é—´åºåˆ—æ•°æ®åˆ†å‰²
        """
        split_idx = int(len(X) * self.config['train_test_split'])
        
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}, æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test)}")
        
        return X_train, X_test, y_train, y_test
    
    def cross_validate_model(self, X: np.ndarray, y: np.ndarray, 
                           model: Optional[EnsembleModel] = None,
                           feature_info: Optional[Dict] = None,
                           n_splits: int = None) -> Dict[str, float]:
        """
        æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            model: æ¨¡å‹å®ä¾‹
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        # ä½¿ç”¨é…ç½®ä¸­çš„äº¤å‰éªŒè¯æŠ˜æ•°
        if n_splits is None:
            n_splits = self.config['training_params']['cv_folds']
        
        logger.info(f"å¼€å§‹ {n_splits} æŠ˜äº¤å‰éªŒè¯...")
        
        if model is None:
            # ä½¿ç”¨LightGBMé›†æˆæ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯
            model = self.create_lightgbm_ensemble_model(
                n_features=X.shape[-1],
                feature_info=feature_info
            )
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        scores = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'auc': []
        }
        
        fold = 1
        for train_idx, val_idx in tscv.split(X):
            logger.info(f"äº¤å‰éªŒè¯ æŠ˜ {fold}/{n_splits}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            # ğŸš€ äº¤å‰éªŒè¯ä¼˜åŒ–é…ç½®
            cv_epochs = self.config['training_params']['cv_epochs']  # ä½¿ç”¨é…ç½®çš„äº¤å‰éªŒè¯è½®æ¬¡
            cv_batch_size = get_optimal_batch_size(self.config['training_params']['batch_size'], 2)  # åŠ¨æ€è®¡ç®—æœ€ä¼˜batch size
            
            if feature_info and hasattr(model, 'fit') and 'feature_info' in model.fit.__code__.co_varnames:
                model.fit(X_train_fold, y_train_fold, X_val_fold, y_val_fold, 
                         feature_info=feature_info, epochs=cv_epochs, batch_size=cv_batch_size)
            else:
                model.fit(X_train_fold, y_train_fold, X_val_fold, y_val_fold, 
                         epochs=cv_epochs, batch_size=cv_batch_size)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = model.predict(X_val_fold)
            y_proba = model.predict_proba(X_val_fold)[:, 1]
            
            # è®¡ç®—æŒ‡æ ‡
            scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
            scores['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))
            scores['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))
            scores['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))
            scores['auc'].append(roc_auc_score(y_val_fold, y_proba))
            
            fold += 1
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_scores = {metric: np.mean(values) for metric, values in scores.items()}
        std_scores = {f"{metric}_std": np.std(values) for metric, values in scores.items()}
        
        result = {**avg_scores, **std_scores}
        
        logger.info("äº¤å‰éªŒè¯ç»“æœ:")
        for metric, score in avg_scores.items():
            logger.info(f"  {metric}: {score:.4f} Â± {std_scores[f'{metric}_std']:.4f}")
        
        return result
    
    def hyperparameter_optimization(self, X_train: np.ndarray, y_train: np.ndarray,
                                  X_val: np.ndarray, y_val: np.ndarray,
                                  n_trials: int = 50) -> Dict:
        """
        è¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_val, y_val: éªŒè¯æ•°æ®
            n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            
        Returns:
            æœ€ä¼˜å‚æ•°å­—å…¸
        """
        logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œè¯•éªŒæ¬¡æ•°: {n_trials}")
        
        def objective(trial):
            # LSTMå‚æ•°
            lstm_units = trial.suggest_int('lstm_units', 64, 256, step=64)
            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
            
            # LightGBMå‚æ•° - æ›´è¯¦ç»†çš„è¶…å‚æ•°
            lgb_max_depth = trial.suggest_int('lgb_max_depth', 3, 10)
            lgb_learning_rate = trial.suggest_float('lgb_learning_rate', 0.01, 0.3)
            lgb_n_estimators = trial.suggest_int('lgb_n_estimators', 100, 1000, step=100)
            lgb_num_leaves = trial.suggest_int('lgb_num_leaves', 20, 300)
            lgb_feature_fraction = trial.suggest_float('lgb_feature_fraction', 0.6, 1.0)
            lgb_bagging_fraction = trial.suggest_float('lgb_bagging_fraction', 0.6, 1.0)
            lgb_bagging_freq = trial.suggest_int('lgb_bagging_freq', 1, 10)
            lgb_min_child_samples = trial.suggest_int('lgb_min_child_samples', 10, 100)
            
            # Transformerå‚æ•°
            d_model = trial.suggest_int('d_model', 64, 256, step=64)
            num_heads = trial.suggest_int('num_heads', 4, 16, step=4)
            
            try:
                # åˆ›å»ºæ¨¡å‹ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦ä¼ å…¥å‚æ•°ï¼‰
                try:
                    from .ai_models import LSTMModel, LightGBMModel, TransformerModel, CNNLSTMModel
                except ImportError:
                    from ai_models import LSTMModel, LightGBMModel, TransformerModel, CNNLSTMModel
                
                ensemble = EnsembleModel()
                
                # æ·»åŠ ä¼˜åŒ–åçš„æ¨¡å‹ - LightGBMä½œä¸ºæ ¸å¿ƒæ¨¡å‹ç»™æ›´é«˜æƒé‡
                lgb_model = LightGBMModel(
                    objective='binary',
                    max_depth=lgb_max_depth,
                    learning_rate=lgb_learning_rate,
                    n_estimators=lgb_n_estimators,
                    num_leaves=lgb_num_leaves,
                    feature_fraction=lgb_feature_fraction,
                    bagging_fraction=lgb_bagging_fraction,
                    bagging_freq=lgb_bagging_freq,
                    min_child_samples=lgb_min_child_samples,
                    random_state=42,
                    verbose=-1
                )
                
                lstm_model = LSTMModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1],
                    lstm_units=lstm_units,
                    dropout_rate=dropout_rate
                )
                
                transformer_model = TransformerModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1],
                    d_model=d_model,
                    num_heads=num_heads
                )
                
                cnn_lstm_model = CNNLSTMModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1]
                )
                
                # LightGBMæƒé‡0.4ï¼Œå…¶ä»–æ¨¡å‹å…±äº«0.6
                ensemble.add_model(lgb_model, weight=0.4)      # LightGBM - ä¸»è¦æ¨¡å‹
                ensemble.add_model(lstm_model, weight=0.25)     # LSTM
                ensemble.add_model(transformer_model, weight=0.2) # Transformer  
                ensemble.add_model(cnn_lstm_model, weight=0.15)  # CNN-LSTM
                
                # è®­ç»ƒæ¨¡å‹
                ensemble.fit(X_train, y_train, X_val, y_val, epochs=10, batch_size=32)
                
                # è¯„ä¼°
                y_pred = ensemble.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)
                
                return accuracy
                
            except Exception as e:
                logger.error(f"è¶…å‚æ•°ä¼˜åŒ–è¯•éªŒå¤±è´¥: {str(e)}")
                return 0.0
        
        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(),
            pruner=MedianPruner()
        )
        
        study.optimize(objective, n_trials=n_trials, timeout=3600)  # æœ€å¤š1å°æ—¶
        
        logger.info(f"è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
        logger.info(f"æœ€ä½³å‚æ•°: {study.best_params}")
        
        return study.best_params
    
    def create_lightgbm_ensemble_model(self, n_features: int, feature_info: Optional[Dict] = None) -> EnsembleModel:
        """
        åˆ›å»ºåŒ…å«LightGBMçš„é›†æˆæ¨¡å‹
        
        Args:
            n_features: ç‰¹å¾æ•°é‡
            feature_info: ç‰¹å¾ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            é›†æˆæ¨¡å‹
        """
        logger.info("åˆ›å»ºLightGBMé›†æˆæ¨¡å‹...")
        
        # è·å–å®é™…çš„åºåˆ—é•¿åº¦
        actual_sequence_length = self.config['sequence_length']
        if feature_info and 'actual_sequence_length' in feature_info:
            actual_sequence_length = feature_info['actual_sequence_length']
            logger.info(f"ä½¿ç”¨ä¼˜åŒ–åçš„åºåˆ—é•¿åº¦: {actual_sequence_length} (åŸå§‹: {self.config['sequence_length']})")
        
        ensemble = EnsembleModel()
        
        # 1. LightGBMæ¨¡å‹ - ä¸»è¦æ¨¡å‹ï¼Œæƒé‡40%
        lgb_config = self.config['lightgbm_config'].copy()
        # é…ç½®å·²ç»åœ¨__init__ä¸­ä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨
        
        lgb_model = LightGBMModel(**lgb_config)
        ensemble.add_model(lgb_model, weight=0.40)
        
        # 2. LSTMæ¨¡å‹ - æ—¶åºç‰¹å¾ï¼Œæƒé‡25%
        try:
            try:
                from .ai_models import LSTMModel
            except ImportError:
                from ai_models import LSTMModel
                
            lstm_model = LSTMModel(
                sequence_length=actual_sequence_length,  # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦
                n_features=n_features,
                lstm_units=128,
                dropout_rate=0.3
            )
            ensemble.add_model(lstm_model, weight=0.25)
        except Exception as e:
            logger.warning(f"LSTMæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # 3. Transformeræ¨¡å‹ - æ³¨æ„åŠ›æœºåˆ¶ï¼Œæƒé‡20%
        try:
            try:
                from .ai_models import TransformerModel
            except ImportError:
                from ai_models import TransformerModel
                
            transformer_model = TransformerModel(
                sequence_length=actual_sequence_length,  # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦
                n_features=n_features,
                d_model=128,
                num_heads=8
            )
            ensemble.add_model(transformer_model, weight=0.20)
        except Exception as e:
            logger.warning(f"Transformeræ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # 4. CNN-LSTMæ¨¡å‹ - å±€éƒ¨æ¨¡å¼è¯†åˆ«ï¼Œæƒé‡15%
        try:
            try:
                from .ai_models import CNNLSTMModel
            except ImportError:
                from ai_models import CNNLSTMModel
                
            cnn_lstm_model = CNNLSTMModel(
                sequence_length=actual_sequence_length,  # ä½¿ç”¨å®é™…åºåˆ—é•¿åº¦
                n_features=n_features
            )
            ensemble.add_model(cnn_lstm_model, weight=0.15)
        except Exception as e:
            logger.warning(f"CNN-LSTMæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        logger.info(f"é›†æˆæ¨¡å‹åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(ensemble.models)} ä¸ªå­æ¨¡å‹")
        return ensemble
    
    def train_model(self, stock_codes: List[str], 
                   prediction_days: int = 1,
                   use_hyperparameter_optimization: bool = True,
                   save_model: bool = True,
                   clear_cache: bool = False) -> EnsembleModel:
        """
        è®­ç»ƒæ¨¡å‹ä¸»æµç¨‹
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            prediction_days: é¢„æµ‹å¤©æ•°
            use_hyperparameter_optimization: æ˜¯å¦ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–
            save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
            clear_cache: æ˜¯å¦æ¸…ç†ç‰¹å¾ç¼“å­˜
            
        Returns:
            è®­ç»ƒå¥½çš„é›†æˆæ¨¡å‹
        """
        
        # æ‰€æœ‰ç¼“å­˜å·²ç¦ç”¨
        if clear_cache:
            logger.info("ğŸ“Š æ‰€æœ‰ç¼“å­˜å·²ç¦ç”¨ï¼Œæ— éœ€æ¸…ç†")
        
        # ç¼“å­˜çŠ¶æ€
        logger.info("ğŸ“Š æ‰€æœ‰ç¼“å­˜çŠ¶æ€: å·²ç¦ç”¨")
        
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œé¢„æµ‹ {prediction_days} å¤©")
        logger.info(f"   æ‰¹é‡ç¼“å­˜: {'âœ… å¯ç”¨' if self.enable_batch_cache else 'âŒ ç¦ç”¨'}")
        
        # 1. å‡†å¤‡æ•°æ®
        X, y, feature_names, feature_info = self.prepare_training_data(stock_codes, prediction_days)
        
        # 2. æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = self.split_data(X, y)
        
        # 3. è¿›ä¸€æ­¥åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_split_idx = int(len(X_train) * (1 - self.config['validation_split']))
        X_train_final, X_val = X_train[:val_split_idx], X_train[val_split_idx:]
        y_train_final, y_val = y_train[:val_split_idx], y_train[val_split_idx:]
        
        # 4. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
        best_params = None
        if use_hyperparameter_optimization:
            best_params = self.hyperparameter_optimization(
                X_train_final, y_train_final, X_val, y_val
            )
        
        # 5. åˆ›å»ºå’Œè®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨LightGBMé›†æˆæ¨¡å‹ï¼‰
        logger.info("åˆ›å»ºLightGBMä¸ºæ ¸å¿ƒçš„é›†æˆæ¨¡å‹...")
        model = self.create_lightgbm_ensemble_model(
            n_features=X.shape[-1],
            feature_info=feature_info
        )
        
        # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è¿›è¡Œæœ€ç»ˆè®­ç»ƒï¼ˆä¼ é€’feature_infoï¼‰
        # ğŸš€ å®Œæ•´è®­ç»ƒé…ç½®
        optimized_epochs = self.config['training_params']['epochs']  # ä½¿ç”¨é…ç½®çš„å®Œæ•´è®­ç»ƒè½®æ¬¡
        optimized_batch_size = get_optimal_batch_size(self.config['training_params']['batch_size'], 2)  # åŠ¨æ€è®¡ç®—æœ€ä¼˜batch size
        
        if feature_info and hasattr(model, 'fit') and 'feature_info' in model.fit.__code__.co_varnames:
            model.fit(X_train, y_train, X_test, y_test, feature_info=feature_info, epochs=optimized_epochs, batch_size=optimized_batch_size)
        else:
            model.fit(X_train, y_train, X_test, y_test, epochs=optimized_epochs, batch_size=optimized_batch_size)
        
        # 6. æ¨¡å‹è¯„ä¼°
        results = model.evaluate(X_test, y_test)
        
        # 7. äº¤å‰éªŒè¯ï¼ˆä¼ é€’feature_infoï¼‰
        cv_results = self.cross_validate_model(X_train, y_train, model, feature_info=feature_info)
        
        # 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
        if save_model:
            # åˆ›å»ºç»Ÿä¸€çš„ä¿å­˜ç›®å½•
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_save_path = os.path.join(self.model_dir, f"model_{prediction_days}d_{timestamp}")
            os.makedirs(model_save_path, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æ–‡ä»¶
            model.save_models(model_save_path)
            
            # ä¿å­˜è®­ç»ƒä¿¡æ¯å’Œè¯„ä¼°ç»“æœåˆ°åŒä¸€ç›®å½•
            training_info = {
                'stock_codes': stock_codes,
                'prediction_days': prediction_days,
                'feature_names': feature_names,
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'test_results': results,
                'cv_results': cv_results,
                'best_params': best_params,
                'training_time': datetime.now().isoformat(),
                'gpu_config': {
                    'gpu_strategy': str(type(self.gpu_strategy).__name__) if self.gpu_strategy else None,
                    'batch_size_optimized': optimized_batch_size,
                    'epochs_optimized': optimized_epochs
                }
            }
            
            # ä¿å­˜è®­ç»ƒä¿¡æ¯ä¸ºJSONæ ¼å¼ï¼ˆæ˜“è¯»ï¼‰
            import json
            
            def convert_to_json_serializable(obj):
                """é€’å½’è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, dict):
                    return {key: convert_to_json_serializable(value) for key, value in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [convert_to_json_serializable(item) for item in obj]
                elif hasattr(obj, '__dict__'):
                    return str(obj)  # å¤æ‚å¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²
                else:
                    return obj
            
            training_info_json = convert_to_json_serializable(training_info)
            
            with open(os.path.join(model_save_path, 'training_info.json'), 'w', encoding='utf-8') as f:
                json.dump(training_info_json, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜æ€§èƒ½æ‘˜è¦ä¸ºJSONå’ŒCSVæ ¼å¼
            performance_summary = {
                'model_name': f"model_{prediction_days}d",
                'ensemble_accuracy': results.get('Ensemble', {}).get('accuracy', 0),
                'cv_accuracy_mean': cv_results['accuracy'],
                'cv_accuracy_std': cv_results['accuracy_std'],
                'feature_count': len(feature_names),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'total_samples': len(X_train) + len(X_test),
                'training_time': timestamp,
                'gpu_optimized': self.gpu_strategy is not None
            }
            
            # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            performance_summary_json = convert_to_json_serializable(performance_summary)
            
            # ä¿å­˜ä¸ºJSON
            with open(os.path.join(model_save_path, 'performance_summary.json'), 'w', encoding='utf-8') as f:
                json.dump(performance_summary_json, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ä¸ºCSVï¼ˆæ–¹ä¾¿Excelæ‰“å¼€ï¼‰
            import pandas as pd
            pd.DataFrame([performance_summary]).to_csv(
                os.path.join(model_save_path, 'performance_summary.csv'), 
                index=False, encoding='utf-8-sig'
            )
            
            # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
            detailed_results = {}
            for model_name, model_result in results.items():
                if isinstance(model_result, dict) and 'accuracy' in model_result:
                    detailed_results[model_name] = {
                        'accuracy': convert_to_json_serializable(model_result['accuracy']),
                        'model_type': model_name
                    }
            
            # è½¬æ¢è¯¦ç»†ç»“æœä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            detailed_results_json = convert_to_json_serializable(detailed_results)
            
            # ä¿å­˜è¯¦ç»†ç»“æœä¸ºJSON
            with open(os.path.join(model_save_path, 'detailed_results.json'), 'w', encoding='utf-8') as f:
                json.dump(detailed_results_json, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜è¯¦ç»†ç»“æœä¸ºCSV
            if detailed_results:
                pd.DataFrame(detailed_results).T.to_csv(
                    os.path.join(model_save_path, 'detailed_results.csv'), 
                    encoding='utf-8-sig'
                )
            
            # ä¿å­˜ç‰¹å¾åç§°åˆ—è¡¨
            with open(os.path.join(model_save_path, 'feature_names.txt'), 'w', encoding='utf-8') as f:
                f.write(f"æ€»è®¡ {len(feature_names)} ä¸ªç‰¹å¾:\n\n")
                for i, name in enumerate(feature_names, 1):
                    f.write(f"{i:3d}. {name}\n")
            
            # ä¿å­˜æ¨¡å‹æ–‡ä»¶ï¼ˆpklæ ¼å¼ï¼Œç»™ç¨‹åºç”¨ï¼‰
            joblib.dump(training_info, os.path.join(model_save_path, 'training_info.pkl'))
            
            logger.info(f"ğŸ“ æ¨¡å‹å’Œè¯„ä¼°ç»“æœå·²ç»Ÿä¸€ä¿å­˜åˆ°: {model_save_path}")
            logger.info(f"   â”œâ”€â”€ æ¨¡å‹æ–‡ä»¶: *.pkl (ç¨‹åºä½¿ç”¨)")
            logger.info(f"   â”œâ”€â”€ è®­ç»ƒä¿¡æ¯: training_info.json / .pkl")
            logger.info(f"   â”œâ”€â”€ æ€§èƒ½æ‘˜è¦: performance_summary.json / .csv")
            logger.info(f"   â”œâ”€â”€ è¯¦ç»†ç»“æœ: detailed_results.json / .csv")
            logger.info(f"   â””â”€â”€ ç‰¹å¾åˆ—è¡¨: feature_names.txt")
            
            # ğŸš€ è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒå®ŒæˆæŠ¥å‘Š
            try:
                report_path = self.report_generator.generate_training_report(
                    model_save_path=model_save_path,
                    training_info=training_info,
                    results=results,
                    cv_results=cv_results,
                    feature_names=feature_names,
                    stock_codes=stock_codes,
                    prediction_days=prediction_days
                )
                if report_path:
                    logger.info(f"   â”œâ”€â”€ è®­ç»ƒæŠ¥å‘Š: è®­ç»ƒå®ŒæˆæŠ¥å‘Š.md")
            except Exception as e:
                logger.warning(f"âš ï¸  è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¤±è´¥: {str(e)}")
        
        # 9. è®°å½•æ€§èƒ½å†å²
        self.performance_history.append({
            'timestamp': datetime.now(),
            'prediction_days': prediction_days,
            'test_accuracy': results['Ensemble']['accuracy'],
            'cv_accuracy': cv_results['accuracy'],
            'cv_accuracy_std': cv_results['accuracy_std']
        })
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return model
    
    def batch_train_models(self, stock_codes: List[str], 
                         warm_up_cache_first: bool = True) -> Dict[int, EnsembleModel]:
        """
        æ‰¹é‡è®­ç»ƒä¸åŒé¢„æµ‹å¤©æ•°çš„æ¨¡å‹ - æ”¯æŒç¼“å­˜ä¼˜åŒ–
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            warm_up_cache_first: æ˜¯å¦å…ˆé¢„çƒ­ç¼“å­˜
            
        Returns:
            é¢„æµ‹å¤©æ•°åˆ°æ¨¡å‹çš„æ˜ å°„
        """
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡è®­ç»ƒæ¨¡å‹...")
        logger.info(f"   è‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        logger.info(f"   é¢„æµ‹å¤©æ•°: {self.config['prediction_days']}")
        logger.info(f"   æ‰¹é‡ç¼“å­˜: {'âœ… å¯ç”¨' if self.enable_batch_cache else 'âŒ ç¦ç”¨'}")
        
        # ç¼“å­˜å·²ç¦ç”¨ï¼Œè·³è¿‡é¢„çƒ­
        if warm_up_cache_first:
            logger.info("\nâš ï¸ ç¼“å­˜åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡é¢„çƒ­æ­¥éª¤")
        
        models = {}
        total_models = len(self.config['prediction_days'])
        
        for i, prediction_days in enumerate(self.config['prediction_days'], 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ æ¨¡å‹ {i}/{total_models}: è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹")
            logger.info(f"{'='*60}")
            
            try:
                start_time = datetime.now()
                
                model = self.train_model(
                    stock_codes=stock_codes,
                    prediction_days=prediction_days,
                    use_hyperparameter_optimization=False,  # æ‰¹é‡è®­ç»ƒæ—¶å…³é—­ä»¥èŠ‚çœæ—¶é—´
                    save_model=True,
                    clear_cache=False  # ä¸æ¸…ç†ç¼“å­˜ï¼Œä¿æŒåŠ é€Ÿæ•ˆæœ
                )
                models[prediction_days] = model
                
                training_time = datetime.now() - start_time
                logger.info(f"âœ… {prediction_days} å¤©é¢„æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time}")
                
                # æ˜¾ç¤ºå‰©ä½™è®­ç»ƒä¼°è®¡æ—¶é—´
                if i < total_models:
                    remaining_models = total_models - i
                    estimated_remaining_time = training_time * remaining_models
                    logger.info(f"ğŸ“Š é¢„è®¡å‰©ä½™è®­ç»ƒæ—¶é—´: {estimated_remaining_time}")
                
            except Exception as e:
                logger.error(f"âŒ è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹å¤±è´¥: {str(e)}")
                continue
        
        logger.info(f"\nğŸ‰ æ‰¹é‡è®­ç»ƒå®Œæˆï¼")
        logger.info(f"   æˆåŠŸè®­ç»ƒ: {len(models)}/{total_models} ä¸ªæ¨¡å‹")
        logger.info(f"   å¤±è´¥æ•°é‡: {total_models - len(models)}")
        
        # ç¼“å­˜å·²ç¦ç”¨ï¼Œæ— éœ€æ˜¾ç¤ºç»Ÿè®¡
        logger.info(f"\nğŸ“Š ç¼“å­˜çŠ¶æ€: å·²ç¦ç”¨")
        
        return models
    
    def get_performance_summary(self) -> pd.DataFrame:
        """è·å–æ€§èƒ½å†å²æ‘˜è¦"""
        if not self.performance_history:
            return pd.DataFrame()
        
        df = pd.DataFrame(self.performance_history)
        return df.groupby('prediction_days').agg({
            'test_accuracy': ['mean', 'std', 'max'],
            'cv_accuracy': ['mean', 'std', 'max'],
            'cv_accuracy_std': 'mean'
        }).round(4)


class AutoRetrainingSystem:
    """
    è‡ªåŠ¨é‡è®­ç»ƒç³»ç»Ÿ
    ç›‘æ§æ¨¡å‹æ€§èƒ½ï¼Œè‡ªåŠ¨è§¦å‘é‡è®­ç»ƒ
    """
    
    def __init__(self, pipeline: ModelTrainingPipeline, 
                 performance_threshold: float = 0.55,
                 monitoring_window: int = 30):
        self.pipeline = pipeline
        self.performance_threshold = performance_threshold
        self.monitoring_window = monitoring_window
        self.prediction_history = []
        
    def log_prediction(self, prediction: int, actual: int, stock_code: str, 
                      prediction_days: int):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        self.prediction_history.append({
            'timestamp': datetime.now(),
            'stock_code': stock_code,
            'prediction_days': prediction_days,
            'prediction': prediction,
            'actual': actual,
            'correct': prediction == actual
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.prediction_history) > 10000:
            self.prediction_history = self.prediction_history[-5000:]
    
    def check_performance(self, prediction_days: int) -> float:
        """æ£€æŸ¥æœ€è¿‘çš„é¢„æµ‹æ€§èƒ½"""
        if len(self.prediction_history) < 100:
            return 1.0  # æ•°æ®ä¸è¶³ï¼Œä¸è§¦å‘é‡è®­ç»ƒ
        
        # è·å–æœ€è¿‘çš„é¢„æµ‹è®°å½•
        recent_predictions = [
            record for record in self.prediction_history[-self.monitoring_window:]
            if record['prediction_days'] == prediction_days
        ]
        
        if len(recent_predictions) < 20:
            return 1.0  # æ ·æœ¬ä¸è¶³
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = sum(record['correct'] for record in recent_predictions) / len(recent_predictions)
        logger.info(f"æœ€è¿‘ {len(recent_predictions)} æ¬¡é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}")
        
        return accuracy
    
    def should_retrain(self, prediction_days: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è®­ç»ƒ"""
        current_performance = self.check_performance(prediction_days)
        
        if current_performance < self.performance_threshold:
            logger.warning(f"{prediction_days}å¤©é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦é‡è®­ç»ƒ")
            return True
        
        return False
    
    def auto_retrain(self, stock_codes: List[str]):
        """è‡ªåŠ¨é‡è®­ç»ƒæ£€æŸ¥"""
        logger.info("å¼€å§‹è‡ªåŠ¨é‡è®­ç»ƒæ£€æŸ¥...")
        
        for prediction_days in self.pipeline.config['prediction_days']:
            if self.should_retrain(prediction_days):
                logger.info(f"å¼€å§‹é‡è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹...")
                
                try:
                    new_model = self.pipeline.train_model(
                        stock_codes=stock_codes,
                        prediction_days=prediction_days,
                        use_hyperparameter_optimization=True,
                        save_model=True
                    )
                    logger.info(f"{prediction_days} å¤©é¢„æµ‹æ¨¡å‹é‡è®­ç»ƒå®Œæˆ")
                    
                except Exception as e:
                    logger.error(f"é‡è®­ç»ƒå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒç®¡é“ - å±•ç¤ºæ‰¹é‡ç¼“å­˜åŠŸèƒ½
    logger.info("ğŸš€ å¯åŠ¨LightGBMé›†æˆè®­ç»ƒæµæ°´çº¿æµ‹è¯•ï¼ˆå¸¦æ‰¹é‡ç¼“å­˜ä¼˜åŒ–ï¼‰")
    
    # åˆå§‹åŒ–ç®¡é“ï¼ˆå¯ç”¨æ‰¹é‡ç¼“å­˜ï¼‰
    pipeline = ModelTrainingPipeline(
        enable_batch_cache=True,  # å¯ç”¨æ‰¹é‡ç¼“å­˜
        cache_workers=1  # ç¼“å­˜å·¥ä½œè¿›ç¨‹æ•°
    )
    
    # ä½¿ç”¨æ–°çš„æ–¹æ³•è·å–å¯ç”¨è‚¡ç¥¨
    available_stocks = pipeline.get_available_stocks()  # è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
    
    if not available_stocks:
        logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
        print("è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶åœ¨ datas_em ç›®å½•ä¸­")
        exit(1)
    
    logger.info(f"ä½¿ç”¨è‚¡ç¥¨ä»£ç : {available_stocks}")
    
    try:
        # æµ‹è¯•1: å•ä¸ªæ¨¡å‹è®­ç»ƒï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œ - å»ºç«‹ç¼“å­˜ï¼‰
        logger.info("\n" + "="*60)
        logger.info("æµ‹è¯•1: ç¬¬ä¸€æ¬¡è®­ç»ƒï¼ˆå»ºç«‹ç¼“å­˜ï¼‰")
        logger.info("="*60)
        
        start_time = datetime.now()
        model1 = pipeline.train_model(
            stock_codes=available_stocks,
            prediction_days=1,
            use_hyperparameter_optimization=False,
            save_model=True,
            clear_cache=False
        )
        first_training_time = datetime.now() - start_time
        
        # æµ‹è¯•2: ç¬¬äºŒæ¬¡è®­ç»ƒï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        logger.info("\n" + "="*60)
        logger.info("æµ‹è¯•2: ç¬¬äºŒæ¬¡è®­ç»ƒï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
        logger.info("="*60)
        
        start_time = datetime.now()
        model2 = pipeline.train_model(
            stock_codes=available_stocks,
            prediction_days=1,
            use_hyperparameter_optimization=False,
            save_model=False,  # ç¬¬äºŒæ¬¡ä¸ä¿å­˜ï¼Œåªæµ‹è¯•é€Ÿåº¦
            clear_cache=False
        )
        second_training_time = datetime.now() - start_time
        
        # æ€§èƒ½å¯¹æ¯”
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ æ‰¹é‡ç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœ")
        logger.info("="*60)
        logger.info(f"ç¬¬ä¸€æ¬¡è®­ç»ƒæ—¶é—´: {first_training_time}")
        logger.info(f"ç¬¬äºŒæ¬¡è®­ç»ƒæ—¶é—´: {second_training_time}")
        
        if second_training_time.total_seconds() > 0:
            speedup = first_training_time.total_seconds() / second_training_time.total_seconds()
            time_saved = first_training_time - second_training_time
            efficiency = (time_saved.total_seconds() / first_training_time.total_seconds()) * 100
            
            logger.info(f"âš¡ åŠ é€Ÿæ¯”: {speedup:.1f}x")
            logger.info(f"â±ï¸ æ—¶é—´èŠ‚çœ: {time_saved}")
            logger.info(f"ğŸ“ˆ æ•ˆç‡æå‡: {efficiency:.1f}%")
            
            # å¤§è§„æ¨¡é¢„æµ‹
            total_stocks = len(pipeline.get_available_stocks())
            if total_stocks > len(available_stocks):
                estimated_time_no_cache = (first_training_time.total_seconds() / len(available_stocks)) * total_stocks
                estimated_time_with_cache = (second_training_time.total_seconds() / len(available_stocks)) * total_stocks
                
                logger.info(f"\nğŸ”® {total_stocks}åªè‚¡ç¥¨è®­ç»ƒæ—¶é—´é¢„ä¼°:")
                logger.info(f"   æ— ç¼“å­˜: {estimated_time_no_cache / 60:.1f} åˆ†é’Ÿ")
                logger.info(f"   æœ‰ç¼“å­˜: {estimated_time_with_cache / 60:.1f} åˆ†é’Ÿ")
                logger.info(f"   é¢„è®¡èŠ‚çœ: {(estimated_time_no_cache - estimated_time_with_cache) / 60:.1f} åˆ†é’Ÿ")
        
        # æµ‹è¯•3: ç¼“å­˜é¢„çƒ­åŠŸèƒ½
        logger.info("\n" + "="*60)
        logger.info("æµ‹è¯•3: ç¼“å­˜é¢„çƒ­åŠŸèƒ½")
        logger.info("="*60)
        
        # æ¸…ç†ç¼“å­˜åé‡æ–°é¢„çƒ­
        pipeline.batch_processor.cache.clear_all_cache()
        pipeline.warm_up_cache(available_stocks, show_progress=True)
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = pipeline.get_performance_summary()
        if not summary.empty:
            logger.info("\nğŸ“Š æ€§èƒ½æ‘˜è¦:")
            print(summary)
        
        logger.info("\nğŸ‰ æ‰¹é‡ç¼“å­˜æµ‹è¯•å®Œæˆï¼")
        logger.info("âœ… ç¼“å­˜ç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼Œå¯æ˜¾è‘—æå‡å¤§è§„æ¨¡è®­ç»ƒæ•ˆç‡")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        print("è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶åœ¨ datas_em ç›®å½•ä¸­")