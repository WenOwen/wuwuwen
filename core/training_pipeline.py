# -*- coding: utf-8 -*-
"""
AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - è®­ç»ƒç®¡é“æ¨¡å—
åŠŸèƒ½ï¼šè‡ªåŠ¨åŒ–æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–å’Œéƒ¨ç½²ç®¡é“
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import joblib
from datetime import datetime, timedelta
import logging
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

from .feature_engineering import FeatureEngineering
from .ai_models import EnsembleModel, create_ensemble_model
from .enhanced_ai_models import create_enhanced_ensemble_model
from ..utils.gpu_config import setup_dual_gpu, get_optimal_batch_size

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainingPipeline:
    """
    æ¨¡å‹è®­ç»ƒç®¡é“ç±»
    è´Ÿè´£æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€è¶…å‚æ•°ä¼˜åŒ–ç­‰
    """
    
    def __init__(self, data_dir: str = "datas_em", model_dir: str = "models"):
        self.data_dir = data_dir
        self.model_dir = model_dir
        self.feature_engineer = FeatureEngineering()
        self.models = {}
        self.performance_history = []
        
        # ğŸš€ é…ç½®åŒGPUç¯å¢ƒ
        self.gpu_strategy = setup_dual_gpu()
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        os.makedirs(model_dir, exist_ok=True)
        
        # è®­ç»ƒé…ç½®
        self.config = {
            'sequence_length': 60,
            'prediction_days': [1, 3, 5],
            'train_test_split': 0.8,
            'validation_split': 0.2,
            'min_samples': 500,  # æœ€å°‘æ ·æœ¬æ•°
            'performance_threshold': 0.55,  # æœ€ä½å‡†ç¡®ç‡è¦æ±‚
        }
    
    def load_stock_data(self, stock_code: str) -> pd.DataFrame:
        """åŠ è½½å•åªè‚¡ç¥¨æ•°æ®"""
        file_path = os.path.join(self.data_dir, f"{stock_code}.csv")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è‚¡ç¥¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        df = pd.read_csv(file_path)
        df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
        df = df.sort_values('äº¤æ˜“æ—¥æœŸ').reset_index(drop=True)
        
        logger.info(f"åŠ è½½è‚¡ç¥¨ {stock_code} æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        return df
    
    def prepare_training_data(self, stock_codes: List[str], 
                            prediction_days: int = 1) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            prediction_days: é¢„æµ‹å¤©æ•°
            
        Returns:
            X, y, feature_names
        """
        logger.info(f"å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        
        all_X, all_y = [], []
        feature_names = None
        feature_info = None
        
        for stock_code in stock_codes:
            try:
                # åŠ è½½è‚¡ç¥¨æ•°æ®
                df = self.load_stock_data(stock_code)
                
                if len(df) < self.config['min_samples']:
                    logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡")
                    continue
                
                # ç‰¹å¾å·¥ç¨‹ï¼ˆä¼ é€’è‚¡ç¥¨ä»£ç ï¼‰
                df_features = self.feature_engineer.create_all_features(df, stock_code)
                
                # å‡†å¤‡æ¨¡å‹æ•°æ®
                X, y, feature_names_temp, feature_info_temp = self.feature_engineer.prepare_model_data(
                    df_features, 
                    prediction_days=prediction_days,
                    lookback_window=self.config['sequence_length']
                )
                
                if len(X) > 0:
                    all_X.append(X)
                    all_y.append(y)
                    if feature_names is None:
                        feature_names = feature_names_temp
                    if feature_info is None:
                        feature_info = feature_info_temp
                    logger.info(f"è‚¡ç¥¨ {stock_code} ç”Ÿæˆæ ·æœ¬ {len(X)} ä¸ª")
                
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_code} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        if not all_X:
            raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†çš„è‚¡ç¥¨æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        X_combined = np.vstack(all_X)
        y_combined = np.hstack(all_y)
        
        logger.info(f"æ€»è®¡æ ·æœ¬æ•°: {len(X_combined)}, ç‰¹å¾æ•°: {len(feature_names)}")
        logger.info(f"æ­£æ ·æœ¬æ¯”ä¾‹: {y_combined.mean():.3f}")
        
        return X_combined, y_combined, feature_names, feature_info
    
    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        æ—¶é—´åºåˆ—æ•°æ®åˆ†å‰²
        """
        split_idx = int(len(X) * self.config['train_test_split'])
        
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}, æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test)}")
        
        return X_train, X_test, y_train, y_test
    
    def cross_validate_model(self, X: np.ndarray, y: np.ndarray, 
                           model: Optional[EnsembleModel] = None,
                           feature_info: Optional[Dict] = None,
                           n_splits: int = 3) -> Dict[str, float]:  # ä»5æŠ˜å‡å°‘åˆ°3æŠ˜
        """
        æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            model: æ¨¡å‹å®ä¾‹
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹äº¤å‰éªŒè¯...")
        
        if model is None:
            if feature_info:
                model = create_enhanced_ensemble_model(
                    sequence_length=self.config['sequence_length'],
                    n_features=X.shape[-1],
                    n_stocks=feature_info.get('n_stocks', 100),
                    n_sectors=feature_info.get('n_sectors', 20)
                )
            else:
                model = create_ensemble_model(
                    sequence_length=self.config['sequence_length'],
                    n_features=X.shape[-1]
                )
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        scores = {
            'accuracy': [],
            'precision': [],
            'recall': [],
            'f1': [],
            'auc': []
        }
        
        fold = 1
        for train_idx, val_idx in tscv.split(X):
            logger.info(f"äº¤å‰éªŒè¯ æŠ˜ {fold}/{n_splits}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            # ğŸš€ äº¤å‰éªŒè¯ä¼˜åŒ–é…ç½®
            cv_epochs = 10  # äº¤å‰éªŒè¯å‡å°‘epochsåŠ å¿«é€Ÿåº¦
            cv_batch_size = get_optimal_batch_size(32, 2)  # åŠ¨æ€è®¡ç®—æœ€ä¼˜batch size
            
            if feature_info and hasattr(model, 'fit') and 'feature_info' in model.fit.__code__.co_varnames:
                model.fit(X_train_fold, y_train_fold, X_val_fold, y_val_fold, 
                         feature_info=feature_info, epochs=cv_epochs, batch_size=cv_batch_size)
            else:
                model.fit(X_train_fold, y_train_fold, X_val_fold, y_val_fold, 
                         epochs=cv_epochs, batch_size=cv_batch_size)
            
            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = model.predict(X_val_fold)
            y_proba = model.predict_proba(X_val_fold)[:, 1]
            
            # è®¡ç®—æŒ‡æ ‡
            scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
            scores['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))
            scores['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))
            scores['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))
            scores['auc'].append(roc_auc_score(y_val_fold, y_proba))
            
            fold += 1
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_scores = {metric: np.mean(values) for metric, values in scores.items()}
        std_scores = {f"{metric}_std": np.std(values) for metric, values in scores.items()}
        
        result = {**avg_scores, **std_scores}
        
        logger.info("äº¤å‰éªŒè¯ç»“æœ:")
        for metric, score in avg_scores.items():
            logger.info(f"  {metric}: {score:.4f} Â± {std_scores[f'{metric}_std']:.4f}")
        
        return result
    
    def hyperparameter_optimization(self, X_train: np.ndarray, y_train: np.ndarray,
                                  X_val: np.ndarray, y_val: np.ndarray,
                                  n_trials: int = 50) -> Dict:
        """
        è¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_val, y_val: éªŒè¯æ•°æ®
            n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            
        Returns:
            æœ€ä¼˜å‚æ•°å­—å…¸
        """
        logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œè¯•éªŒæ¬¡æ•°: {n_trials}")
        
        def objective(trial):
            # LSTMå‚æ•°
            lstm_units = trial.suggest_int('lstm_units', 64, 256, step=64)
            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
            
            # LightGBMå‚æ•°
            lgb_max_depth = trial.suggest_int('lgb_max_depth', 3, 10)
            lgb_learning_rate = trial.suggest_float('lgb_learning_rate', 0.01, 0.3)
            lgb_n_estimators = trial.suggest_int('lgb_n_estimators', 100, 1000, step=100)
            
            # Transformerå‚æ•°
            d_model = trial.suggest_int('d_model', 64, 256, step=64)
            num_heads = trial.suggest_int('num_heads', 4, 16, step=4)
            
            try:
                # åˆ›å»ºæ¨¡å‹ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦ä¼ å…¥å‚æ•°ï¼‰
                from ai_models import LSTMModel, LightGBMModel, TransformerModel, CNNLSTMModel
                
                ensemble = EnsembleModel()
                
                # æ·»åŠ ä¼˜åŒ–åçš„æ¨¡å‹
                lstm_model = LSTMModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1],
                    lstm_units=lstm_units,
                    dropout_rate=dropout_rate
                )
                
                lgb_model = LightGBMModel(
                    max_depth=lgb_max_depth,
                    learning_rate=lgb_learning_rate,
                    n_estimators=lgb_n_estimators
                )
                
                transformer_model = TransformerModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1],
                    d_model=d_model,
                    num_heads=num_heads
                )
                
                cnn_lstm_model = CNNLSTMModel(
                    sequence_length=self.config['sequence_length'],
                    n_features=X_train.shape[-1]
                )
                
                ensemble.add_model(lstm_model)
                ensemble.add_model(lgb_model)
                ensemble.add_model(transformer_model)
                ensemble.add_model(cnn_lstm_model)
                
                # è®­ç»ƒæ¨¡å‹
                ensemble.fit(X_train, y_train, X_val, y_val, epochs=10, batch_size=32)
                
                # è¯„ä¼°
                y_pred = ensemble.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)
                
                return accuracy
                
            except Exception as e:
                logger.error(f"è¶…å‚æ•°ä¼˜åŒ–è¯•éªŒå¤±è´¥: {str(e)}")
                return 0.0
        
        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(),
            pruner=MedianPruner()
        )
        
        study.optimize(objective, n_trials=n_trials, timeout=3600)  # æœ€å¤š1å°æ—¶
        
        logger.info(f"è¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
        logger.info(f"æœ€ä½³å‚æ•°: {study.best_params}")
        
        return study.best_params
    
    def train_model(self, stock_codes: List[str], 
                   prediction_days: int = 1,
                   use_hyperparameter_optimization: bool = True,
                   save_model: bool = True) -> EnsembleModel:
        """
        è®­ç»ƒæ¨¡å‹ä¸»æµç¨‹
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            prediction_days: é¢„æµ‹å¤©æ•°
            use_hyperparameter_optimization: æ˜¯å¦ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–
            save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
            
        Returns:
            è®­ç»ƒå¥½çš„é›†æˆæ¨¡å‹
        """
        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œé¢„æµ‹ {prediction_days} å¤©")
        
        # 1. å‡†å¤‡æ•°æ®
        X, y, feature_names, feature_info = self.prepare_training_data(stock_codes, prediction_days)
        
        # 2. æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = self.split_data(X, y)
        
        # 3. è¿›ä¸€æ­¥åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_split_idx = int(len(X_train) * (1 - self.config['validation_split']))
        X_train_final, X_val = X_train[:val_split_idx], X_train[val_split_idx:]
        y_train_final, y_val = y_train[:val_split_idx], y_train[val_split_idx:]
        
        # 4. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
        best_params = None
        if use_hyperparameter_optimization:
            best_params = self.hyperparameter_optimization(
                X_train_final, y_train_final, X_val, y_val
            )
        
        # 5. åˆ›å»ºå’Œè®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å¢å¼ºæ¨¡å‹ï¼‰
        if feature_info:
            model = create_enhanced_ensemble_model(
                sequence_length=self.config['sequence_length'],
                n_features=X.shape[-1],
                n_stocks=feature_info.get('n_stocks', 100),
                n_sectors=feature_info.get('n_sectors', 20)
            )
        else:
            # å›é€€åˆ°åŸå§‹æ¨¡å‹
            model = create_ensemble_model(
                sequence_length=self.config['sequence_length'],
                n_features=X.shape[-1]
            )
        
        # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è¿›è¡Œæœ€ç»ˆè®­ç»ƒï¼ˆä¼ é€’feature_infoï¼‰
        # ğŸš€ åŒ3090ä¼˜åŒ–é…ç½®
        optimized_epochs = 30  # ä»100å‡å°‘åˆ°30ï¼Œå¹³è¡¡é€Ÿåº¦å’Œæ•ˆæœ
        optimized_batch_size = get_optimal_batch_size(48, 2)  # åŠ¨æ€è®¡ç®—æœ€ä¼˜batch size
        
        if feature_info and hasattr(model, 'fit') and 'feature_info' in model.fit.__code__.co_varnames:
            model.fit(X_train, y_train, X_test, y_test, feature_info=feature_info, epochs=optimized_epochs, batch_size=optimized_batch_size)
        else:
            model.fit(X_train, y_train, X_test, y_test, epochs=optimized_epochs, batch_size=optimized_batch_size)
        
        # 6. æ¨¡å‹è¯„ä¼°
        results = model.evaluate(X_test, y_test)
        
        # 7. äº¤å‰éªŒè¯ï¼ˆä¼ é€’feature_infoï¼‰
        cv_results = self.cross_validate_model(X_train, y_train, model, feature_info=feature_info)
        
        # 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
        if save_model:
            model_save_path = os.path.join(
                self.model_dir, 
                f"model_{prediction_days}d_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            model.save_models(model_save_path)
            
            # ä¿å­˜è®­ç»ƒä¿¡æ¯
            training_info = {
                'stock_codes': stock_codes,
                'prediction_days': prediction_days,
                'feature_names': feature_names,
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'test_results': results,
                'cv_results': cv_results,
                'best_params': best_params,
                'training_time': datetime.now().isoformat()
            }
            
            joblib.dump(training_info, os.path.join(model_save_path, 'training_info.pkl'))
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
        
        # 9. è®°å½•æ€§èƒ½å†å²
        self.performance_history.append({
            'timestamp': datetime.now(),
            'prediction_days': prediction_days,
            'test_accuracy': results['Ensemble']['accuracy'],
            'cv_accuracy': cv_results['accuracy'],
            'cv_accuracy_std': cv_results['accuracy_std']
        })
        
        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return model
    
    def batch_train_models(self, stock_codes: List[str]) -> Dict[int, EnsembleModel]:
        """
        æ‰¹é‡è®­ç»ƒä¸åŒé¢„æµ‹å¤©æ•°çš„æ¨¡å‹
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            é¢„æµ‹å¤©æ•°åˆ°æ¨¡å‹çš„æ˜ å°„
        """
        logger.info("å¼€å§‹æ‰¹é‡è®­ç»ƒæ¨¡å‹...")
        
        models = {}
        for prediction_days in self.config['prediction_days']:
            logger.info(f"\n{'='*50}")
            logger.info(f"è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹")
            logger.info(f"{'='*50}")
            
            try:
                model = self.train_model(
                    stock_codes=stock_codes,
                    prediction_days=prediction_days,
                    use_hyperparameter_optimization=False,  # æ‰¹é‡è®­ç»ƒæ—¶å…³é—­ä»¥èŠ‚çœæ—¶é—´
                    save_model=True
                )
                models[prediction_days] = model
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹å¤±è´¥: {str(e)}")
                continue
        
        logger.info(f"æ‰¹é‡è®­ç»ƒå®Œæˆï¼ŒæˆåŠŸè®­ç»ƒ {len(models)} ä¸ªæ¨¡å‹")
        return models
    
    def get_performance_summary(self) -> pd.DataFrame:
        """è·å–æ€§èƒ½å†å²æ‘˜è¦"""
        if not self.performance_history:
            return pd.DataFrame()
        
        df = pd.DataFrame(self.performance_history)
        return df.groupby('prediction_days').agg({
            'test_accuracy': ['mean', 'std', 'max'],
            'cv_accuracy': ['mean', 'std', 'max'],
            'cv_accuracy_std': 'mean'
        }).round(4)


class AutoRetrainingSystem:
    """
    è‡ªåŠ¨é‡è®­ç»ƒç³»ç»Ÿ
    ç›‘æ§æ¨¡å‹æ€§èƒ½ï¼Œè‡ªåŠ¨è§¦å‘é‡è®­ç»ƒ
    """
    
    def __init__(self, pipeline: ModelTrainingPipeline, 
                 performance_threshold: float = 0.55,
                 monitoring_window: int = 30):
        self.pipeline = pipeline
        self.performance_threshold = performance_threshold
        self.monitoring_window = monitoring_window
        self.prediction_history = []
        
    def log_prediction(self, prediction: int, actual: int, stock_code: str, 
                      prediction_days: int):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        self.prediction_history.append({
            'timestamp': datetime.now(),
            'stock_code': stock_code,
            'prediction_days': prediction_days,
            'prediction': prediction,
            'actual': actual,
            'correct': prediction == actual
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.prediction_history) > 10000:
            self.prediction_history = self.prediction_history[-5000:]
    
    def check_performance(self, prediction_days: int) -> float:
        """æ£€æŸ¥æœ€è¿‘çš„é¢„æµ‹æ€§èƒ½"""
        if len(self.prediction_history) < 100:
            return 1.0  # æ•°æ®ä¸è¶³ï¼Œä¸è§¦å‘é‡è®­ç»ƒ
        
        # è·å–æœ€è¿‘çš„é¢„æµ‹è®°å½•
        recent_predictions = [
            record for record in self.prediction_history[-self.monitoring_window:]
            if record['prediction_days'] == prediction_days
        ]
        
        if len(recent_predictions) < 20:
            return 1.0  # æ ·æœ¬ä¸è¶³
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = sum(record['correct'] for record in recent_predictions) / len(recent_predictions)
        logger.info(f"æœ€è¿‘ {len(recent_predictions)} æ¬¡é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.4f}")
        
        return accuracy
    
    def should_retrain(self, prediction_days: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è®­ç»ƒ"""
        current_performance = self.check_performance(prediction_days)
        
        if current_performance < self.performance_threshold:
            logger.warning(f"{prediction_days}å¤©é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦é‡è®­ç»ƒ")
            return True
        
        return False
    
    def auto_retrain(self, stock_codes: List[str]):
        """è‡ªåŠ¨é‡è®­ç»ƒæ£€æŸ¥"""
        logger.info("å¼€å§‹è‡ªåŠ¨é‡è®­ç»ƒæ£€æŸ¥...")
        
        for prediction_days in self.pipeline.config['prediction_days']:
            if self.should_retrain(prediction_days):
                logger.info(f"å¼€å§‹é‡è®­ç»ƒ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹...")
                
                try:
                    new_model = self.pipeline.train_model(
                        stock_codes=stock_codes,
                        prediction_days=prediction_days,
                        use_hyperparameter_optimization=True,
                        save_model=True
                    )
                    logger.info(f"{prediction_days} å¤©é¢„æµ‹æ¨¡å‹é‡è®­ç»ƒå®Œæˆ")
                    
                except Exception as e:
                    logger.error(f"é‡è®­ç»ƒå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒç®¡é“
    pipeline = ModelTrainingPipeline()
    
    # ğŸš€ å¿«é€Ÿè°ƒè¯•ï¼šåªç”¨ä¸€åªè‚¡ç¥¨
    stock_codes = ['sz301636','sh600000']  # å…ˆç”¨ä¸€åªæœ‰æ•°æ®çš„è‚¡ç¥¨å¿«é€ŸéªŒè¯
    
    try:
        # è®­ç»ƒå•ä¸ªæ¨¡å‹
        model = pipeline.train_model(
            stock_codes=stock_codes,
            prediction_days=1,
            use_hyperparameter_optimization=False,
            save_model=False  # å¿«é€Ÿè°ƒè¯•æ—¶ä¸ä¿å­˜æ¨¡å‹
        )
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = pipeline.get_performance_summary()
        print("\næ€§èƒ½æ‘˜è¦:")
        print(summary)
        
    except Exception as e:
        logger.error(f"è®­ç»ƒæµ‹è¯•å¤±è´¥: {str(e)}")
        print("è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„è‚¡ç¥¨æ•°æ®æ–‡ä»¶åœ¨ datas_em ç›®å½•ä¸­")