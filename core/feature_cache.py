# -*- coding: utf-8 -*-
"""
ç‰¹å¾å·¥ç¨‹ç¼“å­˜ç³»ç»Ÿ
åŠŸèƒ½ï¼šä¸ºå¤§è§„æ¨¡è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹æä¾›é«˜æ•ˆç¼“å­˜æœºåˆ¶
"""

import os
import pickle
import hashlib
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, Tuple
import logging
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class FeatureCache:
    """
    ç‰¹å¾å·¥ç¨‹ç¼“å­˜ç³»ç»Ÿ
    - åŸºäºæ–‡ä»¶çš„ç¼“å­˜å­˜å‚¨
    - æ”¯æŒç‰ˆæœ¬æ§åˆ¶å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    - è‡ªåŠ¨è¿‡æœŸæœºåˆ¶
    - æ‰¹é‡ç¼“å­˜ä¼˜åŒ–
    """
    
    def __init__(self, cache_dir: str = "cache/features", cache_days: int = 7):
        """
        åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            cache_days: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
        """
        self.cache_dir = cache_dir
        self.cache_days = cache_days
        self.version = "v1.0"  # ç‰¹å¾ç‰ˆæœ¬ï¼Œå˜æ›´æ—¶ä¼šæ¸…ç†æ—§ç¼“å­˜
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = os.path.join(cache_dir, "cache_metadata.json")
        self.metadata = self._load_metadata()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.cache_hits = 0
        self.cache_misses = 0
        
    def _load_metadata(self) -> Dict:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {
            "version": self.version,
            "created_time": datetime.now().isoformat(),
            "cache_entries": {}
        }
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _generate_cache_key(self, stock_code: str, data_hash: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            data_hash: åŸå§‹æ•°æ®å“ˆå¸Œå€¼
            
        Returns:
            ç¼“å­˜é”®
        """
        # ç»„åˆç‰ˆæœ¬ã€è‚¡ç¥¨ä»£ç å’Œæ•°æ®å“ˆå¸Œ
        key_string = f"{self.version}_{stock_code}_{data_hash}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _calculate_data_hash(self, df: pd.DataFrame) -> str:
        """
        è®¡ç®—DataFrameçš„å“ˆå¸Œå€¼
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            æ•°æ®å“ˆå¸Œå€¼
        """
        # ä½¿ç”¨DataFrameçš„å…³é”®ä¿¡æ¯ç”Ÿæˆå“ˆå¸Œ
        def convert_to_serializable(obj):
            """è½¬æ¢å¯¹è±¡ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
            if pd.isna(obj):
                return None
            elif hasattr(obj, 'isoformat'):  # datetime/Timestampå¯¹è±¡
                return obj.isoformat()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            else:
                return obj
        
        key_info = {
            'shape': df.shape,
            'columns': list(df.columns),
            'first_date': convert_to_serializable(df['äº¤æ˜“æ—¥æœŸ'].iloc[0]) if 'äº¤æ˜“æ—¥æœŸ' in df.columns and len(df) > 0 else None,
            'last_date': convert_to_serializable(df['äº¤æ˜“æ—¥æœŸ'].iloc[-1]) if 'äº¤æ˜“æ—¥æœŸ' in df.columns and len(df) > 0 else None,
            'first_close': convert_to_serializable(df['æ”¶ç›˜ä»·'].iloc[0]) if 'æ”¶ç›˜ä»·' in df.columns and len(df) > 0 else None,
            'last_close': convert_to_serializable(df['æ”¶ç›˜ä»·'].iloc[-1]) if 'æ”¶ç›˜ä»·' in df.columns and len(df) > 0 else None,
        }
        
        key_string = json.dumps(key_info, sort_keys=True)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self.metadata["cache_entries"]:
            return False
        
        cache_info = self.metadata["cache_entries"][cache_key]
        
        # æ£€æŸ¥ç‰ˆæœ¬
        if cache_info.get("version") != self.version:
            return False
        
        # æ£€æŸ¥è¿‡æœŸæ—¶é—´
        cached_time = datetime.fromisoformat(cache_info["cached_time"])
        if datetime.now() - cached_time > timedelta(days=self.cache_days):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cache_path = self._get_cache_path(cache_key)
        if not os.path.exists(cache_path):
            return False
        
        return True
    
    def get_cached_features(self, stock_code: str, df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        è·å–ç¼“å­˜çš„ç‰¹å¾æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            df: åŸå§‹æ•°æ®
            
        Returns:
            ç¼“å­˜çš„ç‰¹å¾æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            # è®¡ç®—æ•°æ®å“ˆå¸Œ
            data_hash = self._calculate_data_hash(df)
            cache_key = self._generate_cache_key(stock_code, data_hash)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            if not self._is_cache_valid(cache_key):
                self.cache_misses += 1
                return None
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            cache_path = self._get_cache_path(cache_key)
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            
            self.cache_hits += 1
            logger.info(f"âœ… ç¼“å­˜å‘½ä¸­: {stock_code}")
            return cached_data["features"]
            
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥ {stock_code}: {e}")
            self.cache_misses += 1
            return None
    
    def save_features_to_cache(self, stock_code: str, df_original: pd.DataFrame, 
                              df_features: pd.DataFrame):
        """
        ä¿å­˜ç‰¹å¾æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            df_original: åŸå§‹æ•°æ®
            df_features: ç‰¹å¾æ•°æ®
        """
        try:
            # è®¡ç®—æ•°æ®å“ˆå¸Œ
            data_hash = self._calculate_data_hash(df_original)
            cache_key = self._generate_cache_key(stock_code, data_hash)
            
            # å‡†å¤‡ç¼“å­˜æ•°æ®
            cache_data = {
                "stock_code": stock_code,
                "data_hash": data_hash,
                "features": df_features,
                "cached_time": datetime.now().isoformat(),
                "version": self.version
            }
            
            # ä¿å­˜ç¼“å­˜æ–‡ä»¶
            cache_path = self._get_cache_path(cache_key)
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            # æ›´æ–°å…ƒæ•°æ®
            self.metadata["cache_entries"][cache_key] = {
                "stock_code": stock_code,
                "data_hash": data_hash,
                "cached_time": datetime.now().isoformat(),
                "version": self.version,
                "file_size": os.path.getsize(cache_path)
            }
            self._save_metadata()
            
            logger.info(f"ğŸ’¾ ç¼“å­˜ä¿å­˜: {stock_code}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥ {stock_code}: {e}")
    
    def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        expired_keys = []
        current_time = datetime.now()
        
        for cache_key, cache_info in self.metadata["cache_entries"].items():
            cached_time = datetime.fromisoformat(cache_info["cached_time"])
            if current_time - cached_time > timedelta(days=self.cache_days):
                expired_keys.append(cache_key)
        
        for cache_key in expired_keys:
            try:
                # åˆ é™¤ç¼“å­˜æ–‡ä»¶
                cache_path = self._get_cache_path(cache_key)
                if os.path.exists(cache_path):
                    os.remove(cache_path)
                
                # ä»å…ƒæ•°æ®ä¸­ç§»é™¤
                del self.metadata["cache_entries"][cache_key]
                
            except Exception as e:
                logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥ {cache_key}: {e}")
        
        if expired_keys:
            self._save_metadata()
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)} ä¸ª")
    
    def clear_all_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            # åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            for cache_key in list(self.metadata["cache_entries"].keys()):
                cache_path = self._get_cache_path(cache_key)
                if os.path.exists(cache_path):
                    os.remove(cache_path)
            
            # é‡ç½®å…ƒæ•°æ®
            self.metadata = {
                "version": self.version,
                "created_time": datetime.now().isoformat(),
                "cache_entries": {}
            }
            self._save_metadata()
            
            logger.info("ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†")
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_entries = len(self.metadata["cache_entries"])
        total_size = sum(entry.get("file_size", 0) for entry in self.metadata["cache_entries"].values())
        
        hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0
        
        return {
            "cache_entries": total_entries,
            "total_size_mb": total_size / (1024 * 1024),
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": hit_rate,
            "version": self.version,
            "cache_dir": self.cache_dir
        }
    
    def print_cache_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_cache_stats()
        print("\nğŸ“Š ç‰¹å¾ç¼“å­˜ç»Ÿè®¡:")
        print(f"  ç¼“å­˜æ¡ç›®: {stats['cache_entries']}")
        print(f"  æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        print(f"  å‘½ä¸­æ¬¡æ•°: {stats['cache_hits']}")
        print(f"  æœªå‘½ä¸­æ¬¡æ•°: {stats['cache_misses']}")
        print(f"  å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
        print(f"  ç‰ˆæœ¬: {stats['version']}")
        print(f"  ç¼“å­˜ç›®å½•: {stats['cache_dir']}")


class BatchFeatureProcessor:
    """
    æ‰¹é‡ç‰¹å¾å¤„ç†å™¨
    - æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    - æ™ºèƒ½ç¼“å­˜ç®¡ç†
    - è¿›åº¦ç›‘æ§
    """
    
    def __init__(self, feature_engineer, cache_dir: str = "cache/features", 
                 cache_days: int = 7, parallel_workers: int = 4):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            feature_engineer: ç‰¹å¾å·¥ç¨‹å®ä¾‹
            cache_dir: ç¼“å­˜ç›®å½•
            cache_days: ç¼“å­˜æœ‰æ•ˆæœŸ
            parallel_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        """
        self.feature_engineer = feature_engineer
        self.cache = FeatureCache(cache_dir, cache_days)
        self.parallel_workers = parallel_workers
        
    def process_stocks_with_cache(self, stock_codes: list, data_loader_func, 
                                 show_progress: bool = True) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡å¤„ç†è‚¡ç¥¨ç‰¹å¾ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            data_loader_func: æ•°æ®åŠ è½½å‡½æ•°ï¼Œæ¥å—è‚¡ç¥¨ä»£ç è¿”å›DataFrame
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            è‚¡ç¥¨ä»£ç åˆ°ç‰¹å¾DataFrameçš„æ˜ å°„
        """
        results = {}
        cached_count = 0
        processed_count = 0
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self.cache.clear_expired_cache()
        
        if show_progress:
            print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(stock_codes)} åªè‚¡ç¥¨çš„ç‰¹å¾...")
        
        for i, stock_code in enumerate(stock_codes):
            try:
                if show_progress and i % 10 == 0:
                    print(f"è¿›åº¦: {i+1}/{len(stock_codes)} ({(i+1)/len(stock_codes)*100:.1f}%)")
                
                # åŠ è½½åŸå§‹æ•°æ®
                df_original = data_loader_func(stock_code)
                if df_original is None or len(df_original) == 0:
                    logger.warning(f"è‚¡ç¥¨ {stock_code} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # å°è¯•ä»ç¼“å­˜è·å–
                df_features = self.cache.get_cached_features(stock_code, df_original)
                
                if df_features is not None:
                    # ç¼“å­˜å‘½ä¸­
                    cached_count += 1
                    results[stock_code] = df_features
                else:
                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è®¡ç®—ç‰¹å¾
                    if show_progress:
                        print(f"ğŸ”§ å¤„ç†è‚¡ç¥¨ {stock_code} ç‰¹å¾...")
                    
                    df_features = self.feature_engineer.create_all_features(df_original, stock_code)
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self.cache.save_features_to_cache(stock_code, df_original, df_features)
                    
                    processed_count += 1
                    results[stock_code] = df_features
                
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_code} å¤±è´¥: {e}")
                continue
        
        if show_progress:
            print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"  æ€»è‚¡ç¥¨æ•°: {len(stock_codes)}")
            print(f"  æˆåŠŸå¤„ç†: {len(results)}")
            print(f"  ç¼“å­˜å‘½ä¸­: {cached_count}")
            print(f"  æ–°è®¡ç®—: {processed_count}")
            
            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            self.cache.print_cache_stats()
        
        return results