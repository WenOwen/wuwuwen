# -*- coding: utf-8 -*-
"""
AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - é¢„æµ‹æœåŠ¡æ¨¡å—
åŠŸèƒ½ï¼šæä¾›å®æ—¶é¢„æµ‹æœåŠ¡ã€é¢„æµ‹ç»“æœç®¡ç†å’ŒAPIæ¥å£
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import joblib
import json
import redis
from datetime import datetime, timedelta
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio
import schedule
import time
from threading import Thread

# å¯¼å…¥å¤„ç† - æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—å¯¼å…¥
try:
    from .feature_engineering import FeatureEngineering
    from .ai_models import EnsembleModel, create_ensemble_model
    from .training_pipeline import ModelTrainingPipeline, AutoRetrainingSystem
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from feature_engineering import FeatureEngineering
    from ai_models import EnsembleModel, create_ensemble_model
    from training_pipeline import ModelTrainingPipeline, AutoRetrainingSystem

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    stock_code: str
    prediction_days: int = 1
    include_probability: bool = True
    include_analysis: bool = True


class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    stock_code: str
    prediction_days: int
    prediction: int  # 0: ä¸‹è·Œ, 1: ä¸Šæ¶¨
    probability: float
    confidence: str  # 'low', 'medium', 'high'
    current_price: float
    predicted_direction: str
    analysis: Dict
    timestamp: str


class RiskAssessment(BaseModel):
    """é£é™©è¯„ä¼°æ¨¡å‹"""
    stock_code: str
    risk_level: str  # 'low', 'medium', 'high'
    volatility_forecast: float
    max_drawdown_forecast: float
    stop_loss_suggestion: float
    position_size_suggestion: float
    risk_factors: List[str]


class PredictionService:
    """
    é¢„æµ‹æœåŠ¡ç±»
    æä¾›è‚¡ç¥¨é¢„æµ‹ã€é£é™©è¯„ä¼°ã€æ‰¹é‡é¢„æµ‹ç­‰åŠŸèƒ½
    """
    
    def __init__(self, model_dir: str = "models", 
                 data_dir: str = "data/datas_em",
                 redis_host: str = "localhost",
                 redis_port: int = 6379):
        self.model_dir = model_dir
        self.data_dir = data_dir
        self.feature_engineer = FeatureEngineering()
        self.models = {}  # prediction_days -> model
        self.model_metadata = {}
        
        # Redisç¼“å­˜
        try:
            self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis_client.ping()
            self.use_cache = True
            logger.info("Redisç¼“å­˜è¿æ¥æˆåŠŸ")
        except:
            self.redis_client = None
            self.use_cache = False
            logger.warning("Redisç¼“å­˜è¿æ¥å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨ç¼“å­˜")
        
        # åŠ è½½æ¨¡å‹
        self.load_models()
        
        # é¢„æµ‹å†å²è®°å½•
        self.prediction_history = []
        
        # é£é™©è¯„ä¼°é…ç½®
        self.risk_config = {
            'volatility_threshold_low': 0.02,
            'volatility_threshold_high': 0.05,
            'confidence_threshold_low': 0.6,
            'confidence_threshold_high': 0.8,
        }
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰å·²è®­ç»ƒçš„æ¨¡å‹"""
        if not os.path.exists(self.model_dir):
            logger.warning(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.model_dir}")
            return
        
        model_folders = [f for f in os.listdir(self.model_dir) 
                        if os.path.isdir(os.path.join(self.model_dir, f))]
        
        for folder in model_folders:
            try:
                folder_path = os.path.join(self.model_dir, folder)
                
                # åŠ è½½è®­ç»ƒä¿¡æ¯
                info_path = os.path.join(folder_path, 'training_info.pkl')
                if os.path.exists(info_path):
                    training_info = joblib.load(info_path)
                    prediction_days = training_info['prediction_days']
                    
                    # ä»è®­ç»ƒä¿¡æ¯ä¸­è·å–æ­£ç¡®çš„å‚æ•°
                    # æ ¹æ®é”™è¯¯ä¿¡æ¯ï¼Œæ¨¡å‹æœŸæœ›çš„æ˜¯(None, 30, 170)çš„è¾“å…¥å½¢çŠ¶
                    sequence_length = training_info.get('sequence_length', 30)  # æ”¹ä¸ºé»˜è®¤30
                    n_features = len(training_info['feature_names'])
                    
                    logger.info(f"åˆ›å»ºæ¨¡å‹: sequence_length={sequence_length}, n_features={n_features}")
                    
                    # åˆ›å»ºæ¨¡å‹å®ä¾‹
                    model = create_ensemble_model(
                        sequence_length=sequence_length,
                        n_features=n_features
                    )
                    
                    # åŠ è½½å„å­æ¨¡å‹
                    self._load_individual_models(model, folder_path)
                    
                    # ç¡®ä¿training_infoä¸­åŒ…å«sequence_length
                    training_info['sequence_length'] = sequence_length
                    
                    self.models[prediction_days] = model
                    self.model_metadata[prediction_days] = training_info
                    
                    logger.info(f"åŠ è½½ {prediction_days} å¤©é¢„æµ‹æ¨¡å‹æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ¨¡å‹ {folder} å¤±è´¥: {str(e)}")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹")
    
    def _load_individual_models(self, ensemble_model: EnsembleModel, model_path: str):
        """åŠ è½½é›†æˆæ¨¡å‹ä¸­çš„å„ä¸ªå­æ¨¡å‹"""
        try:
            import tensorflow as tf
            import lightgbm as lgb
            from sklearn.preprocessing import StandardScaler
            
            # åŠ è½½æ¨¡å‹æƒé‡
            weights_path = os.path.join(model_path, 'model_weights.pkl')
            if os.path.exists(weights_path):
                ensemble_model.model_weights = joblib.load(weights_path)
            
            # åŠ è½½å„ä¸ªå­æ¨¡å‹
            model_files = {
                'LSTM': ('LSTM_model.h5', 'LSTM_scaler.pkl'),
                'CNN-LSTM': ('CNN-LSTM_model.h5', 'CNN-LSTM_scaler.pkl'),
                'Transformer': ('Transformer_model.h5', 'Transformer_scaler.pkl'),
                'LightGBM': ('LightGBM_model.pkl', 'LightGBM_scaler.pkl')
            }
            
            for model_name, (model_file, scaler_file) in model_files.items():
                model_file_path = os.path.join(model_path, model_file)
                scaler_file_path = os.path.join(model_path, scaler_file)
                
                if os.path.exists(model_file_path) and model_name in ensemble_model.models:
                    try:
                        # åŠ è½½æ¨¡å‹
                        if model_name == 'LightGBM':
                            loaded_model = joblib.load(model_file_path)
                        else:
                            loaded_model = tf.keras.models.load_model(model_file_path)
                        
                        # åŠ è½½å¯¹åº”çš„scaler
                        if os.path.exists(scaler_file_path):
                            scaler = joblib.load(scaler_file_path)
                            ensemble_model.models[model_name].scaler = scaler
                        
                        # è®¾ç½®æ¨¡å‹ä¸ºå·²è®­ç»ƒçŠ¶æ€
                        ensemble_model.models[model_name].model = loaded_model
                        ensemble_model.models[model_name].is_fitted = True
                        
                        logger.info(f"æˆåŠŸåŠ è½½å­æ¨¡å‹: {model_name}")
                        
                    except Exception as e:
                        logger.warning(f"åŠ è½½å­æ¨¡å‹ {model_name} å¤±è´¥: {str(e)}")
                        continue
            
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹åŠ è½½æˆåŠŸ
            loaded_models = sum(1 for model in ensemble_model.models.values() if model.is_fitted)
            if loaded_models == 0:
                logger.warning("æ²¡æœ‰ä»»ä½•å­æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå°†ä½¿ç”¨è§„åˆ™é¢„æµ‹")
                # è®¾ç½®ä¸€ä¸ªç®€å•çš„è§„åˆ™æ¨¡å‹ä½œä¸ºåå¤‡
                self._setup_fallback_model(ensemble_model)
            else:
                logger.info(f"æˆåŠŸåŠ è½½ {loaded_models} ä¸ªå­æ¨¡å‹")
                
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            # è®¾ç½®åå¤‡æ¨¡å‹
            self._setup_fallback_model(ensemble_model)
    
    def _setup_fallback_model(self, ensemble_model: EnsembleModel):
        """è®¾ç½®åå¤‡æ¨¡å‹ï¼ˆç®€å•è§„åˆ™æ¨¡å‹ï¼‰"""
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„è§„åˆ™é¢„æµ‹æ¨¡å‹
            class SimpleFallbackModel:
                def __init__(self):
                    self.is_fitted = True
                
                def predict(self, X):
                    # ç®€å•è§„åˆ™ï¼šåŸºäºæœ€åå‡ å¤©çš„ä»·æ ¼è¶‹åŠ¿
                    if len(X) == 0:
                        return np.array([1])  # é»˜è®¤é¢„æµ‹ä¸Šæ¶¨
                    
                    # è®¡ç®—ç®€å•çš„è¶‹åŠ¿
                    recent_prices = X[0, -5:, 0] if X.shape[1] >= 5 else X[0, :, 0]
                    if len(recent_prices) >= 2:
                        trend = (recent_prices[-1] - recent_prices[0]) / recent_prices[0]
                        return np.array([1 if trend > 0 else 0])
                    else:
                        return np.array([1])
                
                def predict_proba(self, X):
                    pred = self.predict(X)
                    prob = 0.6 if pred[0] == 1 else 0.4
                    return np.array([[1-prob, prob]])
            
            # æ¸…ç©ºç°æœ‰æ¨¡å‹ï¼Œåªä½¿ç”¨åå¤‡æ¨¡å‹
            ensemble_model.models.clear()
            ensemble_model.model_weights.clear()
            
            # è®¾ç½®åå¤‡æ¨¡å‹
            ensemble_model.models['Fallback'] = SimpleFallbackModel()
            ensemble_model.model_weights['Fallback'] = 1.0
            
            logger.info("å·²è®¾ç½®ç®€å•è§„åˆ™åå¤‡æ¨¡å‹")
            
        except Exception as e:
            logger.error(f"è®¾ç½®åå¤‡æ¨¡å‹å¤±è´¥: {str(e)}")
            raise ValueError("æ— æ³•åˆå§‹åŒ–ä»»ä½•é¢„æµ‹æ¨¡å‹")
    
    def get_latest_stock_data(self, stock_code: str, days: int = 100) -> pd.DataFrame:
        """è·å–æœ€æ–°è‚¡ç¥¨æ•°æ®"""
        file_path = os.path.join(self.data_dir, f"{stock_code}.csv")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è‚¡ç¥¨æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {stock_code}")
        
        df = pd.read_csv(file_path)
        df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
        df = df.sort_values('äº¤æ˜“æ—¥æœŸ').reset_index(drop=True)
        
        # è¿”å›æœ€è¿‘çš„æ•°æ®
        return df.tail(days).reset_index(drop=True)
    
    def predict_single_stock(self, stock_code: str, prediction_days: int = 1,
                           include_analysis: bool = True, 
                           prediction_threshold: float = 0.6) -> PredictionResponse:
        """
        å•åªè‚¡ç¥¨é¢„æµ‹
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            prediction_days: é¢„æµ‹å¤©æ•°
            include_analysis: æ˜¯å¦åŒ…å«åˆ†æä¿¡æ¯
            prediction_threshold: é¢„æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼Œå¤§äºæ­¤å€¼é¢„æµ‹ä¸Šæ¶¨ï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"prediction:{stock_code}:{prediction_days}"
        if self.use_cache:
            cached_result = self.redis_client.get(cache_key)
            if cached_result:
                logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {stock_code}")
                return PredictionResponse(**json.loads(cached_result))
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if prediction_days not in self.models:
            raise ValueError(f"ä¸æ”¯æŒ {prediction_days} å¤©é¢„æµ‹")
        
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            df = self.get_latest_stock_data(stock_code)
            current_price = df['æ”¶ç›˜ä»·'].iloc[-1]
            
            # ç‰¹å¾å·¥ç¨‹
            df_features = self.feature_engineer.create_all_features(df, stock_code)
            
            # è·å–æ¨¡å‹çš„sequence_length
            model = self.models[prediction_days]
            model_metadata = self.model_metadata.get(prediction_days, {})
            sequence_length = model_metadata.get('sequence_length', 30)  # æ”¹ä¸ºé»˜è®¤30
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            X, _, feature_names, _ = self.feature_engineer.prepare_model_data(
                df_features, 
                prediction_days=prediction_days,
                lookback_window=sequence_length
            )
            
            if len(X) == 0:
                raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            
            # ä½¿ç”¨æœ€åä¸€ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
            X_pred = X[-1].reshape(1, *X[-1].shape)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å®é™…çš„æ•°æ®å½¢çŠ¶
            logger.info(f"è°ƒè¯•ä¿¡æ¯ - sequence_length: {sequence_length}")
            logger.info(f"è°ƒè¯•ä¿¡æ¯ - X.shape: {X.shape}")
            logger.info(f"è°ƒè¯•ä¿¡æ¯ - X_pred.shape: {X_pred.shape}")
            logger.info(f"è°ƒè¯•ä¿¡æ¯ - æœŸæœ›å½¢çŠ¶: (1, {sequence_length}, {len(feature_names)})")
            
            # è·å–æ¨¡å‹
            model = self.models[prediction_days]
            
            # é¢„æµ‹
            prediction = model.predict(X_pred)[0]
            raw_probability = model.predict_proba(X_pred)[0, 1]  # ä¸Šæ¶¨æ¦‚ç‡
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå„ä¸ªå­æ¨¡å‹çš„é¢„æµ‹
            if hasattr(model, 'models') and model.models:
                logger.info("ğŸ” å„å­æ¨¡å‹é¢„æµ‹è¯¦æƒ…:")
                for sub_model_name, sub_model in model.models.items():
                    if hasattr(sub_model, 'is_fitted') and sub_model.is_fitted:
                        try:
                            sub_pred = sub_model.predict(X_pred)[0]
                            sub_prob = sub_model.predict_proba(X_pred)[0, 1]
                            weight = model.model_weights.get(sub_model_name, 0)
                            logger.info(f"  {sub_model_name}: é¢„æµ‹={sub_pred}, æ¦‚ç‡={sub_prob:.3f}, æƒé‡={weight:.3f}")
                        except Exception as e:
                            logger.warning(f"  {sub_model_name}: é¢„æµ‹å¤±è´¥ - {str(e)}")
                logger.info(f"ğŸ¯ é›†æˆç»“æœ: é¢„æµ‹={prediction}, åŸå§‹æ¦‚ç‡={raw_probability:.3f}")
            
            # åŸºäºç”¨æˆ·è®¾å®šçš„é˜ˆå€¼åˆ¤æ–­é¢„æµ‹æ–¹å‘
            if raw_probability > prediction_threshold:
                prediction = 1
                predicted_direction = "ä¸Šæ¶¨"
            else:
                prediction = 0
                predicted_direction = "ä¸‹è·Œ"
            
            # ç›´æ¥ä½¿ç”¨åŸå§‹æ¦‚ç‡ï¼Œä¸åšè½¬æ¢
            probability = raw_probability
            
            # ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆåŸºäºæ¦‚ç‡è·ç¦»é˜ˆå€¼çš„è¿œè¿‘ï¼‰
            confidence = self._assess_confidence(probability, prediction_threshold)
            
            # åˆ†æä¿¡æ¯
            analysis = {}
            if include_analysis:
                analysis = self._generate_analysis(df_features, stock_code, prediction_days)
            
            # æ„å»ºå“åº”
            response = PredictionResponse(
                stock_code=stock_code,
                prediction_days=prediction_days,
                prediction=prediction,
                probability=float(probability),
                confidence=confidence,
                current_price=float(current_price),
                predicted_direction=predicted_direction,
                analysis=analysis,
                timestamp=datetime.now().isoformat()
            )
            
            # ç¼“å­˜ç»“æœï¼ˆ5åˆ†é’Ÿï¼‰
            if self.use_cache:
                self.redis_client.setex(
                    cache_key, 
                    300,  # 5åˆ†é’Ÿè¿‡æœŸ
                    json.dumps(response.dict())
                )
            
            # è®°å½•é¢„æµ‹å†å²
            self.prediction_history.append({
                'timestamp': datetime.now(),
                'stock_code': stock_code,
                'prediction_days': prediction_days,
                'prediction': prediction,
                'probability': probability,
                'current_price': current_price
            })
            
            return response
            
        except Exception as e:
            logger.error(f"é¢„æµ‹è‚¡ç¥¨ {stock_code} å¤±è´¥: {str(e)}")
            # å¯¹äºstreamlitä½¿ç”¨ï¼ŒæŠ›å‡ºæ™®é€šå¼‚å¸¸è€Œä¸æ˜¯HTTPException
            raise ValueError(f"é¢„æµ‹å¤±è´¥: {str(e)}")
    
    def _assess_confidence(self, probability: float, prediction_threshold: float = 0.6) -> str:
        """è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦
        
        Args:
            probability: é¢„æµ‹æ¦‚ç‡
            prediction_threshold: é¢„æµ‹é˜ˆå€¼ï¼ˆç”¨äºåˆ¤æ–­ä¸Šæ¶¨/ä¸‹è·Œçš„åˆ†ç•Œç‚¹ï¼‰
        """
        # è®¡ç®—æ¦‚ç‡è·ç¦»é¢„æµ‹é˜ˆå€¼çš„è¿œè¿‘
        distance_from_threshold = abs(probability - prediction_threshold)
        
        # åŸºäºè·ç¦»é˜ˆå€¼çš„è¿œè¿‘è¯„ä¼°ç½®ä¿¡åº¦
        if distance_from_threshold >= 0.2:  # è·ç¦»é˜ˆå€¼20%ä»¥ä¸Š
            return "high"
        elif distance_from_threshold >= 0.1:  # è·ç¦»é˜ˆå€¼10%-20%
            return "medium"
        else:  # è·ç¦»é˜ˆå€¼10%ä»¥å†…
            return "low"
    
    def _generate_analysis(self, df_features: pd.DataFrame, 
                          stock_code: str, prediction_days: int) -> Dict:
        """ç”Ÿæˆé¢„æµ‹åˆ†æä¿¡æ¯"""
        latest_data = df_features.iloc[-1]
        
        analysis = {
            'technical_indicators': {},
            'market_sentiment': {},
            'risk_factors': [],
            'support_resistance': {},
            'trend_analysis': {}
        }
        
        # æŠ€æœ¯æŒ‡æ ‡åˆ†æ
        if 'RSI_14' in df_features.columns:
            rsi = latest_data['RSI_14']
            analysis['technical_indicators']['RSI'] = {
                'value': float(rsi),
                'signal': 'overbought' if rsi > 70 else 'oversold' if rsi < 30 else 'neutral'
            }
        
        if 'MACD' in df_features.columns:
            macd = latest_data['MACD']
            macd_signal = latest_data['MACD_signal']
            analysis['technical_indicators']['MACD'] = {
                'value': float(macd),
                'signal': 'bullish' if macd > macd_signal else 'bearish'
            }
        
        # å¸ƒæ—å¸¦åˆ†æ
        if 'BB_position' in df_features.columns:
            bb_pos = latest_data['BB_position']
            analysis['technical_indicators']['Bollinger'] = {
                'position': float(bb_pos),
                'signal': 'upper' if bb_pos > 0.8 else 'lower' if bb_pos < 0.2 else 'middle'
            }
        
        # è¶‹åŠ¿åˆ†æ
        if 'trend_up' in df_features.columns:
            trend_up = latest_data['trend_up']
            trend_down = latest_data['trend_down']
            analysis['trend_analysis'] = {
                'direction': 'up' if trend_up else 'down' if trend_down else 'sideways',
                'strength': 'strong' if (trend_up or trend_down) else 'weak'
            }
        
        # æˆäº¤é‡åˆ†æ
        if 'volume_ratio_20' in df_features.columns:
            vol_ratio = latest_data['volume_ratio_20']
            analysis['market_sentiment']['volume'] = {
                'ratio': float(vol_ratio),
                'signal': 'high' if vol_ratio > 1.5 else 'low' if vol_ratio < 0.5 else 'normal'
            }
        
        # é£é™©å› ç´ 
        risk_factors = []
        if 'volatility_20d' in df_features.columns:
            volatility = latest_data['volatility_20d']
            if volatility > 0.05:
                risk_factors.append("é«˜æ³¢åŠ¨ç‡")
        
        if 'limit_down' in df_features.columns and latest_data['limit_down']:
            risk_factors.append("è·Œåœé£é™©")
        
        analysis['risk_factors'] = risk_factors
        
        return analysis
    
    def assess_risk(self, stock_code: str) -> RiskAssessment:
        """é£é™©è¯„ä¼°"""
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            df = self.get_latest_stock_data(stock_code)
            df_features = self.feature_engineer.create_all_features(df, stock_code)
            
            latest_data = df_features.iloc[-1]
            current_price = df['æ”¶ç›˜ä»·'].iloc[-1]
            
            # æ³¢åŠ¨ç‡é¢„æµ‹
            volatility_forecast = self._forecast_volatility(df_features)
            
            # æœ€å¤§å›æ’¤é¢„æµ‹
            max_drawdown_forecast = self._forecast_max_drawdown(df_features)
            
            # é£é™©ç­‰çº§è¯„ä¼°
            risk_level = self._assess_risk_level(volatility_forecast, max_drawdown_forecast)
            
            # æ­¢æŸå»ºè®®
            stop_loss_suggestion = self._calculate_stop_loss(current_price, volatility_forecast)
            
            # ä»“ä½å»ºè®®
            position_size_suggestion = self._calculate_position_size(risk_level, volatility_forecast)
            
            # é£é™©å› ç´ 
            risk_factors = self._identify_risk_factors(df_features)
            
            return RiskAssessment(
                stock_code=stock_code,
                risk_level=risk_level,
                volatility_forecast=volatility_forecast,
                max_drawdown_forecast=max_drawdown_forecast,
                stop_loss_suggestion=stop_loss_suggestion,
                position_size_suggestion=position_size_suggestion,
                risk_factors=risk_factors
            )
            
        except Exception as e:
            logger.error(f"é£é™©è¯„ä¼°å¤±è´¥ {stock_code}: {str(e)}")
            raise ValueError(f"é£é™©è¯„ä¼°å¤±è´¥: {str(e)}")
    
    def _forecast_volatility(self, df_features: pd.DataFrame) -> float:
        """é¢„æµ‹æœªæ¥æ³¢åŠ¨ç‡"""
        if 'volatility_20d' in df_features.columns:
            recent_vol = df_features['volatility_20d'].tail(10).mean()
            return float(recent_vol * 1.2)  # é€‚å½“æ”¾å¤§ä½œä¸ºé¢„æµ‹
        return 0.03  # é»˜è®¤å€¼
    
    def _forecast_max_drawdown(self, df_features: pd.DataFrame) -> float:
        """é¢„æµ‹æœ€å¤§å›æ’¤"""
        if 'price_change' in df_features.columns:
            returns = df_features['price_change'].dropna()
            # è®¡ç®—å†å²æœ€å¤§å›æ’¤
            cumulative = (1 + returns).cumprod()
            running_max = cumulative.expanding().max()
            drawdown = (cumulative - running_max) / running_max
            max_drawdown_hist = drawdown.min()
            return float(abs(max_drawdown_hist) * 1.3)  # é€‚å½“æ”¾å¤§
        return 0.15  # é»˜è®¤å€¼
    
    def _assess_risk_level(self, volatility: float, max_drawdown: float) -> str:
        """è¯„ä¼°é£é™©ç­‰çº§"""
        if volatility > 0.05 or max_drawdown > 0.2:
            return "high"
        elif volatility > 0.03 or max_drawdown > 0.1:
            return "medium"
        else:
            return "low"
    
    def _calculate_stop_loss(self, current_price: float, volatility: float) -> float:
        """è®¡ç®—æ­¢æŸä»·æ ¼"""
        # åŸºäºæ³¢åŠ¨ç‡çš„åŠ¨æ€æ­¢æŸ
        stop_loss_pct = max(0.05, volatility * 2)  # æœ€å°‘5%æ­¢æŸ
        return float(current_price * (1 - stop_loss_pct))
    
    def _calculate_position_size(self, risk_level: str, volatility: float) -> float:
        """è®¡ç®—å»ºè®®ä»“ä½å¤§å°"""
        base_position = 0.1  # åŸºç¡€ä»“ä½10%
        
        if risk_level == "low":
            return min(0.2, base_position / volatility * 0.02)
        elif risk_level == "medium":
            return min(0.15, base_position / volatility * 0.015)
        else:  # high risk
            return min(0.1, base_position / volatility * 0.01)
    
    def _identify_risk_factors(self, df_features: pd.DataFrame) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        risk_factors = []
        latest_data = df_features.iloc[-1]
        
        # æŠ€æœ¯é£é™©
        if 'RSI_14' in df_features.columns:
            rsi = latest_data['RSI_14']
            if rsi > 80:
                risk_factors.append("æŠ€æœ¯æŒ‡æ ‡è¶…ä¹°")
            elif rsi < 20:
                risk_factors.append("æŠ€æœ¯æŒ‡æ ‡è¶…å–")
        
        # æˆäº¤é‡å¼‚å¸¸
        if 'volume_ratio_20' in df_features.columns:
            vol_ratio = latest_data['volume_ratio_20']
            if vol_ratio > 3:
                risk_factors.append("æˆäº¤é‡å¼‚å¸¸æ”¾å¤§")
        
        # æ³¢åŠ¨ç‡é£é™©
        if 'volatility_20d' in df_features.columns:
            volatility = latest_data['volatility_20d']
            if volatility > 0.05:
                risk_factors.append("é«˜æ³¢åŠ¨ç‡")
        
        # è¶‹åŠ¿é£é™©
        if 'momentum_20d' in df_features.columns:
            momentum = latest_data['momentum_20d']
            if momentum < -0.2:
                risk_factors.append("ä¸‹è·Œè¶‹åŠ¿å¼ºçƒˆ")
        
        return risk_factors
    
    def batch_predict(self, stock_codes: List[str], prediction_days: int = 1) -> List[PredictionResponse]:
        """æ‰¹é‡é¢„æµ‹"""
        logger.info(f"å¼€å§‹æ‰¹é‡é¢„æµ‹ï¼Œè‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        
        results = []
        for stock_code in stock_codes:
            try:
                result = self.predict_single_stock(
                    stock_code=stock_code,
                    prediction_days=prediction_days,
                    include_analysis=False,  # æ‰¹é‡é¢„æµ‹æ—¶ä¸åŒ…å«è¯¦ç»†åˆ†æ
                    prediction_threshold=0.6  # ä½¿ç”¨é»˜è®¤é˜ˆå€¼
                )
                results.append(result)
                
            except Exception as e:
                logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥ {stock_code}: {str(e)}")
                continue
        
        logger.info(f"æ‰¹é‡é¢„æµ‹å®Œæˆï¼ŒæˆåŠŸ {len(results)} ä¸ª")
        return results
    
    def get_prediction_history(self, stock_code: str = None, 
                             days: int = 30) -> List[Dict]:
        """è·å–é¢„æµ‹å†å²"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        filtered_history = [
            record for record in self.prediction_history
            if record['timestamp'] >= cutoff_date
        ]
        
        if stock_code:
            filtered_history = [
                record for record in filtered_history
                if record['stock_code'] == stock_code
            ]
        
        return filtered_history


# FastAPI åº”ç”¨
app = FastAPI(title="AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ", version="1.0.0")
prediction_service = PredictionService()


@app.get("/")
async def root():
    return {"message": "AIè‚¡å¸‚é¢„æµ‹ç³»ç»ŸAPI", "version": "1.0.0"}


@app.post("/predict", response_model=PredictionResponse)
async def predict_stock(request: PredictionRequest):
    """å•åªè‚¡ç¥¨é¢„æµ‹æ¥å£"""
    return prediction_service.predict_single_stock(
        stock_code=request.stock_code,
        prediction_days=request.prediction_days,
        include_analysis=request.include_analysis,
        prediction_threshold=0.6  # ä½¿ç”¨é»˜è®¤é˜ˆå€¼
    )


@app.post("/predict/batch")
async def batch_predict_stocks(stock_codes: List[str], prediction_days: int = 1):
    """æ‰¹é‡é¢„æµ‹æ¥å£"""
    return prediction_service.batch_predict(stock_codes, prediction_days)


@app.get("/risk/{stock_code}", response_model=RiskAssessment)
async def assess_stock_risk(stock_code: str):
    """é£é™©è¯„ä¼°æ¥å£"""
    return prediction_service.assess_risk(stock_code)


@app.get("/history/{stock_code}")
async def get_prediction_history(stock_code: str, days: int = 30):
    """è·å–é¢„æµ‹å†å²æ¥å£"""
    return prediction_service.get_prediction_history(stock_code, days)


@app.get("/models")
async def get_available_models():
    """è·å–å¯ç”¨æ¨¡å‹ä¿¡æ¯"""
    return {
        "available_models": list(prediction_service.models.keys()),
        "model_metadata": prediction_service.model_metadata
    }


class RealtimeDataUpdater:
    """
    å®æ—¶æ•°æ®æ›´æ–°å™¨
    å®šæœŸæ›´æ–°è‚¡ç¥¨æ•°æ®å’Œé‡è®­ç»ƒæ¨¡å‹
    """
    
    def __init__(self, prediction_service: PredictionService):
        self.prediction_service = prediction_service
        self.pipeline = ModelTrainingPipeline()
        self.auto_retrainer = AutoRetrainingSystem(self.pipeline)
        
    def update_stock_data(self):
        """æ›´æ–°è‚¡ç¥¨æ•°æ®"""
        logger.info("å¼€å§‹æ›´æ–°è‚¡ç¥¨æ•°æ®...")
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨æ‚¨ç°æœ‰çš„æ•°æ®è·å–è„šæœ¬
        # ä¾‹å¦‚ï¼šsamequant_functions.Spider_func()
        try:
            # å‡è®¾ä½¿ç”¨ç°æœ‰çš„æ•°æ®è·å–åŠŸèƒ½
            from samequant_functions import Spider_func
            spider = Spider_func()
            
            # æ›´æ–°ä¸»è¦æŒ‡æ•°å’Œçƒ­é—¨è‚¡ç¥¨
            # spider.update_market_data()
            
            logger.info("è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ›´æ–°è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}")
    
    def check_model_performance(self):
        """æ£€æŸ¥æ¨¡å‹æ€§èƒ½å¹¶è§¦å‘é‡è®­ç»ƒ"""
        logger.info("æ£€æŸ¥æ¨¡å‹æ€§èƒ½...")
        
        # è·å–çƒ­é—¨è‚¡ç¥¨åˆ—è¡¨è¿›è¡Œæ£€æŸ¥
        popular_stocks = ['sh600519', 'sz000001', 'sz000002']  # ç¤ºä¾‹
        
        try:
            self.auto_retrainer.auto_retrain(popular_stocks)
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def start_scheduled_tasks(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        logger.info("å¯åŠ¨å®šæ—¶ä»»åŠ¡...")
        
        # æ¯æ—¥æ”¶ç›˜åæ›´æ–°æ•°æ®
        schedule.every().day.at("15:30").do(self.update_stock_data)
        
        # æ¯å‘¨æ£€æŸ¥æ¨¡å‹æ€§èƒ½
        schedule.every().monday.at("18:00").do(self.check_model_performance)
        
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


def start_realtime_updater():
    """å¯åŠ¨å®æ—¶æ›´æ–°å™¨"""
    updater = RealtimeDataUpdater(prediction_service)
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ
    thread = Thread(target=updater.start_scheduled_tasks, daemon=True)
    thread.start()
    
    logger.info("å®æ—¶æ›´æ–°å™¨å·²å¯åŠ¨")


if __name__ == "__main__":
    import uvicorn
    
    # å¯åŠ¨å®æ—¶æ›´æ–°å™¨
    start_realtime_updater()
    
    # å¯åŠ¨APIæœåŠ¡
    uvicorn.run(app, host="0.0.0.0", port=8000)