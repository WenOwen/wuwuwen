# -*- coding: utf-8 -*-
"""
AIè‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ - æ¨¡å‹æ¶æ„æ¨¡å—
åŠŸèƒ½ï¼šå®ç°å¤šæ¨¡å‹èåˆçš„è‚¡å¸‚é¢„æµ‹ç³»ç»Ÿ
"""

import os
# å¿…é¡»åœ¨å¯¼å…¥TensorFlowä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # åªæ˜¾ç¤ºé”™è¯¯
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # å…³é—­oneDNNæç¤º
os.environ['TF_DISABLE_MKL'] = '1'  # ç¦ç”¨MKLä¼˜åŒ–æç¤º
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'  # GPUå†…å­˜å¢é•¿
# æ³¨æ„ï¼šä¸è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä¿æŒGPUå¯ç”¨

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import joblib
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)

# æ·±åº¦å­¦ä¹ æ¡†æ¶
import tensorflow as tf
# è®¾ç½®TensorFlowå®Œå…¨é™é»˜è¿è¡Œ
tf.get_logger().setLevel('ERROR')
tf.autograph.set_verbosity(0)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

# ç¦ç”¨TensorFlowçš„æ‰€æœ‰infoå’Œwarningè¾“å‡º
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Attention, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.preprocessing import StandardScaler, RobustScaler


class SimplifiedProgressCallback(Callback):
    """ç®€åŒ–çš„è®­ç»ƒè¿›åº¦å›è°ƒ - æ¯è½®æ¬¡ä¸€è¡Œï¼Œé¿å…è·³åŠ¨"""
    
    def __init__(self, model_name="æ¨¡å‹", total_epochs=30):
        super().__init__()
        self.model_name = model_name
        self.current_epoch = 0
        self.total_epochs = total_epochs
        self.last_val_acc = 0
        self.best_val_acc = 0
        
    def on_train_begin(self, logs=None):
        print(f"å¼€å§‹è®­ç»ƒ {self.model_name}ï¼Œæ€»å…± {self.total_epochs} è½®")
        
    def on_epoch_end(self, epoch, logs=None):
        self.current_epoch = epoch + 1
        loss = logs.get('loss', 0)
        acc = logs.get('accuracy', 0)
        val_loss = logs.get('val_loss', None)
        val_acc = logs.get('val_accuracy', None)
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress = int((self.current_epoch / self.total_epochs) * 20)  # 20ä¸ªå­—ç¬¦çš„è¿›åº¦æ¡
        progress_bar = 'â–ˆ' * progress + 'â–‘' * (20 - progress)
        percentage = (self.current_epoch / self.total_epochs) * 100
        
        # è·Ÿè¸ªæœ€ä½³éªŒè¯å‡†ç¡®ç‡
        if val_acc is not None:
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                improvement = "â†‘"
            elif val_acc < self.last_val_acc:
                improvement = "â†“"
            else:
                improvement = "â†’"
            self.last_val_acc = val_acc
            
            print(f"è½®æ¬¡ {self.current_epoch:2d}/{self.total_epochs} [{progress_bar}] {percentage:5.1f}% | "
                  f"æŸå¤±:{loss:.4f} å‡†ç¡®ç‡:{acc:.4f} | éªŒè¯æŸå¤±:{val_loss:.4f} éªŒè¯å‡†ç¡®ç‡:{val_acc:.4f} {improvement}")
        else:
            print(f"è½®æ¬¡ {self.current_epoch:2d}/{self.total_epochs} [{progress_bar}] {percentage:5.1f}% | "
                  f"æŸå¤±:{loss:.4f} å‡†ç¡®ç‡:{acc:.4f}")
    
    def on_train_end(self, logs=None):
        print(f"âœ… {self.model_name}è®­ç»ƒå®Œæˆ (æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f})")


class LightGBMProgressCallback:
    """LightGBMè®­ç»ƒè¿›åº¦å›è°ƒ"""
    def __init__(self, model_name="LightGBM", total_rounds=1000, update_frequency=50):
        self.model_name = model_name
        self.total_rounds = total_rounds
        self.update_frequency = update_frequency
        self.current_round = 0
        self.best_score = float('inf')
        self.no_improve_count = 0
        self.start_time = None

    def __call__(self, env):
        """LightGBMå›è°ƒå‡½æ•°"""
        if self.start_time is None:
            import time
            self.start_time = time.time()
            print(f"å¼€å§‹è®­ç»ƒ {self.model_name}ï¼Œé¢„è®¡æœ€å¤š {self.total_rounds} è½®")
        
        self.current_round = env.iteration + 1
        
        # æ¯éš”ä¸€å®šè½®æ¬¡æ˜¾ç¤ºè¿›åº¦
        if self.current_round % self.update_frequency == 0 or self.current_round == 1:
            progress = min(self.current_round / self.total_rounds, 1.0)
            bar_length = 30
            filled_length = int(bar_length * progress)
            progress_bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            percentage = progress * 100
            
            # è·å–éªŒè¯åˆ†æ•°
            valid_score = None
            improvement = ""
            if env.evaluation_result_list:
                # å–ç¬¬ä¸€ä¸ªéªŒè¯é›†çš„åˆ†æ•°
                valid_score = env.evaluation_result_list[0][2]
                if valid_score < self.best_score:
                    self.best_score = valid_score
                    improvement = "â¬†ï¸"
                    self.no_improve_count = 0
                else:
                    improvement = "â†’"
                    self.no_improve_count += self.update_frequency
            
            if valid_score is not None:
                print(f"è½®æ¬¡ {self.current_round:4d}/{self.total_rounds} [{progress_bar}] {percentage:5.1f}% | "
                      f"éªŒè¯åˆ†æ•°: {valid_score:.6f} {improvement} | æœ€ä½³: {self.best_score:.6f}")
            else:
                print(f"è½®æ¬¡ {self.current_round:4d}/{self.total_rounds} [{progress_bar}] {percentage:5.1f}% | "
                      f"è®­ç»ƒä¸­...")


# æœºå™¨å­¦ä¹ æ¨¡å‹
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# æ¨¡å‹è¯„ä¼°
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error


class StockPredictionModel:
    """
    è‚¡ç¥¨é¢„æµ‹æ¨¡å‹åŸºç±»
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.scaler = None
        self.is_fitted = False
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        raise NotImplementedError
        
    def predict(self, X):
        """é¢„æµ‹"""
        raise NotImplementedError
        
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        raise NotImplementedError


class LSTMModel(StockPredictionModel):
    """
    LSTMæ—¶åºé¢„æµ‹æ¨¡å‹
    """
    
    def __init__(self, sequence_length=60, n_features=50, 
                 lstm_units=128, dropout_rate=0.2):
        super().__init__("LSTM")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºLSTMæ¨¡å‹"""
        model = Sequential([
            LSTM(self.lstm_units, return_sequences=True, 
                 input_shape=(self.sequence_length, self.n_features)),
            Dropout(self.dropout_rate),
            
            LSTM(self.lstm_units // 2, return_sequences=True),
            Dropout(self.dropout_rate),
            
            LSTM(self.lstm_units // 4, return_sequences=False),
            Dropout(self.dropout_rate),
            
            Dense(64, activation='relu'),
            Dropout(self.dropout_rate),
            
            Dense(32, activation='relu'),
            Dense(1, activation='sigmoid')  # äºŒåˆ†ç±»è¾“å‡º
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒLSTMæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            SimplifiedProgressCallback(self.model_name, epochs),
            EarlyStopping(patience=15, restore_best_weights=True, verbose=0),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7, verbose=0)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=0  # å…³é—­é»˜è®¤è¾“å‡ºï¼Œä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
        )
        
        self.is_fitted = True
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†"""
        # é‡å¡‘æ•°æ®ä¸º2Dç”¨äºæ ‡å‡†åŒ–
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        # é‡å¡‘å›3D
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        # è¿”å›[neg_proba, pos_proba]æ ¼å¼
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class TransformerModel(StockPredictionModel):
    """
    Transformeræ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹
    """
    
    def __init__(self, sequence_length=60, n_features=50, 
                 d_model=128, num_heads=8, num_layers=4):
        super().__init__("Transformer")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        inputs = tf.keras.Input(shape=(self.sequence_length, self.n_features))
        
        # è¾“å…¥æŠ•å½±å±‚
        x = Dense(self.d_model)(inputs)
        
        # å¤šå±‚Transformerå—
        for _ in range(self.num_layers):
            # å¤šå¤´æ³¨æ„åŠ›
            attn_output = MultiHeadAttention(
                num_heads=self.num_heads, 
                key_dim=self.d_model // self.num_heads
            )(x, x)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = LayerNormalization()(x + attn_output)
            
            # å‰é¦ˆç½‘ç»œ
            ffn_output = Dense(self.d_model * 2, activation='relu')(x)
            ffn_output = Dense(self.d_model)(ffn_output)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = LayerNormalization()(x + ffn_output)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        
        # åˆ†ç±»å¤´
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.3)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs, outputs)
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†ï¼ˆä¸LSTMç›¸åŒï¼‰
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            SimplifiedProgressCallback(self.model_name, epochs),
            EarlyStopping(patience=15, restore_best_weights=True, verbose=0),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7, verbose=0)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=0  # å…³é—­é»˜è®¤è¾“å‡ºï¼Œä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
        )
        
        self.is_fitted = True
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†ï¼ˆä¸LSTMç›¸åŒï¼‰"""
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class LightGBMModel(StockPredictionModel):
    """
    LightGBMæ¢¯åº¦æå‡æ¨¡å‹
    """
    
    def __init__(self, **params):
        super().__init__("LightGBM")
        self.params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 1000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'verbose': -1,  # ä¿æŒé™é»˜ï¼Œä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ˜¾ç¤º
            **params
        }
        self.scaler = RobustScaler()
        self.training_progress = None
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """è®­ç»ƒLightGBMæ¨¡å‹"""
        # XGBoostä¸éœ€è¦epochs, batch_sizeç­‰æ·±åº¦å­¦ä¹ å‚æ•°ï¼Œç›´æ¥å¿½ç•¥
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # å¦‚æœè¾“å…¥æ˜¯3Dï¼ˆæ—¶åºæ•°æ®ï¼‰ï¼Œéœ€è¦flatten
        if len(X_train.shape) == 3:
            X_train_2d = X_train.reshape(X_train.shape[0], -1)
        else:
            X_train_2d = X_train
        
        # ç¡®ä¿æ•°æ®æ˜¯numpy arrayï¼Œé¿å…ç‰¹å¾åç§°é—®é¢˜
        if hasattr(X_train_2d, 'values'):
            X_train_2d = X_train_2d.values
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train_2d)
        
        # éªŒè¯é›†å¤„ç†
        eval_set = None
        if X_val is not None and y_val is not None:
            if len(X_val.shape) == 3:
                X_val_2d = X_val.reshape(X_val.shape[0], -1)
            else:
                X_val_2d = X_val
            
            # ç¡®ä¿éªŒè¯é›†æ•°æ®ä¹Ÿæ˜¯numpy array
            if hasattr(X_val_2d, 'values'):
                X_val_2d = X_val_2d.values
            
            X_val_scaled = self.scaler.transform(X_val_2d)
            eval_set = [(X_val_scaled, y_val)]
        
        # è®­ç»ƒæ¨¡å‹
        self.model = lgb.LGBMClassifier(**self.params)
        
        # åˆ›å»ºè¿›åº¦å›è°ƒ
        progress_callback = LightGBMProgressCallback(
            model_name=self.model_name,
            total_rounds=self.params.get('n_estimators', 1000),
            update_frequency=50
        )
        
        # LightGBMè®­ç»ƒ
        try:
            callbacks = [progress_callback]
            if eval_set:
                callbacks.append(lgb.early_stopping(50, verbose=False))
            
            self.model.fit(
                X_train_scaled, y_train,
                eval_set=eval_set,
                callbacks=callbacks
            )
        except Exception as e:
            print(f"  âš ï¸ æ—©åœè®­ç»ƒå¤±è´¥: {e}, å°è¯•åŸºæœ¬è®­ç»ƒ")
            # ç®€å•è®­ç»ƒæ–¹å¼ï¼ˆæ— æ—©åœï¼‰
            try:
                self.model = lgb.LGBMClassifier(**self.params)
                self.model.fit(
                    X_train_scaled, y_train,
                    callbacks=[progress_callback]
                )
            except Exception as e2:
                print(f"  âŒ åŸºæœ¬è®­ç»ƒä¹Ÿå¤±è´¥: {e2}")
                raise e2
        
        self.is_fitted = True
        print(f"âœ… {self.model_name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
        if len(X.shape) == 3:
            X_2d = X.reshape(X.shape[0], -1)
        else:
            X_2d = X
        
        # ç¡®ä¿é¢„æµ‹æ•°æ®ä¹Ÿæ˜¯numpy array
        if hasattr(X_2d, 'values'):
            X_2d = X_2d.values
            
        X_scaled = self.scaler.transform(X_2d)
        return self.model.predict(X_scaled)
        
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
            
        if len(X.shape) == 3:
            X_2d = X.reshape(X.shape[0], -1)
        else:
            X_2d = X
        
        # ç¡®ä¿é¢„æµ‹æ•°æ®ä¹Ÿæ˜¯numpy array
        if hasattr(X_2d, 'values'):
            X_2d = X_2d.values
            
        X_scaled = self.scaler.transform(X_2d)
        return self.model.predict_proba(X_scaled)


class CNNLSTMModel(StockPredictionModel):
    """
    CNN-LSTMæ··åˆæ¨¡å‹ï¼Œç”¨äºæ•æ‰å±€éƒ¨æ¨¡å¼å’Œé•¿æœŸä¾èµ–
    """
    
    def __init__(self, sequence_length=60, n_features=50):
        super().__init__("CNN-LSTM")
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.scaler = StandardScaler()
        
    def _build_model(self):
        """æ„å»ºCNN-LSTMæ¨¡å‹"""
        model = Sequential([
            # CNNå±‚æå–å±€éƒ¨ç‰¹å¾
            Conv1D(filters=64, kernel_size=3, activation='relu', 
                   input_shape=(self.sequence_length, self.n_features)),
            MaxPooling1D(pool_size=2),
            
            Conv1D(filters=128, kernel_size=3, activation='relu'),
            MaxPooling1D(pool_size=2),
            
            Conv1D(filters=64, kernel_size=3, activation='relu'),
            
            # LSTMå±‚æ•æ‰æ—¶åºä¾èµ–
            LSTM(100, return_sequences=True),
            Dropout(0.3),
            
            LSTM(50, return_sequences=False),
            Dropout(0.3),
            
            # å…¨è¿æ¥å±‚
            Dense(50, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ{self.model_name}æ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self._preprocess_data(X_train, fit_scaler=True)
        
        if X_val is not None:
            X_val_scaled = self._preprocess_data(X_val, fit_scaler=False)
            validation_data = (X_val_scaled, y_val)
        else:
            validation_data = None
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®­ç»ƒå›è°ƒ
        callbacks = [
            SimplifiedProgressCallback(self.model_name, epochs),
            EarlyStopping(patience=15, restore_best_weights=True, verbose=0),
            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7, verbose=0)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=0  # å…³é—­é»˜è®¤è¾“å‡ºï¼Œä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
        )
        
        self.is_fitted = True
        return history
    
    def _preprocess_data(self, X, fit_scaler=False):
        """æ•°æ®é¢„å¤„ç†"""
        n_samples, n_timesteps, n_features = X.shape
        X_2d = X.reshape(-1, n_features)
        
        if fit_scaler:
            X_scaled_2d = self.scaler.fit_transform(X_2d)
        else:
            X_scaled_2d = self.scaler.transform(X_2d)
        
        X_scaled = X_scaled_2d.reshape(n_samples, n_timesteps, n_features)
        return X_scaled
    
    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        predictions = self.model.predict(X_scaled)
        return (predictions > 0.5).astype(int).flatten()
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self._preprocess_data(X, fit_scaler=False)
        probas = self.model.predict(X_scaled)
        return np.column_stack([1 - probas.flatten(), probas.flatten()])


class EnsembleModel:
    """
    é›†æˆæ¨¡å‹ - å¤šæ¨¡å‹èåˆé¢„æµ‹ç³»ç»Ÿ
    """
    
    def __init__(self, model_weights: Dict[str, float] = None):
        """
        åˆå§‹åŒ–é›†æˆæ¨¡å‹
        
        Args:
            model_weights: æ¨¡å‹æƒé‡å­—å…¸ï¼Œå¦‚ {'LSTM': 0.4, 'LightGBM': 0.3, 'Transformer': 0.2, 'CNN-LSTM': 0.1}
        """
        self.models = {}
        self.model_weights = model_weights or {
            'LSTM': 0.4,
            'LightGBM': 0.3, 
            'Transformer': 0.2,
            'CNN-LSTM': 0.1
        }
        self.performance_history = []
        
    def add_model(self, model: StockPredictionModel, weight: float = None):
        """æ·»åŠ æ¨¡å‹åˆ°é›†æˆ"""
        self.models[model.model_name] = model
        if weight is not None:
            self.model_weights[model.model_name] = weight
        
    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹...")
        
        # è®­ç»ƒå„ä¸ªå­æ¨¡å‹
        for model_name, model in self.models.items():
            print(f"\n--- è®­ç»ƒ {model_name} ---")
            model.fit(X_train, y_train, X_val, y_val, **kwargs)
        
        # åŠ¨æ€è°ƒæ•´æƒé‡ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½ï¼‰
        if X_val is not None and y_val is not None:
            self._adjust_weights(X_val, y_val)
        
        print("âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
    def _adjust_weights(self, X_val, y_val):
        """åŸºäºéªŒè¯é›†æ€§èƒ½åŠ¨æ€è°ƒæ•´æ¨¡å‹æƒé‡"""
        print("\nğŸ”§ åŸºäºéªŒè¯é›†æ€§èƒ½è°ƒæ•´æ¨¡å‹æƒé‡...")
        
        model_scores = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                y_pred = model.predict(X_val)
                accuracy = accuracy_score(y_val, y_pred)
                model_scores[model_name] = accuracy
                print(f"{model_name} éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åŸºäºæ€§èƒ½é‡æ–°åˆ†é…æƒé‡
        total_score = sum(model_scores.values())
        if total_score > 0:
            for model_name in self.model_weights.keys():
                if model_name in model_scores:
                    self.model_weights[model_name] = model_scores[model_name] / total_score
        
        print("è°ƒæ•´åçš„æ¨¡å‹æƒé‡:")
        for model_name, weight in self.model_weights.items():
            print(f"  {model_name}: {weight:.3f}")
    
    def predict(self, X):
        """é›†æˆé¢„æµ‹"""
        if not self.models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        # æ”¶é›†å„æ¨¡å‹é¢„æµ‹
        predictions = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                predictions[model_name] = model.predict(X)
        
        if not predictions:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•çš„è¶‹åŠ¿é¢„æµ‹ä½œä¸ºåå¤‡
            return self._fallback_predict(X)
        
        # åŠ æƒå¹³å‡ï¼ˆç¡¬æŠ•ç¥¨ï¼‰
        ensemble_pred = np.zeros(len(predictions[list(predictions.keys())[0]]))
        
        for model_name, pred in predictions.items():
            weight = self.model_weights.get(model_name, 0)
            ensemble_pred += weight * pred
        
        return (ensemble_pred > 0.5).astype(int)
    
    def _fallback_predict(self, X):
        """åå¤‡é¢„æµ‹æ–¹æ³•ï¼šåŸºäºç®€å•çš„ä»·æ ¼è¶‹åŠ¿"""
        if len(X) == 0:
            return np.array([1])  # é»˜è®¤é¢„æµ‹ä¸Šæ¶¨
        
        # å¯¹äºæ‰¹é‡é¢„æµ‹ï¼Œé€ä¸ªå¤„ç†
        predictions = []
        for i in range(len(X)):
            # è®¡ç®—ç®€å•çš„è¶‹åŠ¿
            sample = X[i]
            if sample.shape[0] >= 5:  # å¦‚æœæœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
                recent_prices = sample[-5:, 0]  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ä»·æ ¼
                if len(recent_prices) >= 2:
                    trend = (recent_prices[-1] - recent_prices[0]) / recent_prices[0]
                    prediction = 1 if trend > 0 else 0
                else:
                    prediction = 1  # é»˜è®¤ä¸Šæ¶¨
            else:
                prediction = 1  # é»˜è®¤ä¸Šæ¶¨
            predictions.append(prediction)
        
        return np.array(predictions)
    
    def predict_proba(self, X):
        """é›†æˆæ¦‚ç‡é¢„æµ‹"""
        if not self.models:
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œä½¿ç”¨åå¤‡é¢„æµ‹
            pred = self._fallback_predict(X)
            # è½¬æ¢ä¸ºæ¦‚ç‡æ ¼å¼
            proba = np.zeros((len(pred), 2))
            for i, p in enumerate(pred):
                if p == 1:
                    proba[i] = [0.4, 0.6]  # ä¸Šæ¶¨æ¦‚ç‡60%
                else:
                    proba[i] = [0.6, 0.4]  # ä¸‹è·Œæ¦‚ç‡60%
            return proba
        
        # æ”¶é›†å„æ¨¡å‹æ¦‚ç‡é¢„æµ‹
        prob_predictions = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                prob_predictions[model_name] = model.predict_proba(X)
        
        if not prob_predictions:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨åå¤‡é¢„æµ‹
            pred = self._fallback_predict(X)
            # è½¬æ¢ä¸ºæ¦‚ç‡æ ¼å¼
            proba = np.zeros((len(pred), 2))
            for i, p in enumerate(pred):
                if p == 1:
                    proba[i] = [0.4, 0.6]  # ä¸Šæ¶¨æ¦‚ç‡60%
                else:
                    proba[i] = [0.6, 0.4]  # ä¸‹è·Œæ¦‚ç‡60%
            return proba
        
        # åŠ æƒå¹³å‡æ¦‚ç‡
        first_pred = list(prob_predictions.values())[0]
        ensemble_proba = np.zeros_like(first_pred)
        
        for model_name, proba in prob_predictions.items():
            weight = self.model_weights.get(model_name, 0)
            ensemble_proba += weight * proba
        
        return ensemble_proba
    
    def evaluate(self, X_test, y_test):
        """è¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # å„å­æ¨¡å‹æ€§èƒ½
        results = {}
        for model_name, model in self.models.items():
            if model.is_fitted:
                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1]
                
                accuracy = accuracy_score(y_test, y_pred)
                results[model_name] = {
                    'accuracy': accuracy,
                    'predictions': y_pred,
                    'probabilities': y_proba
                }
                
                print(f"  {model_name}: å‡†ç¡®ç‡ {accuracy:.4f}")
        
        # é›†æˆæ¨¡å‹æ€§èƒ½
        ensemble_pred = self.predict(X_test)
        ensemble_proba = self.predict_proba(X_test)[:, 1]
        ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
        
        results['Ensemble'] = {
            'accuracy': ensemble_accuracy,
            'predictions': ensemble_pred,
            'probabilities': ensemble_proba
        }
        
        print(f"  é›†æˆæ¨¡å‹: å‡†ç¡®ç‡ {ensemble_accuracy:.4f}")
        print(f"ğŸ“ˆ æœ€ç»ˆæ€§èƒ½: {ensemble_accuracy:.4f}")
        
        return results
    
    def save_models(self, save_dir: str):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        for model_name, model in self.models.items():
            if model.is_fitted:
                if hasattr(model.model, 'save'):  # Kerasæ¨¡å‹
                    model.model.save(f"{save_dir}/{model_name}_model.h5")
                    joblib.dump(model.scaler, f"{save_dir}/{model_name}_scaler.pkl")
                else:  # sklearnæ¨¡å‹
                    joblib.dump(model.model, f"{save_dir}/{model_name}_model.pkl")
                    joblib.dump(model.scaler, f"{save_dir}/{model_name}_scaler.pkl")
        
        # ä¿å­˜æƒé‡
        joblib.dump(self.model_weights, f"{save_dir}/model_weights.pkl")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")


def create_ensemble_model(sequence_length=60, n_features=50) -> EnsembleModel:
    """
    åˆ›å»ºå®Œæ•´çš„é›†æˆæ¨¡å‹
    
    Args:
        sequence_length: æ—¶åºé•¿åº¦
        n_features: ç‰¹å¾æ•°é‡
        
    Returns:
        EnsembleModelå®ä¾‹
    """
    
    # åˆ›å»ºé›†æˆæ¨¡å‹
    ensemble = EnsembleModel()
    
    # æ·»åŠ å„ä¸ªå­æ¨¡å‹
    lstm_model = LSTMModel(sequence_length, n_features)
    lgb_model = LightGBMModel()
    transformer_model = TransformerModel(sequence_length, n_features)
    cnn_lstm_model = CNNLSTMModel(sequence_length, n_features)
    
    ensemble.add_model(lstm_model)
    ensemble.add_model(lgb_model)
    ensemble.add_model(transformer_model)
    ensemble.add_model(cnn_lstm_model)
    
    return ensemble


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•AIæ¨¡å‹æ¶æ„...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    n_samples, sequence_length, n_features = 1000, 60, 50
    X = np.random.randn(n_samples, sequence_length, n_features)
    y = np.random.randint(0, 2, n_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split_idx = int(0.8 * n_samples)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # åˆ›å»ºå’Œè®­ç»ƒé›†æˆæ¨¡å‹
    ensemble = create_ensemble_model(sequence_length, n_features)
    ensemble.fit(X_train, y_train, X_test, y_test, epochs=5, batch_size=32)
    
    # è¯„ä¼°æ¨¡å‹
    results = ensemble.evaluate(X_test, y_test)
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")