#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤è®­ç»ƒè„šæœ¬ï¼šæ›¿æ¢ä¸ºç®€å•çš„æ–‡ä»¶é…å¯¹æ–¹æ¡ˆ
"""

def fix_training_script():
    """ä¿®å¤è®­ç»ƒè„šæœ¬"""
    print("ğŸ”§ å¼€å§‹ä¿®å¤è®­ç»ƒè„šæœ¬...")
    
    # è¯»å–åŸæ–‡ä»¶
    with open('lightgbm_stock_train.py', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"   åŸæ–‡ä»¶è¡Œæ•°: {len(lines)}")
    
    # æ–°çš„_load_direct_dataæ–¹æ³•
    new_method = '''    def _load_direct_data(self) -> bool:
        """ç›´æ¥åŠ è½½parquetæ ¼å¼çš„è‚¡ç¥¨æ•°æ® - ç®€åŒ–ç‰ˆ"""
        try:
            self.logger.info("ğŸ“Š ä½¿ç”¨ç›´æ¥è®­ç»ƒæ¨¡å¼åŠ è½½æ•°æ®...")
            
            data_config = self.config.get('data', {})
            direct_training = data_config.get('direct_training', {})
            
            data_dir = Path(data_config.get('data_dir', './data/professional_parquet'))
            data_format = direct_training.get('data_format', 'parquet')
            target_column = direct_training.get('target_column', 'æ¶¨è·Œå¹…')
            exclude_columns = direct_training.get('exclude_columns', ['name', 'æ¶¨è·Œå¹…'])
            
            # åŠ è½½æ•°æ®æ–‡ä»¶
            if data_format == 'parquet':
                # æŸ¥æ‰¾parquetæ–‡ä»¶
                parquet_files = list(data_dir.glob("*.parquet"))
                if not parquet_files:
                    self.logger.error(f"âŒ åœ¨{data_dir}ä¸­æœªæ‰¾åˆ°parquetæ–‡ä»¶")
                    return False
                
                self.logger.info(f"   å‘ç° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
                
                # ä½¿ç”¨ç®€å•é«˜æ•ˆçš„æ–‡ä»¶é…å¯¹æ–¹æ¡ˆ
                parquet_files = sorted(parquet_files)  # æŒ‰æ—¥æœŸæ’åº
                self.logger.info("   ğŸ“… ä½¿ç”¨æ–‡ä»¶é…å¯¹æ–¹æ¡ˆï¼šä»Šå¤©æ–‡ä»¶ â†’ æ˜å¤©ç›®æ ‡")
                
                features_list = []
                targets_list = []
                processed_pairs = 0
                
                # ç›¸é‚»æ–‡ä»¶é…å¯¹
                for i in range(len(parquet_files) - 1):
                    today_file = parquet_files[i]      # ä»Šå¤©çš„ç‰¹å¾
                    tomorrow_file = parquet_files[i+1]  # æ˜å¤©çš„ç›®æ ‡
                    
                    try:
                        # è¯»å–ä»Šå¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
                        today_data = pd.read_parquet(today_file)
                        # è¯»å–æ˜å¤©çš„æ•°æ®æå–ç›®æ ‡
                        tomorrow_data = pd.read_parquet(tomorrow_file)
                        
                        # æŒ‰è‚¡ç¥¨ä»£ç åŒ¹é…ï¼ˆå–äº¤é›†ï¼‰
                        common_stocks = today_data.index.intersection(tomorrow_data.index)
                        
                        if len(common_stocks) > 0:
                            # ä»Šå¤©çš„æ‰€æœ‰ä¿¡æ¯ä½œä¸ºç‰¹å¾
                            features_list.append(today_data.loc[common_stocks])
                            # æ˜å¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç›®æ ‡
                            targets_list.append(tomorrow_data.loc[common_stocks, target_column])
                            processed_pairs += 1
                            
                        self.logger.info(f"   âœ… é…å¯¹: {today_file.name} â†’ {tomorrow_file.name}, è‚¡ç¥¨: {len(common_stocks)}")
                        
                    except Exception as e:
                        self.logger.warning(f"   è·³è¿‡é…å¯¹ {today_file.name} â†’ {tomorrow_file.name}: {e}")
                        continue
                
                if not features_list:
                    self.logger.error("âŒ æ²¡æœ‰æˆåŠŸé…å¯¹ä»»ä½•æ–‡ä»¶")
                    return False
                
                # åˆå¹¶æ‰€æœ‰é…å¯¹çš„æ•°æ®
                self.logger.info(f"   ğŸ”„ åˆå¹¶ {processed_pairs} ä¸ªæ–‡ä»¶é…å¯¹çš„æ•°æ®...")
                full_data = pd.concat(features_list, ignore_index=False)
                targets_data = pd.concat(targets_list, ignore_index=False)
                
                # æ·»åŠ ç›®æ ‡åˆ—
                full_data['next_day_target'] = targets_data
                
                self.logger.info(f"   âœ… æ–‡ä»¶é…å¯¹å®Œæˆ:")
                self.logger.info(f"   - å¤„ç†æ–‡ä»¶å¯¹: {processed_pairs}")
                self.logger.info(f"   - æœ€ç»ˆæ ·æœ¬æ•°: {len(full_data):,}")
                self.logger.info(f"   - ç‰¹å¾åˆ—æ•°: {len(full_data.columns)}")
                
            else:
                self.logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_format}")
                return False
            
            # æ£€æŸ¥æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
            if 'next_day_target' not in full_data.columns:
                self.logger.error(f"âŒ æœªæ‰¾åˆ°æ¬¡æ—¥é¢„æµ‹ç›®æ ‡åˆ— 'next_day_target'")
                return False
            
            # è®¾ç½®ç›®æ ‡å˜é‡ï¼ˆæ˜å¤©çš„æ¶¨è·Œå¹…ï¼‰
            self.y = full_data['next_day_target']
            actual_target_column = 'next_day_target'
            
            # æ’é™¤ç›®æ ‡åˆ—å’Œè¾…åŠ©åˆ—ï¼Œä¿ç•™ä»Šå¤©çš„æ¶¨è·Œå¹…ä½œä¸ºç‰¹å¾
            exclude_columns = exclude_columns + ['next_day_target']
            self.logger.info(f"   ğŸ’¡ ä»Šå¤©çš„'{target_column}'ç”¨ä½œé¢„æµ‹æ˜å¤©æ¶¨è·Œå¹…çš„ç‰¹å¾")
            
            # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤æŒ‡å®šçš„åˆ—ï¼‰
            feature_columns = [col for col in full_data.columns if col not in exclude_columns]
            self.X = full_data[feature_columns]
            
            # åªä¿ç•™æ•°å€¼åˆ—ä½œä¸ºç‰¹å¾
            numeric_columns = self.X.select_dtypes(include=[np.number]).columns
            self.X = self.X[numeric_columns]
            
            self.logger.info(f"   ğŸ“‹ æ’é™¤çš„åˆ—: {exclude_columns}")
            self.logger.info(f"   ğŸ“Š æ•°å€¼ç‰¹å¾åˆ—æ•°: {len(numeric_columns)}")
            
            # å¤„ç†ç¼ºå¤±å€¼
            self.X = self.X.fillna(0)
            self.y = self.y.fillna(0)
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = list(self.X.columns)
            
            # ä¿å­˜è‚¡ç¥¨ä¿¡æ¯
            stock_name_column = direct_training.get('stock_name_column', 'name')
            if stock_name_column in full_data.columns:
                self.stock_info = full_data[[stock_name_column]].copy()
            else:
                self.stock_info = None
            
            self.logger.info(f"   âœ… æ¬¡æ—¥é¢„æµ‹æ•°æ®åŠ è½½å®Œæˆ:")
            self.logger.info(f"     - ç‰¹å¾ç»´åº¦: {self.X.shape}")
            self.logger.info(f"     - ç›®æ ‡ç»´åº¦: {self.y.shape}")
            self.logger.info(f"     - ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            self.logger.info(f"     - ç›®æ ‡åˆ—: {actual_target_column}")
            self.logger.info(f"     - é¢„æµ‹ä»»åŠ¡: ä»Šå¤©ç‰¹å¾ â†’ æ˜å¤©æ¶¨è·Œå¹…")
            self.logger.info(f"     - ç›®æ ‡å€¼èŒƒå›´: [{self.y.min():.4f}, {self.y.max():.4f}]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

'''
    
    # æ„å»ºæ–°æ–‡ä»¶
    new_lines = []
    
    # 1. å¤åˆ¶307è¡Œä¹‹å‰çš„å†…å®¹ï¼ˆä¿ç•™_create_next_day_prediction_dataä¹‹å‰çš„éƒ¨åˆ†ï¼‰
    new_lines.extend(lines[:306])  # 0-305è¡Œ
    
    # 2. è·³è¿‡_create_next_day_prediction_dataæ–¹æ³•ï¼ˆ307-412è¡Œï¼‰
    # ç›´æ¥åˆ°_load_direct_dataæ–¹æ³•çš„å¼€å§‹
    
    # 3. æ’å…¥æ–°çš„_load_direct_dataæ–¹æ³•
    new_lines.append(new_method)
    new_lines.append('\n')
    
    # 4. å¤åˆ¶526è¡Œä¹‹åçš„å†…å®¹ï¼ˆsplit_dataæ–¹æ³•å¼€å§‹ï¼‰
    new_lines.extend(lines[525:])  # ä»525è¡Œå¼€å§‹
    
    # å†™å…¥æ–°æ–‡ä»¶
    with open('lightgbm_stock_train.py', 'w', encoding='utf-8') as f:
        f.writelines(new_lines)
    
    print(f"   ä¿®å¤åè¡Œæ•°: {len(new_lines)}")
    print("   âœ… ä¿®å¤å®Œæˆï¼")
    print("   - åˆ é™¤äº†å¤æ‚çš„_create_next_day_prediction_dataæ–¹æ³•")
    print("   - æ›¿æ¢ä¸ºç®€å•çš„æ–‡ä»¶é…å¯¹_load_direct_dataæ–¹æ³•")
    print("   - å¤§å¹…æå‡å¤„ç†é€Ÿåº¦")

if __name__ == "__main__":
    fix_training_script()