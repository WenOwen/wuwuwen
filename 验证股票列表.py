#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
éªŒè¯è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶å’Œæ¿å—æ•°æ®è·å–è„šæœ¬
å¿«é€Ÿæ£€æŸ¥all_stock_list.csvæ–‡ä»¶å’Œç›¸å…³é…ç½®
"""

import os
import sys
import pandas as pd
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_stock_list_file():
    """æ£€æŸ¥è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶"""
    logger.info("ğŸ” æ£€æŸ¥è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶...")
    
    stock_list_paths = [
        "data/stockcode_list/all_stock_list.csv",
        "../data/stockcode_list/all_stock_list.csv", 
        "stockcode_list/all_stock_list.csv",
        "all_stock_list.csv"
    ]
    
    stock_list_file = None
    for path in stock_list_paths:
        if os.path.exists(path):
            stock_list_file = path
            break
    
    if not stock_list_file:
        logger.error("âŒ æœªæ‰¾åˆ° all_stock_list.csv æ–‡ä»¶")
        logger.info("ğŸ“ è¯·ç¡®ä¿æ–‡ä»¶ä½äºä»¥ä¸‹ä»»ä¸€ä½ç½®:")
        for path in stock_list_paths:
            logger.info(f"   - {path}")
        return False
    
    try:
        df_stocks = pd.read_csv(stock_list_file, encoding='utf-8-sig')
        logger.info(f"âœ… æˆåŠŸæ‰¾åˆ°è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶: {stock_list_file}")
        logger.info(f"ğŸ“Š æ–‡ä»¶æ€»è¡Œæ•°: {len(df_stocks)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        required_columns = ['è‚¡ç¥¨ä»£ç ', 'åç§°', 'ä¸Šå¸‚çŠ¶æ€']
        missing_columns = [col for col in required_columns if col not in df_stocks.columns]
        
        if missing_columns:
            logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            logger.info(f"ğŸ“‹ å®é™…åˆ—å: {list(df_stocks.columns)}")
        else:
            logger.info("âœ… æ–‡ä»¶æ ¼å¼æ­£ç¡®")
        
        # ç»Ÿè®¡è‚¡ç¥¨çŠ¶æ€
        if 'ä¸Šå¸‚çŠ¶æ€' in df_stocks.columns:
            status_counts = df_stocks['ä¸Šå¸‚çŠ¶æ€'].value_counts()
            logger.info("ğŸ“Š è‚¡ç¥¨çŠ¶æ€ç»Ÿè®¡:")
            for status, count in status_counts.items():
                logger.info(f"   - {status}: {count}åª")
            
            # ç­›é€‰æ­£å¸¸äº¤æ˜“çš„è‚¡ç¥¨
            active_stocks = df_stocks[df_stocks['ä¸Šå¸‚çŠ¶æ€'] == 'æ­£å¸¸äº¤æ˜“']
            logger.info(f"ğŸ¯ æ­£å¸¸äº¤æ˜“è‚¡ç¥¨æ•°: {len(active_stocks)}åª")
            
            # æ˜¾ç¤ºå‰10åªè‚¡ç¥¨ä½œä¸ºç¤ºä¾‹
            logger.info("ğŸ“‹ æ­£å¸¸äº¤æ˜“è‚¡ç¥¨ç¤ºä¾‹ (å‰10åª):")
            for _, stock in active_stocks.head(10).iterrows():
                logger.info(f"   - {stock['è‚¡ç¥¨ä»£ç ']}: {stock['åç§°']}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¯»å–è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False

def check_samequant_functions():
    """æ£€æŸ¥samequant_functionsæ˜¯å¦å¯ç”¨"""
    logger.info("ğŸ” æ£€æŸ¥samequant_functionsæ¨¡å—...")
    
    # æ·»åŠ é¡¹ç›®è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = current_dir
    sys.path.append(project_root)
    sys.path.append(os.path.join(project_root, 'core'))
    
    try:
        from samequant_functions import Spider_func
        logger.info("âœ… samequant_functions æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å®ä¾‹åŒ–
        s_f_1 = Spider_func()
        logger.info("âœ… Spider_func å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¸€ä¸ªç®€å•çš„åŠŸèƒ½ï¼ˆè·å–è¡Œä¸šæ•°æ®ï¼‰
        try:
            logger.info("ğŸ§ª æµ‹è¯•è·å–è¡Œä¸šæ•°æ®åŠŸèƒ½...")
            df_industry = s_f_1.get_industry_data_from_eastmoney(sort_field='f3')
            if not df_industry.empty:
                logger.info(f"âœ… è¡Œä¸šæ•°æ®è·å–æˆåŠŸ: {len(df_industry)}ä¸ªè¡Œä¸š")
                logger.info("ğŸ“‹ å‰5ä¸ªè¡Œä¸šç¤ºä¾‹:")
                for i, row in df_industry.head(5).iterrows():
                    logger.info(f"   - {row.get('è¡Œä¸šåç§°', 'N/A')}: {row.get('æ¶¨è·Œå¹…', 'N/A')}")
            else:
                logger.warning("âš ï¸ è¡Œä¸šæ•°æ®ä¸ºç©º")
                
        except Exception as test_e:
            logger.warning(f"âš ï¸ è¡Œä¸šæ•°æ®è·å–æµ‹è¯•å¤±è´¥: {str(test_e)}")
            logger.info("ğŸ’¡ è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œä¸å½±å“è„šæœ¬çš„ä¸»è¦åŠŸèƒ½")
        
        return True
        
    except ImportError as e:
        logger.error(f"âŒ æ— æ³•å¯¼å…¥samequant_functions: {str(e)}")
        logger.info("ğŸ“ è¯·ç¡®ä¿samequant_functions.pyåœ¨é¡¹ç›®æ ¹ç›®å½•")
        return False
    except Exception as e:
        logger.error(f"âŒ samequant_functionsæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def check_output_directory():
    """æ£€æŸ¥è¾“å‡ºç›®å½•"""
    logger.info("ğŸ” æ£€æŸ¥è¾“å‡ºç›®å½•...")
    
    output_dir = "sector_data"
    
    if os.path.exists(output_dir):
        logger.info(f"âœ… è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}")
        
        # æ£€æŸ¥ç°æœ‰æ–‡ä»¶
        existing_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]
        if existing_files:
            logger.info(f"ğŸ“ å‘ç° {len(existing_files)} ä¸ªç°æœ‰CSVæ–‡ä»¶:")
            for file in existing_files:
                file_path = os.path.join(output_dir, file)
                file_size = os.path.getsize(file_path) / 1024  # KB
                logger.info(f"   - {file}: {file_size:.1f} KB")
        else:
            logger.info("ğŸ“ è¾“å‡ºç›®å½•ä¸ºç©º")
    else:
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»º: {output_dir}")
    
    # æ£€æŸ¥å†™æƒé™
    try:
        os.makedirs(output_dir, exist_ok=True)
        test_file = os.path.join(output_dir, "test_write.tmp")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        logger.info("âœ… è¾“å‡ºç›®å½•å†™æƒé™æ­£å¸¸")
        return True
    except Exception as e:
        logger.error(f"âŒ è¾“å‡ºç›®å½•å†™æƒé™æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def estimate_processing_time():
    """é¢„ä¼°å¤„ç†æ—¶é—´"""
    logger.info("â±ï¸ å¤„ç†æ—¶é—´é¢„ä¼°...")
    
    # åŸºäºç»éªŒå€¼çš„æ—¶é—´é¢„ä¼°
    time_per_stock = 0.5  # æ¯åªè‚¡ç¥¨çº¦0.5ç§’ï¼ˆåŒ…å«ç½‘ç»œå»¶è¿Ÿå’Œé‡è¯•ï¼‰
    
    stock_counts = [100, 500, 1000, 5000]
    logger.info("ğŸ“Š ä¸åŒæ¨¡å¼çš„é¢„ä¼°å¤„ç†æ—¶é—´:")
    
    for count in stock_counts:
        estimated_seconds = count * time_per_stock
        estimated_minutes = estimated_seconds / 60
        estimated_hours = estimated_minutes / 60
        
        if estimated_hours >= 1:
            time_str = f"{estimated_hours:.1f}å°æ—¶"
        elif estimated_minutes >= 1:
            time_str = f"{estimated_minutes:.1f}åˆ†é’Ÿ"
        else:
            time_str = f"{estimated_seconds:.0f}ç§’"
        
        logger.info(f"   - {count}åªè‚¡ç¥¨: çº¦{time_str}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ”§ è‚¡ç¥¨æ¿å—æ•°æ®è·å– - ç³»ç»ŸéªŒè¯")
    logger.info("=" * 60)
    
    all_checks_passed = True
    
    # 1. æ£€æŸ¥è‚¡ç¥¨åˆ—è¡¨æ–‡ä»¶
    if not check_stock_list_file():
        all_checks_passed = False
    
    logger.info("")
    
    # 2. æ£€æŸ¥samequant_functionsæ¨¡å—
    if not check_samequant_functions():
        all_checks_passed = False
    
    logger.info("")
    
    # 3. æ£€æŸ¥è¾“å‡ºç›®å½•
    if not check_output_directory():
        all_checks_passed = False
    
    logger.info("")
    
    # 4. é¢„ä¼°å¤„ç†æ—¶é—´
    estimate_processing_time()
    
    logger.info("")
    logger.info("=" * 60)
    
    if all_checks_passed:
        logger.info("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª")
        logger.info("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œ: python data_processing/è·å–æ¿å—æ•°æ®å¹¶ä¿å­˜CSV.py")
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·è§£å†³ä¸Šè¿°é—®é¢˜åå†è¿è¡Œæ•°æ®è·å–è„šæœ¬")
    
    logger.info("=" * 60)

if __name__ == "__main__":
    main()