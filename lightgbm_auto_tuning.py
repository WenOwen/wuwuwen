#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LightGBM è‡ªåŠ¨å‚æ•°è°ƒä¼˜è„šæœ¬
ä½¿ç”¨ç½‘æ ¼æœç´¢å’Œè´å¶æ–¯ä¼˜åŒ–æ¥æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import lightgbm as lgb
from pathlib import Path
from sklearn.model_selection import ParameterGrid, TimeSeriesSplit
from sklearn.metrics import roc_auc_score, accuracy_score
import warnings
warnings.filterwarnings('ignore')

class LightGBMAutoTuner:
    """LightGBMè‡ªåŠ¨è°ƒä¼˜å™¨"""
    
    def __init__(self, data_dir="./data/professional_parquet"):
        self.data_dir = Path(data_dir)
        self.X = None
        self.y = None
        self.best_params = None
        self.best_score = 0
        self.results = []
        
    def load_data(self):
        """åŠ è½½æ•°æ®ï¼ˆå¤ç”¨ç°æœ‰çš„æ–‡ä»¶é…å¯¹é€»è¾‘ï¼‰"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        
        parquet_files = sorted(list(self.data_dir.glob("*.parquet")))
        if len(parquet_files) < 2:
            raise ValueError("éœ€è¦è‡³å°‘2ä¸ªparquetæ–‡ä»¶")
        
        features_list = []
        targets_list = []
        
        # æ–‡ä»¶é…å¯¹ç­–ç•¥
        for i in range(len(parquet_files) - 1):
            today_file = parquet_files[i]
            tomorrow_file = parquet_files[i+1]
            
            try:
                today_data = pd.read_parquet(today_file)
                tomorrow_data = pd.read_parquet(tomorrow_file)
                
                common_stocks = today_data.index.intersection(tomorrow_data.index)
                if len(common_stocks) > 0:
                    features_list.append(today_data.loc[common_stocks])
                    targets_list.append(tomorrow_data.loc[common_stocks, 'æ¶¨è·Œå¹…'])
                    
            except Exception as e:
                print(f"è·³è¿‡æ–‡ä»¶å¯¹ {today_file.name}: {e}")
                continue
        
        # åˆå¹¶æ•°æ®
        full_data = pd.concat(features_list, ignore_index=False)
        targets_data = pd.concat(targets_list, ignore_index=False)
        
        # åˆ›å»ºæ–¹å‘ç›®æ ‡
        self.y = (targets_data > 0).astype(int)
        
        # é€‰æ‹©ç‰¹å¾
        exclude_columns = ['name', 'symbol']
        feature_columns = [col for col in full_data.columns if col not in exclude_columns]
        self.X = full_data[feature_columns]
        
        # åªä¿ç•™æ•°å€¼åˆ—
        numeric_columns = self.X.select_dtypes(include=[np.number]).columns
        self.X = self.X[numeric_columns].fillna(0)
        self.y = self.y.fillna(0)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {self.X.shape[0]} æ ·æœ¬, {self.X.shape[1]} ç‰¹å¾")
        print(f"ç›®æ ‡åˆ†å¸ƒ: çœ‹å¤š={sum(self.y)}, çœ‹ç©º={len(self.y)-sum(self.y)}")
        
    def define_search_space(self):
        """å®šä¹‰æœç´¢ç©ºé—´"""
        
        # ğŸ¯ åŸºç¡€ç½‘æ ¼æœç´¢å‚æ•°
        base_grid = {
            'num_leaves': [15, 31, 63],
            'learning_rate': [0.01, 0.03, 0.05],
            'feature_fraction': [0.5, 0.7, 0.9],
            'bagging_fraction': [0.5, 0.7, 0.9],
            'lambda_l1': [0.1, 0.5, 1.0],
            'lambda_l2': [0.1, 0.5, 1.0],
            'min_data_in_leaf': [50, 100, 200],
            'max_depth': [3, 5, 7]
        }
        
        # ğŸ” ç²¾ç»†æœç´¢å‚æ•°ï¼ˆåœ¨æœ€ä½³åŒºåŸŸé™„è¿‘ï¼‰
        fine_grid = {
            'num_leaves': [20, 25, 30, 35, 40],
            'learning_rate': [0.02, 0.025, 0.03, 0.035, 0.04],
            'feature_fraction': [0.6, 0.65, 0.7, 0.75, 0.8],
            'bagging_fraction': [0.6, 0.65, 0.7, 0.75, 0.8],
            'lambda_l1': [0.2, 0.3, 0.4, 0.5, 0.6],
            'lambda_l2': [0.2, 0.3, 0.4, 0.5, 0.6]
        }
        
        return base_grid, fine_grid
    
    def evaluate_params(self, params, cv_folds=3):
        """è¯„ä¼°å‚æ•°ç»„åˆ"""
        
        # å›ºå®šå‚æ•°
        fixed_params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'verbose': -1,
            'random_state': 42,
            'bagging_freq': 1
        }
        
        # åˆå¹¶å‚æ•°
        all_params = {**fixed_params, **params}
        
        # æ—¶åºäº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=cv_folds)
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(self.X)):
            X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]
            y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]
            
            # æ£€æŸ¥ç›®æ ‡åˆ†å¸ƒ
            if len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2:
                continue
                
            try:
                # è®­ç»ƒæ¨¡å‹
                train_data = lgb.Dataset(X_train, label=y_train)
                val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                
                model = lgb.train(
                    all_params,
                    train_data,
                    valid_sets=[val_data],
                    num_boost_round=200,
                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
                )
                
                # é¢„æµ‹å’Œè¯„ä¼°
                y_pred = model.predict(X_val)
                auc_score = roc_auc_score(y_val, y_pred)
                scores.append(auc_score)
                
            except Exception as e:
                print(f"  âŒ Fold {fold} å¤±è´¥: {e}")
                continue
        
        if len(scores) == 0:
            return 0.0
            
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        return mean_score, std_score
    
    def grid_search(self, param_grid, max_combinations=50):
        """ç½‘æ ¼æœç´¢"""
        print(f"ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œæœ€å¤šæµ‹è¯• {max_combinations} ä¸ªç»„åˆ...")
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = list(ParameterGrid(param_grid))
        
        # å¦‚æœç»„åˆå¤ªå¤šï¼Œéšæœºé‡‡æ ·
        if len(param_combinations) > max_combinations:
            import random
            param_combinations = random.sample(param_combinations, max_combinations)
        
        print(f"ğŸ“Š å°†æµ‹è¯• {len(param_combinations)} ä¸ªå‚æ•°ç»„åˆ")
        
        best_score = 0
        best_params = None
        
        for i, params in enumerate(param_combinations):
            print(f"\nğŸ§ª æµ‹è¯•ç»„åˆ {i+1}/{len(param_combinations)}: {params}")
            
            try:
                mean_score, std_score = self.evaluate_params(params)
                
                result = {
                    'params': params,
                    'mean_auc': mean_score,
                    'std_auc': std_score,
                    'combination_id': i+1
                }
                self.results.append(result)
                
                print(f"   ğŸ“ˆ å¹³å‡AUC: {mean_score:.4f} Â± {std_score:.4f}")
                
                if mean_score > best_score:
                    best_score = mean_score
                    best_params = params
                    print(f"   ğŸ¯ æ–°çš„æœ€ä½³ç»“æœ!")
                    
            except Exception as e:
                print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        self.best_score = best_score
        self.best_params = best_params
        
        print(f"\nğŸ† æœç´¢å®Œæˆ!")
        print(f"æœ€ä½³AUC: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")
        
        return best_params, best_score
    
    def save_results(self, output_dir="./tuning_results"):
        """ä¿å­˜è°ƒä¼˜ç»“æœ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df = pd.DataFrame(self.results)
        results_df = results_df.sort_values('mean_auc', ascending=False)
        results_df.to_csv(output_dir / "tuning_results.csv", index=False)
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        with open(output_dir / "best_params.json", 'w') as f:
            json.dump({
                'best_params': self.best_params,
                'best_score': self.best_score,
                'total_combinations': len(self.results)
            }, f, indent=2)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config = self.create_config_from_params(self.best_params)
        with open(output_dir / "lightGBM_optimized.yaml", 'w', encoding='utf-8') as f:
            f.write(config)
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def create_config_from_params(self, params):
        """ä»æœ€ä½³å‚æ•°åˆ›å»ºé…ç½®æ–‡ä»¶"""
        config = f"""# LightGBM è‡ªåŠ¨è°ƒä¼˜ç»“æœé…ç½®æ–‡ä»¶
# æœ€ä½³AUC: {self.best_score:.4f}
# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}

data:
  data_dir: "./data/professional_parquet"
  direct_training:
    enabled: true
    data_format: "parquet"
    stock_name_column: "name"
    target_column: "æ¶¨è·Œå¹…"
    prediction_mode: "direction"
    exclude_columns:
      - "name"
      - "symbol"

training:
  data_split:
    test_size: 0.2
    validation_size: 0.15
    random_state: 42
    stratify: true
    time_series_split: true
    
  training_params:
    early_stopping_rounds: 50
    verbose: 50
    eval_metric: ["auc", "binary_logloss"]

lightgbm:
  basic_params:
    objective: "binary"
    metric: "auc"
    boosting_type: "gbdt"
    num_leaves: {params.get('num_leaves', 31)}
    learning_rate: {params.get('learning_rate', 0.03)}
    feature_fraction: {params.get('feature_fraction', 0.7)}
    bagging_fraction: {params.get('bagging_fraction', 0.7)}
    bagging_freq: 1
    verbose: -1
    random_state: 42
    
  advanced_params:
    max_depth: {params.get('max_depth', 5)}
    min_data_in_leaf: {params.get('min_data_in_leaf', 100)}
    lambda_l1: {params.get('lambda_l1', 0.1)}
    lambda_l2: {params.get('lambda_l2', 0.1)}
    
  fit_params:
    num_boost_round: 500

output:
  file_naming:
    folder_name_prefix: "optimized_training"
  model_save:
    save_dir: "./models/lightgbm_optimized"
  results_save:
    save_dir: "./results/lightgbm_optimized"

evaluation:
  metrics:
    - "auc"
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
"""
        return config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LightGBM è‡ªåŠ¨å‚æ•°è°ƒä¼˜")
    print("=" * 50)
    
    # åˆ›å»ºè°ƒä¼˜å™¨
    tuner = LightGBMAutoTuner()
    
    # åŠ è½½æ•°æ®
    tuner.load_data()
    
    # å®šä¹‰æœç´¢ç©ºé—´
    base_grid, fine_grid = tuner.define_search_space()
    
    # ç¬¬ä¸€é˜¶æ®µï¼šç²—æœç´¢
    print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç½‘æ ¼æœç´¢")
    best_params_base, best_score_base = tuner.grid_search(base_grid, max_combinations=30)
    
    if best_score_base > 0.5:
        print(f"\nâœ… åŸºç¡€æœç´¢æˆåŠŸ! AUC: {best_score_base:.4f}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†æœç´¢ï¼ˆå¯é€‰ï¼‰
        print("\nğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†ç½‘æ ¼æœç´¢")
        # åŸºäºæœ€ä½³ç»“æœè°ƒæ•´ç²¾ç»†æœç´¢èŒƒå›´
        fine_grid_adjusted = {
            'num_leaves': [max(15, best_params_base['num_leaves']-10), 
                          best_params_base['num_leaves'], 
                          best_params_base['num_leaves']+10],
            'learning_rate': [best_params_base['learning_rate']*0.8,
                             best_params_base['learning_rate'],
                             best_params_base['learning_rate']*1.2],
            # å…¶ä»–å‚æ•°ç±»ä¼¼è°ƒæ•´...
        }
        
        tuner.grid_search(fine_grid_adjusted, max_combinations=20)
    
    # ä¿å­˜ç»“æœ
    tuner.save_results()
    
    print(f"\nğŸ‰ è°ƒä¼˜å®Œæˆ!")
    print(f"ğŸ† æœ€ç»ˆæœ€ä½³AUC: {tuner.best_score:.4f}")
    print(f"ğŸ“‹ æœ€ä½³å‚æ•°: {tuner.best_params}")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: ./tuning_results/lightGBM_optimized.yaml")

if __name__ == "__main__":
    main()