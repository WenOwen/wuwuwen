#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸ºè®­ç»ƒè„šæœ¬æ·»åŠ æ¶¨è·Œæ–¹å‘é¢„æµ‹æ¨¡å¼
"""

# 1. åœ¨_load_direct_dataæ–¹æ³•ä¸­ç›®æ ‡å˜é‡å¤„ç†çš„ä¿®æ”¹
target_processing_code = '''
            # æ£€æŸ¥é¢„æµ‹æ¨¡å¼
            prediction_mode = direct_training.get('prediction_mode', 'regression')
            self.prediction_mode = prediction_mode  # ä¿å­˜é¢„æµ‹æ¨¡å¼
            
            # è®¾ç½®ç›®æ ‡å˜é‡
            raw_targets = full_data['next_day_target']
            
            if prediction_mode == 'direction':
                # ğŸ¯ æ–¹å‘é¢„æµ‹æ¨¡å¼ï¼šæ¶¨è·Œå¹… > 0 ä¸ºçœ‹å¤š(1)ï¼Œ<= 0 ä¸ºçœ‹ç©º(0)
                self.y = (raw_targets > 0).astype(int)
                actual_target_column = 'next_day_direction'
                self.logger.info(f"   ğŸ¯ é¢„æµ‹æ¨¡å¼: æ¶¨è·Œæ–¹å‘é¢„æµ‹ï¼ˆäºŒåˆ†ç±»ï¼‰")
                self.logger.info(f"   ğŸ“Š çœ‹å¤šæ ·æœ¬: {(self.y == 1).sum():,} ({(self.y == 1).mean()*100:.1f}%)")
                self.logger.info(f"   ğŸ“Š çœ‹ç©ºæ ·æœ¬: {(self.y == 0).sum():,} ({(self.y == 0).mean()*100:.1f}%)")
            else:
                # ğŸ“ˆ å›å½’é¢„æµ‹æ¨¡å¼ï¼šé¢„æµ‹å…·ä½“æ¶¨è·Œå¹…
                self.y = raw_targets
                actual_target_column = 'next_day_target'
                self.logger.info(f"   ğŸ“ˆ é¢„æµ‹æ¨¡å¼: æ¶¨è·Œå¹…é¢„æµ‹ï¼ˆå›å½’ï¼‰")
                self.logger.info(f"   ğŸ“Š ç›®æ ‡å€¼èŒƒå›´: [{self.y.min():.4f}, {self.y.max():.4f}]")
'''

# 2. è¯„ä¼°æ–¹æ³•çš„ä¿®æ”¹ä»£ç 
evaluation_code = '''
    def evaluate_model(self) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        try:
            self.logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è·å–é¢„æµ‹ç»“æœ
            if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                # äºŒåˆ†ç±»ï¼šè·å–æ¦‚ç‡é¢„æµ‹
                y_train_pred_proba = self.model.predict(self.X_train)
                y_val_pred_proba = self.model.predict(self.X_val)
                y_test_pred_proba = self.model.predict(self.X_test)
                
                # è½¬æ¢ä¸ºç±»åˆ«é¢„æµ‹ï¼ˆæ¦‚ç‡ > 0.5 ä¸ºçœ‹å¤šï¼‰
                y_train_pred = (y_train_pred_proba > 0.5).astype(int)
                y_val_pred = (y_val_pred_proba > 0.5).astype(int)
                y_test_pred = (y_test_pred_proba > 0.5).astype(int)
            else:
                # å›å½’ï¼šç›´æ¥é¢„æµ‹æ•°å€¼
                y_train_pred = self.model.predict(self.X_train)
                y_val_pred = self.model.predict(self.X_val)
                y_test_pred = self.model.predict(self.X_test)
                y_train_pred_proba = y_val_pred_proba = y_test_pred_proba = None
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            eval_config = self.config.get('evaluation', {})
            metrics_list = eval_config.get('metrics', ['rmse', 'mae', 'r2_score'])
            
            results = {}
            
            for split, y_true, y_pred, y_pred_proba in [
                ('train', self.y_train, y_train_pred, y_train_pred_proba),
                ('val', self.y_val, y_val_pred, y_val_pred_proba),
                ('test', self.y_test, y_test_pred, y_test_pred_proba)
            ]:
                split_metrics = {}
                
                for metric in metrics_list:
                    try:
                        if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction':
                            # ğŸ¯ åˆ†ç±»æŒ‡æ ‡
                            if metric == 'accuracy':
                                value = np.mean(y_true == y_pred) * 100
                            elif metric == 'auc' and y_pred_proba is not None:
                                from sklearn.metrics import roc_auc_score
                                value = roc_auc_score(y_true, y_pred_proba)
                            elif metric == 'precision':
                                from sklearn.metrics import precision_score
                                value = precision_score(y_true, y_pred, zero_division=0)
                            elif metric == 'recall':
                                from sklearn.metrics import recall_score
                                value = recall_score(y_true, y_pred, zero_division=0)
                            elif metric == 'f1_score':
                                from sklearn.metrics import f1_score
                                value = f1_score(y_true, y_pred, zero_division=0)
                            elif metric == 'log_loss' and y_pred_proba is not None:
                                from sklearn.metrics import log_loss
                                # å¤„ç†æ¦‚ç‡è¾¹ç•Œé—®é¢˜
                                y_pred_proba_clipped = np.clip(y_pred_proba, 1e-15, 1-1e-15)
                                value = log_loss(y_true, y_pred_proba_clipped)
                            else:
                                continue
                        else:
                            # ğŸ“ˆ å›å½’æŒ‡æ ‡
                            if metric == 'rmse':
                                value = np.sqrt(mean_squared_error(y_true, y_pred))
                            elif metric == 'mae':
                                value = mean_absolute_error(y_true, y_pred)
                            elif metric == 'mape':
                                value = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                            elif metric == 'r2_score':
                                value = r2_score(y_true, y_pred)
                            elif metric == 'explained_variance':
                                value = explained_variance_score(y_true, y_pred)
                            elif metric == 'directional_accuracy':
                                # æ–¹å‘å‡†ç¡®ç‡ï¼ˆè‚¡ç¥¨é¢„æµ‹ç‰¹æœ‰æŒ‡æ ‡ï¼‰
                                direction_true = np.sign(y_true)
                                direction_pred = np.sign(y_pred)
                                value = np.mean(direction_true == direction_pred) * 100
                            else:
                                continue
                        
                        split_metrics[metric] = float(value)
                        
                    except Exception as e:
                        self.logger.warning(f"   è®¡ç®—æŒ‡æ ‡ {metric} å¤±è´¥: {e}")
                        continue
                
                results[split] = split_metrics
            
            # è¾“å‡ºç»“æœ
            prediction_type = "æ–¹å‘é¢„æµ‹" if hasattr(self, 'prediction_mode') and self.prediction_mode == 'direction' else "å›å½’é¢„æµ‹"
            self.logger.info(f"   ğŸ“ˆ è¯„ä¼°ç»“æœ ({prediction_type}):")
            for split, metrics in results.items():
                self.logger.info(f"     {split.upper()}:")
                for metric, value in metrics.items():
                    if metric in ['mape', 'directional_accuracy', 'accuracy']:
                        self.logger.info(f"       {metric}: {value:.2f}%")
                    else:
                        self.logger.info(f"       {metric}: {value:.6f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}
'''

print("ğŸ“ æ–¹å‘é¢„æµ‹åŠŸèƒ½ä»£ç å·²å‡†å¤‡å¥½")
print("éœ€è¦æ‰‹åŠ¨æ·»åŠ åˆ°è®­ç»ƒè„šæœ¬ä¸­ï¼š")
print("1. ç›®æ ‡å˜é‡å¤„ç†ä»£ç ï¼ˆæ›¿æ¢391-393è¡Œï¼‰")
print("2. è¯„ä¼°æ–¹æ³•ä»£ç ï¼ˆæ›¿æ¢æ•´ä¸ªevaluate_modelæ–¹æ³•ï¼‰")